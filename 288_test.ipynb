{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afedb2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats, linalg\n",
    "from scipy.stats import randint as sp_randint\n",
    "import scipy.sparse\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pylab as pl\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer, OneHotEncoder, PolynomialFeatures, Normalizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC, OneClassSVM\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomTreesEmbedding, RandomForestClassifier, VotingClassifier, IsolationForest, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neural_network import BernoulliRBM, MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, DotProduct, Matern, RationalQuadratic, ExpSineSquared, ConstantKernel as CK, CompoundKernel, PairwiseKernel, WhiteKernel, Product, Exponentiation, Sum\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.kernel_approximation import RBFSampler, AdditiveChi2Sampler, Nystroem, SkewedChi2Sampler\n",
    "from sklearn import mixture\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, FeatureAgglomeration, DBSCAN, MeanShift, estimate_bandwidth, AgglomerativeClustering\n",
    "from sklearn.cluster import SpectralBiclustering, SpectralCoclustering\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split, StratifiedShuffleSplit, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics.pairwise import chi2_kernel, laplacian_kernel, additive_chi2_kernel\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, f1_score, accuracy_score, recall_score, roc_auc_score, roc_curve, auc, consensus_score, brier_score_loss, log_loss\n",
    "from sklearn.decomposition import PCA, KernelPCA, SparsePCA, NMF, FactorAnalysis, FastICA, MiniBatchDictionaryLearning, TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.cross_decomposition import PLSCanonical, PLSSVD, PLSRegression, CCA\n",
    "from sklearn import random_projection, manifold\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.covariance import ShrunkCovariance, LedoitWolf, EmpiricalCovariance, MinCovDet, EllipticEnvelope\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectPercentile, f_classif, RFECV, RFE, SelectKBest, chi2, mutual_info_classif, SelectFromModel, SelectFwe\n",
    "from sklearn.feature_extraction.image import grid_to_graph\n",
    "from imblearn.over_sampling import RandomOverSampler, ADASYN, SMOTE, BorderlineSMOTE, SVMSMOTE, SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion, make_union\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import pywt\n",
    "import cv2\n",
    "import scipy.signal\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K, metrics as krmetrics, regularizers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Activation, Dropout, ZeroPadding2D, advanced_activations as AC, Reshape, Flatten, Embedding, Conv2D, Conv1D, GlobalMaxPooling2D, GlobalMaxPooling1D, MaxPooling2D, MaxPooling1D, AveragePooling2D, GlobalAveragePooling1D, LocallyConnected1D, AveragePooling1D, UpSampling2D, BatchNormalization, Lambda, Layer, Conv2DTranspose, LSTM, GRU, TimeDistributed, SimpleRNN, ConvLSTM2D, Permute, RepeatVector, Cropping1D, Cropping2D, Add, SeparableConv2D, LocallyConnected2D, Multiply, Concatenate, SeparableConv1D, CuDNNLSTM, CuDNNGRU, GlobalAveragePooling2D, ZeroPadding1D\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, array_to_img, load_img\n",
    "from keras.regularizers import l2, l1\n",
    "from keras import initializers\n",
    "from keras.layers.noise import GaussianNoise, GaussianDropout\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.datasets import mnist, cifar10, cifar100, fashion_mnist\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import utils\n",
    "from keras import activations\n",
    "import keras.utils.np_utils as kutils\n",
    "import json\n",
    "import argparse\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import log_loss\n",
    "from keras.models import load_model, save_model, clone_model\n",
    "import skimage.transform\n",
    "from keras_contrib.callbacks import CyclicLR\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19272de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session(devices=\"0\", gpu_fraction=0.25):\n",
    "    np.random.seed(random_state_tf)\n",
    "    tf.compat.v1.set_random_seed(random_state_tf)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = devices\n",
    "    gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction, allow_growth=True)\n",
    "    sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "    return sess\n",
    "\n",
    "class CustomModelCheckpoint(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        if in_cv:\n",
    "            self.ts_type = \"cv\" + str(n_cv)\n",
    "        else:\n",
    "            self.ts_type = \"blind%s\" % conf_permtest\n",
    "        self.rocp = []\n",
    "        self.f1 = []\n",
    "        self.val_acc = []\n",
    "        self.rfa = []\n",
    "        self.df_hist = pd.DataFrame(columns=[\"epoch\", \"loss\", \"acc\", \"val_loss\", \"val_acc\", \"rocp\", \"f1\", \"rfa\"])\n",
    "        self.idx = 0\n",
    "        self.best_epoch = 1\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        print(\"\\n======================BEST:======================\")\n",
    "        best_idx = self.best_epoch - 1\n",
    "        print(self.df_hist.iloc[best_idx:best_idx + 1, :])\n",
    "        if verbose_callback:\n",
    "            print(self.df_hist)\n",
    "            print(self.df_hist.describe())\n",
    "        print(\"\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if in_cv:\n",
    "            x_val = x_test_dl\n",
    "            y_val = y_test_dl[:, watch_cls]\n",
    "        else:\n",
    "            x_val = x_blind_dl\n",
    "            y_val = y_blind_dl[:, watch_cls]\n",
    "        label_out_pred_prob_d = model.predict(x_val)\n",
    "        label_out_pred_d = np.argmax(label_out_pred_prob_d, axis=1)\n",
    "        label_out_pred_m = label_out_pred_d\n",
    "        label_out_pred = np.asarray(label_out_pred_m == watch_cls, dtype=np.int32)\n",
    "        label_out_pred_prob = label_out_pred_prob_d[:, watch_cls]\n",
    "        rocp = roc_auc_score(y_val, label_out_pred_prob)\n",
    "        f1 = f1_score(y_val, label_out_pred)\n",
    "        val_acc = logs.get(\"val_accuracy\")\n",
    "        acc = logs.get(\"accuracy\")\n",
    "        rfa = rocp * w_rocp + f1 * w_f1 + val_acc * w_acc\n",
    "        self.df_hist.loc[self.idx] = [epoch + 1, logs.get(\"loss\"), acc, logs.get(\"val_loss\"), val_acc, rocp, f1, rfa]\n",
    "        self.idx += 1\n",
    "        if verbose_callback:\n",
    "            print(\"======================EPOCH %06d:======================\" % (epoch + 1))\n",
    "            print(self.df_hist.iloc[self.idx - 1:self.idx, :])\n",
    "\n",
    "        need_add = False\n",
    "        if len(self.rfa) == 0 or rfa > np.max(self.rfa):\n",
    "            need_add = True\n",
    "        if need_add:\n",
    "            self.rfa.append(rfa)\n",
    "            if not verbose_callback:\n",
    "                print(\"======================EPOCH %06d:======================\" % (epoch + 1))\n",
    "                print(self.df_hist.iloc[self.idx - 1:self.idx, :])\n",
    "            print(\"RFA :\", rfa)\n",
    "            if save_histweights:\n",
    "                model.save_weights(filepath_custom % (conf_tags_dl, self.ts_type, batch_size_cv, epoch, logs.get(\"loss\"), acc, logs.get(\"val_loss\"), val_acc, rocp, f1, rfa))\n",
    "            if metric_save == \"rfa\":\n",
    "                self.best_epoch = epoch + 1\n",
    "                model.save_weights(save_dir + \"/\" + conf_tags_dl + \"_%s.h5\" % self.ts_type)\n",
    "\n",
    "def lr_log(epoch):\n",
    "    lr = model.optimizer.get_config()[\"learning_rate\"]\n",
    "    print('Learning rate: %0.8f' % lr)\n",
    "    return lr\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < int(epochs * lr_factors[0]):\n",
    "        lr = lr_init\n",
    "        return lr\n",
    "    if epoch < int(epochs * lr_factors[1]):\n",
    "        lr = lr_init * 0.1\n",
    "        return lr\n",
    "    if epoch < int(epochs * lr_factors[2]):\n",
    "        lr = lr_init * 0.01\n",
    "        return lr\n",
    "    if epoch < int(epochs * lr_factors[3]):\n",
    "        lr = lr_init * 0.001\n",
    "        return lr\n",
    "    lr = lr_init * 0.0001\n",
    "    return lr\n",
    "\n",
    "def huber_loss_mean(y_true, y_pred):\n",
    "    clip_delta = 1.0\n",
    "    error_abs = K.abs(y_true - y_pred)\n",
    "    cond = error_abs < clip_delta\n",
    "    squared_loss = 0.5 * K.square(error_abs)\n",
    "    linear_loss = clip_delta * (error_abs - 0.5 * clip_delta)\n",
    "    loss = tf.where(cond, squared_loss, linear_loss)\n",
    "    loss = K.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def xentropy_regu_mean(y_true, y_pred):\n",
    "    e = 0.5\n",
    "    loss1 = K.categorical_crossentropy(y_true, y_pred)\n",
    "    loss2 = K.categorical_crossentropy(K.ones_like(y_pred) / num_classes, y_pred)\n",
    "    return (1 - e) * loss1 + e * loss2\n",
    "\n",
    "def linf_loss_batch(y_true, y_pred):\n",
    "    ytf = K.flatten(y_true)\n",
    "    ypf = K.flatten(y_pred)\n",
    "    error_abs = K.abs(ytf - ypf)\n",
    "    error_max = K.max(error_abs)\n",
    "    return error_max\n",
    "\n",
    "def xcustom_loss_batch(y_true, y_pred):\n",
    "    loss = xentropy_regu_mean(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def clf_sigmoid(x):\n",
    "    raw = 1.0 / (1 + np.exp(-x * 1.0))\n",
    "    norm = preprocessing.normalize(raw, norm=\"l1\")\n",
    "    return norm\n",
    "\n",
    "def plot_peaks(x, indexes, algorithm=None, mph=None, mpd=None):\n",
    "    _, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "    ax.plot(x, 'b', lw=1)\n",
    "    if indexes.size:\n",
    "        label = 'peak'\n",
    "        label = label + 's' if indexes.size > 1 else label\n",
    "        ax.plot(indexes, x[indexes], '+', mfc=None, mec='r', mew=2, ms=8, label='%d %s' % (indexes.size, label))\n",
    "        ax.legend(loc='best', framealpha=.5, numpoints=1)\n",
    "    ax.set_xlim(-.02 * x.size, x.size * 1.02 - 1)\n",
    "    ymin, ymax = x[np.isfinite(x)].min(), x[np.isfinite(x)].max()\n",
    "    yrange = ymax - ymin if ymax > ymin else 1\n",
    "    ax.set_ylim(ymin - 0.1 * yrange, ymax + 0.1 * yrange)\n",
    "    ax.set_xlabel('Data #', fontsize=14)\n",
    "    ax.set_ylabel('Amplitude', fontsize=14)\n",
    "    ax.set_title('%s (mph=%s, mpd=%s)' % (algorithm, mph, mpd))\n",
    "    plt.show()\n",
    "\n",
    "pd.set_option('display.width', 1600)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "random_state_tf = 2100\n",
    "\n",
    "indata_dir = \".\"\n",
    "indata_path = indata_dir + \"/MMD_Diagnosis_Data.csv\"\n",
    "\n",
    "num_classes = 2\n",
    "watch_cls = 1\n",
    "\n",
    "img_rows, img_cols = 32, 32\n",
    "channels = 1\n",
    "\n",
    "bt_all = ['T1', 'T2', 'T3', 'T4', 'T5']\n",
    "\n",
    "fs_core = 134\n",
    "fs_core_idxs = np.arange(fs_core).tolist()\n",
    "fs_ext = 0\n",
    "fs_ext_idxs = (np.arange(fs_ext) + fs_core).tolist()\n",
    "fs_num = fs_core + fs_ext\n",
    "\n",
    "df_org = pd.read_csv(indata_path)\n",
    "cols_core = np.array(df_org.columns[:fs_core], dtype=str)\n",
    "df_fs_all = pd.DataFrame()\n",
    "df_fs_all[\"fs_names\"] = cols_core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44732a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================\n",
      "\n",
      "b51s2100v10s2100-stf_dn0mm10_osbfv2b_mlite-d25_b32e200clr2lr01-85frpt-v190s10\n",
      "\n",
      "===========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conf_ds = \"b51s2100v10s2100-stf_dn0mm10\"\n",
    "conf_tags = conf_ds\n",
    "\n",
    "save_ds = False\n",
    "\n",
    "split_mode = \"RND\"\n",
    "bt_train = ['T1', 'T2', 'T3', 'T4']\n",
    "ct_train = ['C1', 'C2', 'C3', 'C4']\n",
    "n_splits = 5\n",
    "split_no = 1\n",
    "hdx_seed = 2100\n",
    "\n",
    "cv_as_all = False\n",
    "cv_mode = \"RND\"\n",
    "n_cv_splits = 10\n",
    "cv_seed = 2100\n",
    "cv_partial = False\n",
    "n_cv_partial = int(190 * n_cv_splits / (n_cv_splits - 1))\n",
    "cv_partial_seed = 1000\n",
    "\n",
    "fs_idxs = fs_core_idxs + fs_ext_idxs\n",
    "\n",
    "dn_factor = None\n",
    "minmax_scale = True\n",
    "minmax_scale_factor = 10.0\n",
    "\n",
    "conf_osbfb = \"_osbfv2b\"\n",
    "conf_tags += conf_osbfb\n",
    "\n",
    "train_ros = True\n",
    "batch_fix = True\n",
    "batch_fix_v2 = True\n",
    "batch_fix_balance = True\n",
    "osbfb_seed = 2100\n",
    "\n",
    "m_dl = \"mlite\"\n",
    "conf_dl = \"_\" + m_dl + \"-d25_b32e200clr2lr01-85frpt\"\n",
    "conf_dl += \"-v190s10\"\n",
    "conf_tags_dl = conf_tags + conf_dl\n",
    "\n",
    "enable_dl = True\n",
    "need_training_cv = True\n",
    "cv_using_selflast = True\n",
    "cv_using_selfbest = True\n",
    "need_training_bl = True\n",
    "bl_using_selflast = False\n",
    "bl_using_selfbest = False\n",
    "bl_using_cvbests = True\n",
    "tb_enabled = False\n",
    "\n",
    "if enable_dl:\n",
    "    conf_tags += conf_dl\n",
    "\n",
    "batch_size_cv = 32\n",
    "epochs_cv = 200\n",
    "batch_size = batch_size_cv\n",
    "epochs = epochs_cv\n",
    "\n",
    "metric_save = \"rfa\"\n",
    "w_rocp = 0.3\n",
    "w_f1 = 0.35\n",
    "w_acc = 0.35\n",
    "w_merge_exp_factor = 0.5\n",
    "\n",
    "opt_name = \"adam\"\n",
    "lr_init = 0.001\n",
    "beta_1 = 0.75\n",
    "sgd_decay = 5e-5\n",
    "\n",
    "use_clr = True\n",
    "if not use_clr:\n",
    "    lr_factors = [0.4, 0.6, 0.8, 0.9]\n",
    "else:\n",
    "    step_mode = 'triangular'\n",
    "    epochs_clr_cv = epochs_cv / 2 * 1 / 2\n",
    "    epochs_clr = epochs / 2 * 1 / 2\n",
    "    base_lr = 1e-7\n",
    "    max_lr_cv = 0.01\n",
    "    max_lr = max_lr_cv\n",
    "\n",
    "data_augmentation = True\n",
    "mix_enable = False\n",
    "mix_alpha = 0.2\n",
    "mix_datagen = False\n",
    "\n",
    "show_model = True\n",
    "verbose_dl = 2\n",
    "\n",
    "conf_xk = \"_l1\"\n",
    "if enable_dl:\n",
    "    conf_xk += \"d50\"\n",
    "\n",
    "enable_x = False\n",
    "enable_sk = False\n",
    "w_sk = 0.25\n",
    "w_dl = 0.50\n",
    "\n",
    "if enable_x or enable_sk:\n",
    "    conf_tags += conf_xk\n",
    "\n",
    "conf_extra = \"\"\n",
    "conf_tags += conf_extra\n",
    "\n",
    "print(\"\\n===========================================================================\\n\")\n",
    "print(conf_tags)\n",
    "print(\"\\n===========================================================================\\n\")\n",
    "\n",
    "permtest_seed = \"none\"\n",
    "if permtest_seed != \"none\":\n",
    "    permtest_seed = int(permtest_seed)\n",
    "    conf_permtest = \"_PT\" + str(permtest_seed)\n",
    "    need_training_bl = True\n",
    "    bl_using_selflast = False\n",
    "    bl_using_selfbest = True\n",
    "    bl_using_cvbests = False\n",
    "    show_model = False\n",
    "else:\n",
    "    permtest_seed = None\n",
    "    conf_permtest = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b24a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(os.path.dirname(os.path.abspath('288_test.ipynb')), 'saved_models_st_10cv/')\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "if save_ds:\n",
    "    outdata_dir = os.path.join(save_dir, conf_ds)\n",
    "    if not os.path.isdir(outdata_dir):\n",
    "        os.makedirs(outdata_dir)\n",
    "\n",
    "log_dir = os.path.join(save_dir, conf_tags)\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "callbacks_dl = []\n",
    "\n",
    "if not use_clr:\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    callbacks_dl.append(lr_scheduler)\n",
    "else:\n",
    "    callbacks_dl.append(None)\n",
    "\n",
    "verbose_callback = False\n",
    "save_histweights = False\n",
    "model_name_custom = '%s_%s-b%03d.e%03d-l%0.3fd-a%0.3f-tl%0.3f-ta%0.3f-trp%0.3f-tf%0.3f-tw%0.3f.h5'\n",
    "filepath_custom = os.path.join(save_dir, model_name_custom)\n",
    "custom_checkpoint = CustomModelCheckpoint()\n",
    "callbacks_dl.append(custom_checkpoint)\n",
    "\n",
    "lr_logger = LearningRateScheduler(lr_log)\n",
    "callbacks_dl += [lr_logger]\n",
    "\n",
    "if tb_enabled:\n",
    "    tb_dir = os.path.join(save_dir, conf_tags + \"_tb\")\n",
    "    if not os.path.isdir(tb_dir):\n",
    "        os.mkdir(tb_dir)\n",
    "    tb_callback = keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
    "                                              write_graph=False,\n",
    "                                              batch_size=batch_size,\n",
    "                                              histogram_freq=epochs // 10,\n",
    "                                              write_grads=True,\n",
    "                                              write_images=False,\n",
    "                                              update_freq='epoch',\n",
    "                                              embeddings_freq=0,\n",
    "                                              embeddings_layer_names=None,\n",
    "                                              embeddings_data=None,\n",
    "                                              )\n",
    "    callbacks_dl += [tb_callback]\n",
    "    if sys.platform.startswith(\"win\"):\n",
    "        os.system(\"start tensorboard --logdir %s\" % tb_dir)\n",
    "    else:\n",
    "        os.system(\"setsid tensorboard --logdir %s >%s/stnew-tblog.txt 2>&1 &\")\n",
    "\n",
    "act_cnn = \"AC.LeakyReLU()\"\n",
    "\n",
    "act_ds = \"AC.LeakyReLU()\"\n",
    "\n",
    "bn_ct = True\n",
    "bn_sc = True\n",
    "rnn_impl = 0\n",
    "unrolled = True\n",
    "\n",
    "k_init = 'he_uniform'\n",
    "\n",
    "k_regu = None\n",
    "a_regu = None\n",
    "\n",
    "def nl_denselayer(x_in, n_filter_dense, dense_drop_rate, triple_branch=False, branch_add_type=\"old\", pre_act=False, concat_x=True, fully_concat=False, bn=True):\n",
    "    if fully_concat:\n",
    "        if triple_branch:\n",
    "            n_branch = 3\n",
    "        else:\n",
    "            n_branch = 2\n",
    "        if n_filter_dense % n_branch != 0:\n",
    "            return None\n",
    "        n_filter_internal = n_filter_dense // n_branch\n",
    "    else:\n",
    "        n_filter_internal = n_filter_dense\n",
    "\n",
    "    x0 = x_in\n",
    "    if not pre_act:\n",
    "        xr = x0\n",
    "    else:\n",
    "        xr = Activation('relu')(x0)\n",
    "    x0 = xr\n",
    "    xr = Dense(n_filter_internal, activation=\"linear\")(xr)\n",
    "    xr = Activation('relu')(xr)\n",
    "    xr = BatchNormalization(momentum=0.8)(xr)\n",
    "\n",
    "    if fully_concat:\n",
    "        xl1 = Dense(n_filter_internal, activation=\"linear\")(x0)\n",
    "    else:\n",
    "        if branch_add_type == \"old\":\n",
    "            xl1 = x0\n",
    "        else:\n",
    "            xl1 = Dense(n_filter_internal, activation=\"linear\")(x0)\n",
    "    xl2 = Dense(n_filter_internal, activation=\"linear\")(x0)\n",
    "    xm = Multiply()([xl1, xl2])\n",
    "    xm = BatchNormalization(momentum=0.8)(xm)\n",
    "\n",
    "    xt = Dense(n_filter_internal, activation=\"linear\")(x0)\n",
    "    xt = Activation('tanh')(xt)\n",
    "    xs = Dense(n_filter_internal, activation=\"linear\")(x0)\n",
    "    xs = Activation('sigmoid')(xs)\n",
    "    xts = Multiply()([xt, xs])\n",
    "\n",
    "    if not fully_concat:\n",
    "        if triple_branch:\n",
    "            xn = Add()([xr, xm, xts])\n",
    "        else:\n",
    "            xn = Add()([xm, xts])\n",
    "    else:\n",
    "        if triple_branch:\n",
    "            xn = Concatenate()([xr, xm, xts])\n",
    "        else:\n",
    "            xn = Concatenate()([xm, xts])\n",
    "    if bn:\n",
    "        xn = BatchNormalization(momentum=0.8)(xn)\n",
    "    if not concat_x:\n",
    "        x = Add()([x0, xn])\n",
    "    else:\n",
    "        x = Concatenate()([x0, xn])\n",
    "    x = Dropout(dense_drop_rate)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "global model_showed\n",
    "model_showed = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d41ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dlmodel(m_name=None):\n",
    "    np.random.seed(random_state_tf)\n",
    "    tf.compat.v1.set_random_seed(random_state_tf)\n",
    "\n",
    "    if m_name is None:\n",
    "        m_name = m_dl\n",
    "\n",
    "    if opt_name==\"adam\":\n",
    "        optimizer = Adam(lr=lr_init, beta_1=beta_1)\n",
    "    elif opt_name==\"adadelta\":\n",
    "        optimizer = keras.optimizers.Adadelta()\n",
    "    else:\n",
    "        optimizer = SGD(lr=lr_init, momentum=0.9, decay=sgd_decay, nesterov=True)\n",
    "\n",
    "    losses_used=\"categorical\"\n",
    "    metrics_show = ['accuracy']\n",
    "\n",
    "    #==========================================\n",
    "    if m_name==\"mnl\":\n",
    "        x_in = Input(shape=(img_rows,img_cols,1))\n",
    "\n",
    "        n_filter_cur = 1024\n",
    "        dense_drop_rate = 0.5\n",
    "\n",
    "        x = x_in\n",
    "        x = Flatten()(x)\n",
    "        for i in range(1):\n",
    "            for j in range(1): \n",
    "                x = nl_denselayer(x,int(n_filter_cur//1*1.0),dense_drop_rate,\n",
    "                                  triple_branch=True,\n",
    "                                  pre_act=False,concat_x=False,fully_concat=False,\n",
    "                                  bn=False)\n",
    "        x1 = x\n",
    "\n",
    "        x = x_in\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        for i in range(1):\n",
    "\n",
    "            for j in range(2):\n",
    "                x = nl_denselayer(x,int(n_filter_cur//1*1.0),dense_drop_rate,\n",
    "                                  triple_branch=True,\n",
    "                                  pre_act=False,concat_x=False,fully_concat=False,\n",
    "                                  bn=False)\n",
    "        x2 = x\n",
    "\n",
    "        x = x_in\n",
    "        x = Flatten()(x)\n",
    "        for i in range(1):\n",
    "\n",
    "            for j in range(3):\n",
    "                x = nl_denselayer(x,int(n_filter_cur//1*1.0),dense_drop_rate,\n",
    "                                  triple_branch=True,\n",
    "                                  pre_act=False,concat_x=False,fully_concat=False,\n",
    "                                  bn=False)\n",
    "        x3 = x\n",
    "\n",
    "        x = x_in\n",
    "        x = Flatten()(x)\n",
    "        for i in range(1):\n",
    "\n",
    "            for j in range(4):\n",
    "                x = nl_denselayer(x,int(n_filter_cur//1*1.0),dense_drop_rate,\n",
    "                                  triple_branch=True,\n",
    "                                  pre_act=False,concat_x=False,fully_concat=False, \n",
    "                                  bn=False)\n",
    "        x4 = x\n",
    "\n",
    "        x = x2\n",
    "\n",
    "        prediction = Dense(num_classes,activation=\"softmax\",\n",
    "                           )(x)\n",
    "        model = Model(x_in, prediction)\n",
    "    #==========================================\n",
    "\n",
    "    if m_name==\"mlc\":\n",
    "        x_in = Input(shape=(img_rows,img_cols,1))\n",
    "        x_top = Reshape((img_rows*img_cols,1))(x_in)\n",
    "\n",
    "        x = x_top\n",
    "        x = LocallyConnected1D(10, 30, padding=\"valid\", strides=30)(x) #6/30 15\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(4, 10, padding=\"valid\", strides=10)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(2, 4, padding=\"valid\", strides=4)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(1, 2, padding=\"valid\", strides=2)(x)\n",
    "        x1 = x\n",
    "\n",
    "        x = x_top\n",
    "        x = LocallyConnected1D(10, 36, padding=\"valid\", strides=36)(x) #6/30 15\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(3, 10, padding=\"valid\", strides=10)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(1, 3, padding=\"valid\", strides=3)(x)\n",
    "        x2 = x\n",
    "\n",
    "        x = x_top\n",
    "        x = LocallyConnected1D(9, 25, padding=\"valid\", strides=25)(x) #6/30 15\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(4, 9, padding=\"valid\", strides=9)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(2, 4, padding=\"valid\", strides=4)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(1, 2, padding=\"valid\", strides=2)(x)\n",
    "        x3 = x\n",
    "\n",
    "        x = x_top\n",
    "        x = LocallyConnected1D(8, 18, padding=\"valid\", strides=18)(x) #6/30 15\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(4, 8, padding=\"valid\", strides=8)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(2, 4, padding=\"valid\", strides=4)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(1, 2, padding=\"valid\", strides=2)(x)\n",
    "        x4 = x\n",
    "\n",
    "        x = x_top\n",
    "        x = LocallyConnected1D(12, 45, padding=\"valid\", strides=45)(x) #6/30 15\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(4, 12, padding=\"valid\", strides=12)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(2, 4, padding=\"valid\", strides=4)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(1, 2, padding=\"valid\", strides=2)(x)\n",
    "        x5 = x\n",
    "\n",
    "        x1 = Dropout(0.25)(x1)\n",
    "        x2 = Dropout(0.25)(x2)\n",
    "        x3 = Dropout(0.25)(x3)\n",
    "        x4 = Dropout(0.25)(x4)\n",
    "        x5 = Dropout(0.25)(x5)\n",
    "        x1 = Flatten()(x1)\n",
    "        x2 = Flatten()(x2)\n",
    "        x3 = Flatten()(x3)\n",
    "        x4 = Flatten()(x4)\n",
    "        x5 = Flatten()(x5)\n",
    "        x = Concatenate()([x1,x2,x3,x4,x5])\n",
    "\n",
    "        z = x\n",
    "\n",
    "        prediction = Dense(num_classes,\n",
    "                  kernel_initializer=k_init,\n",
    "                  activation=\"softmax\")(z)\n",
    "        model = Model(x_in, prediction)\n",
    "    #==========================================\n",
    "\n",
    "    #==========================================\n",
    "    if m_name==\"mlcn2\":\n",
    "        x_in = Input(shape=(img_rows,img_cols,1))\n",
    "        x_top = Reshape((-1,1))(x_in)\n",
    "\n",
    "        x = x_top\n",
    "        x = LocallyConnected1D(10, 30, padding=\"valid\", strides=30)(x) #6/30 15\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(4, 10, padding=\"valid\", strides=10)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(2, 4, padding=\"valid\", strides=4)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(1, 2, padding=\"valid\", strides=2)(x)\n",
    "        x1 = x\n",
    "\n",
    "        x = x_top\n",
    "        x = LocallyConnected1D(10, 36, padding=\"valid\", strides=36)(x) #6/30 15\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(3, 10, padding=\"valid\", strides=10)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(1, 3, padding=\"valid\", strides=3)(x)\n",
    "        x2 = x\n",
    "\n",
    "        x = x_top\n",
    "        x = LocallyConnected1D(9, 25, padding=\"valid\", strides=25)(x) #6/30 15\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(4, 9, padding=\"valid\", strides=9)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(2, 4, padding=\"valid\", strides=4)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(1, 2, padding=\"valid\", strides=2)(x)\n",
    "        x3 = x\n",
    "\n",
    "        x = x_top\n",
    "        x = LocallyConnected1D(8, 18, padding=\"valid\", strides=18)(x) #6/30 15\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(4, 8, padding=\"valid\", strides=8)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(2, 4, padding=\"valid\", strides=4)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(1, 2, padding=\"valid\", strides=2)(x)\n",
    "        x4 = x\n",
    "\n",
    "        x = x_top\n",
    "        x = LocallyConnected1D(12, 45, padding=\"valid\", strides=45)(x) #6/30 15\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(4, 12, padding=\"valid\", strides=12)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(2, 4, padding=\"valid\", strides=4)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(1, 2, padding=\"valid\", strides=2)(x)\n",
    "        x5 = x\n",
    "\n",
    "        x1 = Flatten()(x1)\n",
    "        x2 = Flatten()(x2)\n",
    "        x3 = Flatten()(x3)\n",
    "        x4 = Flatten()(x4)\n",
    "        x5 = Flatten()(x5)\n",
    "\n",
    "        x0 = x1\n",
    "        x0d = Dropout(0.25)(x0)\n",
    "        x0t = Dense(34,activation=\"tanh\")(x0)\n",
    "        x0s = Dense(34,activation=\"sigmoid\")(x0)\n",
    "        x0l = Dense(34,activation=\"linear\")(x0)\n",
    "        xm = Multiply()([x0,x0l])\n",
    "        xm = Dropout(0.25)(xm)\n",
    "        xts = Multiply()([x0t,x0s])\n",
    "        xts = Dropout(0.25)(xts)\n",
    "        x1 = Add()([x0d,xm,xts])\n",
    "\n",
    "        x0 = x2\n",
    "        x0d = Dropout(0.25)(x0)\n",
    "        x0t = Dense(28,activation=\"tanh\")(x0)\n",
    "        x0s = Dense(28,activation=\"sigmoid\")(x0)\n",
    "        x0l = Dense(28,activation=\"linear\")(x0)\n",
    "        xm = Multiply()([x0,x0l])\n",
    "        xm = Dropout(0.25)(xm)\n",
    "        xts = Multiply()([x0t,x0s])\n",
    "        xts = Dropout(0.25)(xts)\n",
    "        x2 = Add()([x0d,xm,xts])\n",
    "\n",
    "        x0 = x3\n",
    "        x0d = Dropout(0.25)(x0)\n",
    "        x0t = Dense(40,activation=\"tanh\")(x0)\n",
    "        x0s = Dense(40,activation=\"sigmoid\")(x0)\n",
    "        x0l = Dense(40,activation=\"linear\")(x0)\n",
    "        xm = Multiply()([x0,x0l])\n",
    "        xm = Dropout(0.25)(xm)\n",
    "        xts = Multiply()([x0t,x0s])\n",
    "        xts = Dropout(0.25)(xts)\n",
    "        x3 = Add()([x0d,xm,xts])\n",
    "\n",
    "        x0 = x4\n",
    "        x0d = Dropout(0.25)(x0)\n",
    "        x0t = Dense(56,activation=\"tanh\")(x0)\n",
    "        x0s = Dense(56,activation=\"sigmoid\")(x0)\n",
    "        x0l = Dense(56,activation=\"linear\")(x0)\n",
    "        xm = Multiply()([x0,x0l])\n",
    "        xm = Dropout(0.25)(xm)\n",
    "        xts = Multiply()([x0t,x0s])\n",
    "        xts = Dropout(0.25)(xts)\n",
    "        x4 = Add()([x0d,xm,xts])\n",
    "\n",
    "        x0 = x5\n",
    "        x0d = Dropout(0.25)(x0)\n",
    "        x0t = Dense(22,activation=\"tanh\")(x0)\n",
    "        x0s = Dense(22,activation=\"sigmoid\")(x0)\n",
    "        x0l = Dense(22,activation=\"linear\")(x0)\n",
    "        xm = Multiply()([x0,x0l])\n",
    "        xm = Dropout(0.25)(xm)\n",
    "        xts = Multiply()([x0t,x0s])\n",
    "        xts = Dropout(0.25)(xts)\n",
    "        x5 = Add()([x0d,xm,xts])\n",
    "\n",
    "        x_p1 = Concatenate()([x1,x2,x3,x4,x5])\n",
    "\n",
    "        x0 = x_p1\n",
    "        x0d = Dropout(0.25)(x0)\n",
    "        x0t = Dense(180,activation=\"tanh\")(x0)\n",
    "        x0s = Dense(180,activation=\"sigmoid\")(x0)\n",
    "        x0l = Dense(180,activation=\"linear\")(x0)\n",
    "        xm = Multiply()([x0,x0l])\n",
    "        xm = Dropout(0.25)(xm)\n",
    "        xts = Multiply()([x0t,x0s])\n",
    "        xts = Dropout(0.25)(xts)\n",
    "        x_p1 = Add()([x0d,xm,xts])\n",
    "        x = x_p1\n",
    "\n",
    "        x = Activation('relu')(x) \n",
    "        x = Dropout(0.25)(x) \n",
    "\n",
    "        z = x\n",
    "\n",
    "        prediction = Dense(num_classes,\n",
    "                   kernel_initializer=k_init,\n",
    "                   activation=\"softmax\"\n",
    "                  )(z)\n",
    "        model = Model(x_in, prediction)\n",
    "    #==========================================\n",
    "\n",
    "    ##==========================================\n",
    "    if m_name==\"ml2c12\":\n",
    "        drop_rate = 0.25 \n",
    "        mid_drop_rate = 0.25\n",
    "        dense_drop_rate = 0.25 \n",
    "\n",
    "        n_filters_c12 = 64 \n",
    "        drop_rate_c12 = 0.25 \n",
    "        add_drop_rate_c12 = 0.25\n",
    "        dense_drop_rate_c12 = 0.25\n",
    "\n",
    "        x_in = Input(shape=(img_rows,img_cols,1))\n",
    "        x_top = Reshape((img_rows*img_cols,1))(x_in)\n",
    "\n",
    "        x = x_top\n",
    "        x = LocallyConnected1D(12, 32, padding=\"valid\", strides=32)(x) #6/30 15\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(5, 12, padding=\"valid\", strides=12)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(2, 5, padding=\"valid\", strides=5)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(1, 2, padding=\"valid\", strides=2)(x)\n",
    "        x1 = x\n",
    "\n",
    "        x = x_top\n",
    "        x = LocallyConnected1D(15, 42, padding=\"valid\", strides=42)(x) #6/30 15\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(6, 15, padding=\"valid\", strides=15)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(2, 6, padding=\"valid\", strides=6)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(1, 2, padding=\"valid\", strides=2)(x)\n",
    "        x2 = x\n",
    "\n",
    "        x = x_top\n",
    "        x = LocallyConnected1D(9, 22, padding=\"valid\", strides=22)(x) #6/30 15\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(4, 9, padding=\"valid\", strides=9)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(2, 4, padding=\"valid\", strides=4)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(1, 2, padding=\"valid\", strides=2)(x)\n",
    "        x3 = x\n",
    "\n",
    "        x = x_top\n",
    "        x = LocallyConnected1D(7, 18, padding=\"valid\", strides=18)(x) #6/30 15\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(3, 7, padding=\"valid\", strides=7)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(2, 3, padding=\"valid\", strides=3)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(1, 2, padding=\"valid\", strides=2)(x)\n",
    "        x4 = x\n",
    "\n",
    "        x = x_top\n",
    "        x = LocallyConnected1D(20, 46, padding=\"valid\", strides=46)(x) #6/30 15\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(7, 20, padding=\"valid\", strides=20)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(2, 7, padding=\"valid\", strides=7)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(1, 2, padding=\"valid\", strides=2)(x)\n",
    "        x5 = x\n",
    "\n",
    "        x1 = Dropout(drop_rate)(x1)\n",
    "        x2 = Dropout(drop_rate)(x2)\n",
    "        x3 = Dropout(drop_rate)(x3)\n",
    "        x4 = Dropout(drop_rate)(x4)\n",
    "        x5 = Dropout(drop_rate)(x5)\n",
    "        x1 = Flatten()(x1)\n",
    "        x2 = Flatten()(x2)\n",
    "        x3 = Flatten()(x3)\n",
    "        x4 = Flatten()(x4)\n",
    "        x5 = Flatten()(x5)\n",
    "        x1l = x1\n",
    "        x2l = x2\n",
    "        x3l = x3\n",
    "        x4l = x4\n",
    "        x5l = x5\n",
    "\n",
    "        triple_branch=False \n",
    "        branch_add_type=\"new\" \n",
    "        pre_act=False \n",
    "        concat_x=False \n",
    "        fully_concat=False \n",
    "        bn=False \n",
    "\n",
    "        n_filters_nl=32 \n",
    "        for i in range(1): \n",
    "            x1 = nl_denselayer(x1,n_filters_nl,mid_drop_rate,\n",
    "                               triple_branch=triple_branch,\n",
    "                               branch_add_type=branch_add_type,\n",
    "                               pre_act=pre_act,concat_x=concat_x,fully_concat=fully_concat,\n",
    "                               bn=bn) \n",
    "\n",
    "        n_filters_nl=24\n",
    "        for i in range(1): \n",
    "            x2 = nl_denselayer(x2,n_filters_nl,mid_drop_rate,\n",
    "                               triple_branch=triple_branch,\n",
    "                               branch_add_type=branch_add_type,\n",
    "                               pre_act=pre_act,concat_x=concat_x,fully_concat=fully_concat,\n",
    "                               bn=bn) \n",
    "\n",
    "        n_filters_nl=46\n",
    "        for i in range(1):\n",
    "            x3 = nl_denselayer(x3,n_filters_nl,mid_drop_rate,\n",
    "                               triple_branch=triple_branch,\n",
    "                               branch_add_type=branch_add_type,\n",
    "                               pre_act=pre_act,concat_x=concat_x,fully_concat=fully_concat,\n",
    "                               bn=bn)\n",
    "\n",
    "        n_filters_nl=56\n",
    "        for i in range(1): \n",
    "            x4 = nl_denselayer(x4,n_filters_nl,mid_drop_rate,\n",
    "                               triple_branch=triple_branch,\n",
    "                               branch_add_type=branch_add_type,\n",
    "                               pre_act=pre_act,concat_x=concat_x,fully_concat=fully_concat,\n",
    "                               bn=bn) \n",
    "\n",
    "        n_filters_nl=22\n",
    "        for i in range(1): \n",
    "            x5 = nl_denselayer(x5,n_filters_nl,mid_drop_rate,\n",
    "                               triple_branch=triple_branch,\n",
    "                               branch_add_type=branch_add_type,\n",
    "                               pre_act=pre_act,concat_x=concat_x,fully_concat=fully_concat,\n",
    "                               bn=bn)\n",
    "\n",
    "        x = Concatenate()([x1,x2,x3,x4,x5])\n",
    "\n",
    "        n_filters_nl=180\n",
    "        for i in range(1): \n",
    "            x = nl_denselayer(x,n_filters_nl,0,\n",
    "                              triple_branch=False,\n",
    "                              branch_add_type=\"new\",\n",
    "                              pre_act=False,concat_x=False,fully_concat=False,\n",
    "                              bn=False)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(dense_drop_rate)(x)\n",
    "        zmln2nb = x\n",
    "\n",
    "\n",
    "        x = Concatenate()([x1l,x2l,x3l,x4l,x5l])\n",
    "\n",
    "        n_filters_nl=180 \n",
    "        for i in range(1): \n",
    "            x = nl_denselayer(x,n_filters_nl,0,\n",
    "                              triple_branch=False,\n",
    "                              branch_add_type=\"old\",\n",
    "                              pre_act=False,concat_x=False,fully_concat=False, \n",
    "                              bn=True) \n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(dense_drop_rate)(x)\n",
    "        zmlob = x\n",
    "\n",
    "\n",
    "        x = x_top\n",
    "        x = Conv1D(n_filters_c12,16,padding='same',strides=4)(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dropout(drop_rate_c12)(x)\n",
    "        x = Conv1D(n_filters_c12,8,padding='same',strides=4)(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dropout(drop_rate_c12)(x)\n",
    "        x = Conv1D(n_filters_c12,4,padding='same',strides=4)(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dropout(drop_rate_c12)(x)\n",
    "        x = Flatten()(x)\n",
    "        z1 = x\n",
    "\n",
    "        x = Reshape((n_filters_c12,img_rows*img_cols//n_filters_c12,1))(x_in)\n",
    "        x = Conv2D(n_filters_c12,(3, 8),padding='same',strides=(1,4))(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dropout(drop_rate_c12)(x)\n",
    "        x = Conv2D(n_filters_c12,(3, 4),padding='same',strides=(2,4))(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dropout(drop_rate_c12)(x)\n",
    "        x = Conv2D(n_filters_c12,(3, 1),padding='same',strides=(2,1))(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dropout(drop_rate_c12)(x)\n",
    "        x = Flatten()(x)\n",
    "        z2 = x\n",
    "\n",
    "        x = Add()([z1,z2])\n",
    "        x = Dropout(add_drop_rate_c12)(x)\n",
    "        n_filters_dense_c12=n_filters_c12 \n",
    "        for i in range(1):\n",
    "            x = nl_denselayer(x,n_filters_dense_c12,dense_drop_rate_c12,\n",
    "                              triple_branch=False,\n",
    "                              branch_add_type=\"new\",\n",
    "                              pre_act=False,concat_x=True,fully_concat=False, \n",
    "                              bn=True) \n",
    "        z12 = x\n",
    "\n",
    "\n",
    "        x = Concatenate()([zmln2nb,zmlob,z12]) \n",
    "\n",
    "        prediction = Dense(num_classes,activation=\"softmax\")(x)\n",
    "        model = Model(x_in, prediction)\n",
    "\n",
    "    if m_name==\"mlite\":\n",
    "        n_filters_nl = 96 \n",
    "        drop_rate = 0.25 \n",
    "\n",
    "        x_in = Input(shape=(img_rows,img_cols,1))\n",
    "\n",
    "        x = Reshape((-1,1))(x_in)\n",
    "        x = LocallyConnected1D(18, 32, padding=\"valid\", strides=32)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(9, 18, padding=\"valid\", strides=18)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(5, 9, padding=\"valid\", strides=9)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(3, 5, padding=\"valid\", strides=5)(x)\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        x0 = x\n",
    "        x0d = Dropout(drop_rate)(x0)\n",
    "        x0t = Dense(n_filters_nl,activation=\"tanh\")(x0)\n",
    "        x0s = Dense(n_filters_nl,activation=\"sigmoid\")(x0)\n",
    "        x0l = Dense(n_filters_nl,activation=\"linear\")(x0)\n",
    "        xm = Multiply()([x0,x0l])\n",
    "        xm = Dropout(drop_rate)(xm)\n",
    "        xts = Multiply()([x0t,x0s])\n",
    "        xts = Dropout(drop_rate)(xts)\n",
    "        x = Add()([x0d,xm,xts])\n",
    "\n",
    "        prediction = Dense(num_classes,\n",
    "                   kernel_initializer=k_init,\n",
    "                   activation=\"softmax\"\n",
    "                  )(x)\n",
    "        model = Model(x_in, prediction)\n",
    "\n",
    "    if m_name==\"mlitev2\":\n",
    "        n_filters_nl = 128\n",
    "        drop_rate = 0.25 \n",
    "\n",
    "        x_in = Input(shape=(img_rows,img_cols,1))\n",
    "        x_top = Reshape((img_rows*img_cols,1))(x_in)\n",
    "\n",
    "        x = Reshape((-1,1))(x_top)\n",
    "        x = LocallyConnected1D(16, 32, padding=\"valid\", strides=32)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(8, 16, padding=\"valid\", strides=16)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x = LocallyConnected1D(4, 8, padding=\"valid\", strides=8)(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dropout(drop_rate)(x)\n",
    "\n",
    "        for i in range(1): \n",
    "            x = nl_denselayer(x,n_filters_nl,drop_rate,\n",
    "                              triple_branch=True,\n",
    "                              branch_add_type=\"old\",\n",
    "                              pre_act=False,concat_x=False,fully_concat=False,\n",
    "                              bn=False) \n",
    "\n",
    "\n",
    "        prediction = Dense(num_classes,activation=\"softmax\")(x)\n",
    "        model = Model(x_in, prediction)\n",
    "    #==========================================\n",
    "    if m_name==\"mglr\":\n",
    "        x_in = Input(shape=(img_rows,img_cols,1))\n",
    "        x_top = Reshape((-1,1))(x_in)\n",
    "\n",
    "        x = x_top\n",
    "        x = LocallyConnected1D(15, 32, padding=\"valid\", strides=32)(x)\n",
    "        x = Reshape((-1,1))(x)\n",
    "        x1 = LocallyConnected1D(10, 15, padding=\"valid\", strides=15)(x)\n",
    "\n",
    "        x1 = Activation(\"relu\")(x1)\n",
    "        x1 = Conv1D(32, 1, padding='same', strides=1)(x1)\n",
    "        x1 = Conv1D(32, 1, padding='same', strides=1)(x1)\n",
    "\n",
    "        x = x_top\n",
    "        x = Conv1D(32, 32, padding='same', strides=8)(x)\n",
    "        x2 = Conv1D(32, 16, padding='same', strides=4)(x)\n",
    "\n",
    "        x2 = Activation(\"relu\")(x2)\n",
    "\n",
    "        x = Concatenate()([x1,x2])\n",
    "        x = Bidirectional(LSTM(32,\n",
    "                 return_sequences=True))(x)\n",
    "        x = Conv1D(32, 3, padding='same', strides=3)(x)\n",
    "        x = Conv1D(32, 3, padding='same', strides=3)(x)\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        z = x\n",
    "\n",
    "        prediction = Dense(num_classes,\n",
    "                   kernel_initializer=k_init,\n",
    "                   activation=\"softmax\"\n",
    "                  )(z)\n",
    "        model = Model(x_in, prediction)\n",
    "\n",
    "    global model_showed\n",
    "    if show_model and not model_showed:\n",
    "        model.summary()\n",
    "\n",
    "        model_showed = True\n",
    "\n",
    "    if losses_used==\"sparse\":\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss=keras.losses.sparse_categorical_crossentropy,\n",
    "                      metrics=metrics_show)\n",
    "    if losses_used==\"categorical\":\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=metrics_show)\n",
    "    return model\n",
    "\n",
    "model = None\n",
    "\n",
    "\n",
    "crop_prob = -0.01\n",
    "crop_factor = 0.80\n",
    "crop_factor_ub = 1.0 \n",
    "crop_fill_mode = \"constant\"\n",
    "crop_strict=False\n",
    "crop_preserve_range=False \n",
    "crop_anti_aliasing=False \n",
    "\n",
    "from random_eraser import get_random_eraser\n",
    "random_eraser = get_random_eraser(p=0.85, \n",
    "                                  s_l=0.02, s_h=0.4,\n",
    "                                  r_1=0.3, r_2=1/0.3, \n",
    "                                  v_l=-1.0*minmax_scale_factor/1.0, \n",
    "                                  v_h= 1.0*minmax_scale_factor/1.0,\n",
    "                                  pixel_level=True) \n",
    "\n",
    "zoom_fill_mode = \"constant\"  \n",
    "zoom_fill_val = -1*minmax_scale_factor if minmax_scale else 0\n",
    "\n",
    "def random_crop_image(image):\n",
    "    skip_crop = np.random.random()\n",
    "    if skip_crop<=(1-crop_prob):\n",
    "        image_crop = image\n",
    "    else:\n",
    "        random_array = np.random.random(size=4)\n",
    "        height,width,channels = image.shape\n",
    "        if not crop_strict:\n",
    "            w = int(width*(crop_factor+random_array[0]*(crop_factor_ub-crop_factor)))\n",
    "        else:\n",
    "            w = int((width*crop_factor)*(1+random_array[0]*(1-crop_factor)))\n",
    "        h = w\n",
    "        x = int(random_array[2]*(width-w))\n",
    "        y = int(random_array[3]*(height-h))\n",
    "        image_crop = image[y:h+y,x:w+x,0:channels]\n",
    "        image_crop = skimage.transform.resize(image_crop,image.shape,\n",
    "                                              preserve_range=crop_preserve_range,\n",
    "                                              anti_aliasing=crop_anti_aliasing,\n",
    "                                              mode=crop_fill_mode,\n",
    "                                              )\n",
    "    image_crop=random_eraser(image_crop)\n",
    "    return image_crop\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False, \n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False, \n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False, \n",
    "    zca_epsilon=1e-06,\n",
    "\n",
    "    fill_mode=zoom_fill_mode,\n",
    "    cval=zoom_fill_val, \n",
    "    brightness_range=None, \n",
    "    channel_shift_range=0.0, \n",
    "    preprocessing_function=random_crop_image, \n",
    "    rescale=None,\n",
    "    data_format='channels_last',\n",
    "    validation_split=0.0,\n",
    "    dtype=np.float32)\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    import xgboost as xgb\n",
    "\n",
    "eval_metric = [\"merror\"] \n",
    "\n",
    "param_xgb = {'max_depth':7, 'learning_rate':0.02, \n",
    "         'silent':True, 'verbose':0, 'seed':2100,\n",
    "         'objective':'multi:softmax','num_class':num_classes,\n",
    "         'booster':'gbtree',\n",
    "         'grow_policy':'lossguide', \n",
    "         'alpha': 0.0001, 'lambda': 0.5, \n",
    "         'gamma':0.001, 'min_child_weight':1.0, \n",
    "         'subsample':0.8, 'colsample_bytree':0.8, \n",
    "         'scale_pos_weight':1,'max_delta_step':1,\n",
    "         'nthread':4, 'eval_metric':eval_metric}\n",
    "\n",
    "num_round_xgb = 625\n",
    "ntree_limit = 0\n",
    "\n",
    "\n",
    "random_state = 2100\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=150, criterion='gini',\n",
    "                       max_depth=5, min_samples_split=5, oob_score=False,\n",
    "                       random_state=random_state)\n",
    "\n",
    "gbdt = GradientBoostingClassifier(n_estimators=500, max_depth=2, max_features=\"auto\",\n",
    "                          learning_rate=0.4, subsample=1.0,random_state=random_state)\n",
    "\n",
    "feed=linear_model.LogisticRegression(dual=False,C=100,penalty='l2',n_jobs=1,\n",
    "                                     max_iter=5000,solver=\"lbfgs\",random_state=random_state)\n",
    "ada = AdaBoostClassifier(feed,algorithm=\"SAMME.R\", \n",
    "                              n_estimators=100,learning_rate=0.1,\n",
    "                              random_state=random_state)\n",
    "\n",
    "feed=linear_model.LogisticRegression(dual=False,C=10.2,penalty='l1',n_jobs=1,\n",
    "                                     max_iter=5000,solver=\"liblinear\",random_state=random_state)\n",
    "\n",
    "bg=BaggingClassifier(base_estimator=feed, n_estimators=20,\n",
    "                     max_samples=1.0, max_features=1.0,\n",
    "                     bootstrap=True, bootstrap_features=True,\n",
    "                     oob_score=False, warm_start=False,\n",
    "                     n_jobs=4, random_state=random_state, verbose=0)\n",
    "\n",
    "clf1=NuSVC(nu=0.2,kernel='linear',gamma=0.25,probability=True,random_state=random_state)\n",
    "\n",
    "clf2=linear_model.LogisticRegression(dual=False,C=0.02,penalty='l1',n_jobs=1,\n",
    "                                     max_iter=5000,solver=\"liblinear\",random_state=random_state)\n",
    "\n",
    "clf5=linear_model.LogisticRegression(dual=False,C=0.0004,penalty='l2',n_jobs=1,\n",
    "                                     max_iter=5000,solver=\"lbfgs\",random_state=random_state)\n",
    "\n",
    "clf6=SVC(kernel=\"rbf\",gamma=0.002,C=15.0,probability=True,random_state=random_state)\n",
    "\n",
    "clf8_fx0 = VarianceThreshold(threshold=0.01)\n",
    "clf8_fx1 = PolynomialFeatures(degree=2, include_bias=False)\n",
    "clf8_fx2 = PCA(n_components=120,whiten=False,random_state=random_state)\n",
    "\n",
    "clf8_feed=linear_model.LogisticRegression(dual=False,C=0.5,penalty='l2',n_jobs=1,\n",
    "                                          max_iter=5000,solver=\"lbfgs\",random_state=random_state)\n",
    "\n",
    "clf8=Pipeline(steps=[\n",
    "        ('fx2', clf8_fx2),\n",
    "        ('clf', clf8_feed)])\n",
    "\n",
    "\n",
    "vc=VotingClassifier(estimators=[('l1lg', clf2), ],\n",
    "\n",
    "                        voting='soft') \n",
    "ml=vc\n",
    "\n",
    "def load_hdxdata(fs_idxs,\n",
    "                 minmax_scale=True, \n",
    "                 split_mode=\"RND\",\n",
    "\n",
    "                 hdx_seed=2100,\n",
    "                 n_splits=5,\n",
    "                 split_no=2,\n",
    "                 dn_factor=None): \n",
    "\n",
    "    def split_fromcv(df_mb):\n",
    "        kf_out = StratifiedKFold(\n",
    "                n_splits=n_splits,shuffle=True,random_state=hdx_seed)\n",
    "        splits_out = []\n",
    "        for tridx_out,tsidx_out in kf_out.split(df_mb,df_mb[\"label\"].values):\n",
    "            splits_out.append((tridx_out,tsidx_out))\n",
    "        ids_mb_train = splits_out[split_no][0]\n",
    "        ids_mb_test = splits_out[split_no][1]\n",
    "        df_train=df_mb.iloc[ids_mb_train,:].copy()\n",
    "        df_test=df_mb.iloc[ids_mb_test,:].copy()\n",
    "        return df_train,df_test,\n",
    "    \n",
    "    def dn_qxL1lgns(data_org):\n",
    "        def log_noscale(data):\n",
    "            data_ = np.copy(data)\n",
    "            e = np.ones(data_.shape[0])\n",
    "            for i in range(data_.shape[1]):\n",
    "                if np.min(data_[:,i]) != np.max(data_[:,i]):\n",
    "                    sign_col = np.sign(data_[:,i])\n",
    "                    data_[:,i] = np.log(e+np.abs(data_[:,i])) * sign_col\n",
    "            return data_\n",
    "\n",
    "        if dn_factor is None:\n",
    "            return data_org\n",
    "        if dn_factor>0:\n",
    "            qc=np.percentile(data_org,[dn_factor],axis=1).T\n",
    "            data_new=np.subtract(data_org,qc)\n",
    "            data_new=data_new.clip(min=0)\n",
    "        else:\n",
    "            data_new=data_org\n",
    "        data_new = preprocessing.normalize(data_new,norm=\"l1\",axis=1) *4000\n",
    "        data_new = log_noscale(data_new.T).T\n",
    "        data_new = data_new *0.02\n",
    "        return data_new.astype(np.float32)\n",
    "\n",
    "    df_org = pd.read_csv(indata_path)\n",
    "    df_train_new = df_org[df_org['m/z'].isin([f\"Discovery_{i}\" for i in range(1, 231)])].iloc[:,1:]\n",
    "    df_test_new = df_org[df_org['m/z'].isin([f\"Validation_{i}\" for i in range(1, 59)])].iloc[:,1:]\n",
    "\n",
    "    cols_org=np.array(df_org.columns[1:fs_num+1],dtype=str)\n",
    "    cols_zero=np.array([\"NULL%05d\"%(i+1)\\\n",
    "        for i in range(img_rows*img_cols-fs_num)],dtype=str)\n",
    "    cols_new=np.concatenate([cols_org,cols_zero])\n",
    "\n",
    "    if split_mode==\"BT\" or split_mode==\"CT\":\n",
    "        if split_mode==\"BT\":\n",
    "            bt_ct_train=bt_train\n",
    "            bt_ct_all=bt_all\n",
    "            bt_ct_col=\"batch\"\n",
    "\n",
    "        bt_ct_test=list(set(bt_ct_all)-set(bt_ct_train))\n",
    "        bt_ct_test.sort()\n",
    "        tr_list=[]\n",
    "        for bc in bt_ct_train:\n",
    "            tr_list.append(df_org.loc[df_org[bt_ct_col]==bc].copy())\n",
    "        ts_list=[]\n",
    "        for bc in bt_ct_test:\n",
    "            ts_list.append(df_org.loc[df_org[bt_ct_col]==bc].copy())\n",
    "        df_train=pd.concat(tr_list,axis=0)\n",
    "        df_test=pd.concat(ts_list,axis=0)\n",
    "\n",
    "    elif split_mode==\"RND\":\n",
    "        # df_train,df_test,=split_fromcv(df_org)\n",
    "        df_train = df_train_new\n",
    "        df_test = df_test_new\n",
    "\n",
    "        # df_test.to_csv(\"test_samples.csv\")\n",
    "        \n",
    "    elif split_mode==\"BTRND\" or split_mode==\"CTRND\":\n",
    "        if split_mode==\"BTRND\":\n",
    "            bt_ct_all=bt_all\n",
    "            bt_ct_col=\"batch\"\n",
    "\n",
    "        tr_list=[]\n",
    "        ts_list=[]\n",
    "        for bc in bt_ct_all:\n",
    "            df_train_bc,df_test_bc=\\\n",
    "                split_fromcv(df_org.loc[df_org[bt_ct_col]==bc])\n",
    "            tr_list.append(df_train_bc)\n",
    "            ts_list.append(df_test_bc)\n",
    "        df_train=pd.concat(tr_list,axis=0)\n",
    "        df_test=pd.concat(ts_list,axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    df_train=df_train.reset_index(drop=True)\n",
    "    df_test=df_test.reset_index(drop=True)\n",
    "    df_train_org=df_train.copy()\n",
    "    df_test_org=df_test.copy()\n",
    "\n",
    "    if not dn_factor is None:\n",
    "        if dn_factor>0:\n",
    "            print(\"dn:\",dn_factor)\n",
    "            x_train_core=np.array(df_train.iloc[:,:fs_core].values,dtype=np.float32)\n",
    "            x_train_core_dn=dn_qxL1lgns(x_train_core)\n",
    "            df_train.iloc[:,:fs_core]=x_train_core_dn\n",
    "            x_test_core=np.array(df_test.iloc[:,:fs_core].values,dtype=np.float32)\n",
    "            x_test_core_dn=dn_qxL1lgns(x_test_core)\n",
    "            df_test.iloc[:,:fs_core]=x_test_core_dn\n",
    "\n",
    "    fs_zero=list(set(np.arange(fs_num).tolist())-set(fs_idxs))\n",
    "    fs_zero.sort()\n",
    "\n",
    "    x_train_org=np.array(df_train.iloc[:,1:fs_num+1].copy().values,dtype=np.float32)\n",
    "    y_train=np.array(df_train[\"label\"].values,dtype=np.int32)\n",
    "\n",
    "    data_zeros=np.zeros([df_train.shape[0],\n",
    "                         img_rows*img_cols-fs_num],dtype=np.float32)\n",
    "    x_train=np.concatenate([x_train_org,data_zeros],axis=1)\n",
    "    x_train[:,fs_zero]=0.0\n",
    "\n",
    "    x_test_org=np.array(df_test.iloc[:,1:fs_num+1].copy().values,dtype=np.float32)\n",
    "    y_test=np.array(df_test[\"label\"].values,dtype=np.int32)\n",
    "\n",
    "    data_zeros=np.zeros([df_test.shape[0],\n",
    "                         img_rows*img_cols-fs_num],dtype=np.float32)\n",
    "    x_test=np.concatenate([x_test_org,data_zeros],axis=1)\n",
    "    x_test[:,fs_zero]=0.0\n",
    "\n",
    "    data_all = np.concatenate([x_train,x_test])\n",
    "    if minmax_scale:\n",
    "        data_all=preprocessing.minmax_scale(data_all,(-1,1))\n",
    "        data_all*=minmax_scale_factor\n",
    "    else:\n",
    "        data_all=data_all-data_all.mean(axis=0)\n",
    "    x_train=data_all[:df_train.shape[0],:]\n",
    "    x_test=data_all[df_train.shape[0]:,:]\n",
    "\n",
    "    df_train_std=pd.DataFrame(np.copy(x_train),columns=cols_new)\n",
    "    df_train_std[\"label\"]=y_train\n",
    "    df_test_std=pd.DataFrame(np.copy(x_test),columns=cols_new)\n",
    "    df_test_std[\"label\"]=y_test\n",
    "\n",
    "    return (x_train, y_train),(x_test, y_test),cols_new,(df_train_org,df_test_org),(df_train_std,df_test_std)\n",
    "\n",
    "\n",
    "\n",
    "def make_cv_splits(data_cv,label_cv,n_cv_splits,cv_seed):\n",
    "    kf_cv = StratifiedKFold(\n",
    "            n_splits=n_cv_splits,shuffle=True,random_state=cv_seed)\n",
    "    splits_cv = []\n",
    "    for tridx_cv,tsidx_cv in kf_cv.split(data_cv,label_cv):\n",
    "        x_train_org = data_cv[tridx_cv,:]\n",
    "        y_train_org = label_cv[tridx_cv]\n",
    "        x_test_org = data_cv[tsidx_cv,:]\n",
    "        y_test_org = label_cv[tsidx_cv]\n",
    "        splits_cv.append([x_train_org,y_train_org,\n",
    "                          x_test_org,y_test_org])\n",
    "    return splits_cv\n",
    "\n",
    "def fix_batch(batch_size,seed,x_train,y_train_flat,\n",
    "              train_ros=True,batch_fix=True):\n",
    "    if train_ros:\n",
    "        x_train_flat = x_train.reshape((x_train.shape[0],-1))\n",
    "        ros=RandomOverSampler(random_state=seed)\n",
    "\n",
    "        x_train_ros, y_train_ros = ros.fit_resample(x_train_flat, y_train_flat)\n",
    "        print('Over sampling:',x_train.shape[0],\"->\",x_train_ros.shape[0])\n",
    "        x_train_new=x_train_ros.reshape(\n",
    "                (x_train_ros.shape[0],img_rows,img_cols,channels))\n",
    "        y_train_new = y_train_ros\n",
    "    else:\n",
    "        x_train_new = x_train\n",
    "        y_train_new = y_train_flat\n",
    "\n",
    "    if batch_fix:\n",
    "        if x_train_new.shape[0]%batch_size!=0:\n",
    "            n_add=batch_size-x_train_new.shape[0]%batch_size\n",
    "            rnd=np.random.RandomState(seed)\n",
    "            if batch_fix_v2:\n",
    "                x_train_lb=x_train\n",
    "                y_train_lb=y_train_flat\n",
    "            else:\n",
    "                x_train_lb=x_train_new\n",
    "                y_train_lb=y_train_new\n",
    "            if not batch_fix_balance:\n",
    "                idxs_add=rnd.choice(x_train_lb.shape[0],n_add,replace=False)\n",
    "                x_tr_add=x_train_lb[idxs_add]\n",
    "                y_tr_add=y_train_lb[idxs_add]\n",
    "                x_train_new = np.concatenate([x_train_new,x_tr_add])\n",
    "                y_train_new = np.concatenate([y_train_new,y_tr_add])\n",
    "            else:\n",
    "                for i in range(2):\n",
    "                    lb_add=i\n",
    "                    if lb_add==0:\n",
    "                        n_add_lbc=n_add//2\n",
    "                    else:\n",
    "                        n_add_lbc=n_add-n_add_lbc\n",
    "                    ids_lbc=np.where(y_train_lb==lb_add)[0]\n",
    "                    x_train_lbc=x_train_lb[ids_lbc]\n",
    "                    y_train_lbc=y_train_lb[ids_lbc]\n",
    "                    idxs_add=rnd.choice(x_train_lbc.shape[0],n_add_lbc,replace=False)\n",
    "                    x_tr_add=x_train_lbc[idxs_add]\n",
    "                    y_tr_add=y_train_lbc[idxs_add]\n",
    "                    x_train_new = np.concatenate([x_train_new,x_tr_add])\n",
    "                    y_train_new = np.concatenate([y_train_new,y_tr_add])\n",
    "            print('Add train samples:',n_add)\n",
    "\n",
    "    return x_train_new, y_train_new\n",
    "\n",
    "\n",
    "##prepare data\n",
    "(data_cv,label_cv),(data_out,label_out),cols_new,(df_train,df_test),(df_train_std,df_test_std) = load_hdxdata(fs_idxs,\n",
    "                                                             minmax_scale=minmax_scale,\n",
    "                                                             split_mode=split_mode, \n",
    "                                                             hdx_seed=hdx_seed,\n",
    "                                                             n_splits=n_splits,\n",
    "                                                             split_no=split_no,\n",
    "                                                             dn_factor=dn_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e561e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((230, 1024), (230,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cv.shape, label_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49777e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>100.9624</th>\n",
       "      <th>103.0613</th>\n",
       "      <th>103.979</th>\n",
       "      <th>105.0359</th>\n",
       "      <th>105.9874</th>\n",
       "      <th>106.9899</th>\n",
       "      <th>108.9946</th>\n",
       "      <th>112.0954</th>\n",
       "      <th>112.9272</th>\n",
       "      <th>...</th>\n",
       "      <th>238.9339</th>\n",
       "      <th>242.0655</th>\n",
       "      <th>242.9152</th>\n",
       "      <th>249.1192</th>\n",
       "      <th>250.944</th>\n",
       "      <th>253.0242</th>\n",
       "      <th>254.9021</th>\n",
       "      <th>265.0862</th>\n",
       "      <th>268.9947</th>\n",
       "      <th>275.0035</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-285.712561</td>\n",
       "      <td>697.6210</td>\n",
       "      <td>-32384.23087</td>\n",
       "      <td>-4211.316744</td>\n",
       "      <td>-1727.094109</td>\n",
       "      <td>-2415.223748</td>\n",
       "      <td>-401.460</td>\n",
       "      <td>-227.813</td>\n",
       "      <td>-55035.61913</td>\n",
       "      <td>...</td>\n",
       "      <td>-571.724</td>\n",
       "      <td>-357.299</td>\n",
       "      <td>-327.097977</td>\n",
       "      <td>-196.399254</td>\n",
       "      <td>-125.484178</td>\n",
       "      <td>32375.2000</td>\n",
       "      <td>996.6690</td>\n",
       "      <td>-188.5170</td>\n",
       "      <td>5955.980</td>\n",
       "      <td>17333.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>220.929439</td>\n",
       "      <td>145.7970</td>\n",
       "      <td>10911.76913</td>\n",
       "      <td>1480.063257</td>\n",
       "      <td>521.789891</td>\n",
       "      <td>1192.876252</td>\n",
       "      <td>392.283</td>\n",
       "      <td>-60.814</td>\n",
       "      <td>-14662.21913</td>\n",
       "      <td>...</td>\n",
       "      <td>-232.758</td>\n",
       "      <td>-54.170</td>\n",
       "      <td>-87.760976</td>\n",
       "      <td>46.669746</td>\n",
       "      <td>-40.261178</td>\n",
       "      <td>-2410.0300</td>\n",
       "      <td>-153.4660</td>\n",
       "      <td>10.0724</td>\n",
       "      <td>-1173.150</td>\n",
       "      <td>-1341.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-545.166561</td>\n",
       "      <td>-644.9410</td>\n",
       "      <td>-27619.33087</td>\n",
       "      <td>-10449.276740</td>\n",
       "      <td>-1567.840109</td>\n",
       "      <td>-2593.703748</td>\n",
       "      <td>-851.215</td>\n",
       "      <td>-566.767</td>\n",
       "      <td>-26117.21913</td>\n",
       "      <td>...</td>\n",
       "      <td>-682.446</td>\n",
       "      <td>-391.911</td>\n",
       "      <td>-435.817976</td>\n",
       "      <td>-324.594954</td>\n",
       "      <td>-167.244478</td>\n",
       "      <td>-2869.2200</td>\n",
       "      <td>-321.2290</td>\n",
       "      <td>-232.0320</td>\n",
       "      <td>-562.987</td>\n",
       "      <td>-1923.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-290.889561</td>\n",
       "      <td>-428.6200</td>\n",
       "      <td>-27087.53087</td>\n",
       "      <td>-8542.156744</td>\n",
       "      <td>-1537.310109</td>\n",
       "      <td>-2019.353748</td>\n",
       "      <td>-630.989</td>\n",
       "      <td>-501.868</td>\n",
       "      <td>-26461.61913</td>\n",
       "      <td>...</td>\n",
       "      <td>-627.868</td>\n",
       "      <td>-376.629</td>\n",
       "      <td>-376.807976</td>\n",
       "      <td>-253.175254</td>\n",
       "      <td>-152.536278</td>\n",
       "      <td>17.6338</td>\n",
       "      <td>-276.4540</td>\n",
       "      <td>-188.5360</td>\n",
       "      <td>-119.657</td>\n",
       "      <td>-296.335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-47.238561</td>\n",
       "      <td>-412.8890</td>\n",
       "      <td>1903.96913</td>\n",
       "      <td>-5049.636744</td>\n",
       "      <td>-23.380109</td>\n",
       "      <td>1005.386252</td>\n",
       "      <td>-374.505</td>\n",
       "      <td>-374.237</td>\n",
       "      <td>1445.78087</td>\n",
       "      <td>...</td>\n",
       "      <td>-438.624</td>\n",
       "      <td>-353.307</td>\n",
       "      <td>-319.689977</td>\n",
       "      <td>-283.251754</td>\n",
       "      <td>-111.837178</td>\n",
       "      <td>-6269.5300</td>\n",
       "      <td>-245.5420</td>\n",
       "      <td>-200.0390</td>\n",
       "      <td>-1981.940</td>\n",
       "      <td>-3151.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>1</td>\n",
       "      <td>-299.254561</td>\n",
       "      <td>55.1307</td>\n",
       "      <td>-19776.73087</td>\n",
       "      <td>8118.463257</td>\n",
       "      <td>-1194.370109</td>\n",
       "      <td>-1008.983748</td>\n",
       "      <td>-238.881</td>\n",
       "      <td>-127.388</td>\n",
       "      <td>-63516.21913</td>\n",
       "      <td>...</td>\n",
       "      <td>-372.870</td>\n",
       "      <td>-59.663</td>\n",
       "      <td>-277.605976</td>\n",
       "      <td>159.690746</td>\n",
       "      <td>-109.173178</td>\n",
       "      <td>-4151.5600</td>\n",
       "      <td>-225.3170</td>\n",
       "      <td>-61.3006</td>\n",
       "      <td>-1849.740</td>\n",
       "      <td>-1785.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>1</td>\n",
       "      <td>452.299439</td>\n",
       "      <td>-117.3160</td>\n",
       "      <td>22300.46913</td>\n",
       "      <td>5984.263257</td>\n",
       "      <td>1303.929891</td>\n",
       "      <td>2560.986252</td>\n",
       "      <td>561.653</td>\n",
       "      <td>879.522</td>\n",
       "      <td>14800.78087</td>\n",
       "      <td>...</td>\n",
       "      <td>548.651</td>\n",
       "      <td>667.471</td>\n",
       "      <td>159.469023</td>\n",
       "      <td>1023.508746</td>\n",
       "      <td>51.685822</td>\n",
       "      <td>-4522.9500</td>\n",
       "      <td>-34.3318</td>\n",
       "      <td>793.2490</td>\n",
       "      <td>-1500.900</td>\n",
       "      <td>-2344.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>1</td>\n",
       "      <td>894.669439</td>\n",
       "      <td>1153.6000</td>\n",
       "      <td>41763.16913</td>\n",
       "      <td>16484.563260</td>\n",
       "      <td>2581.319891</td>\n",
       "      <td>3502.896252</td>\n",
       "      <td>1895.460</td>\n",
       "      <td>104.617</td>\n",
       "      <td>4259.78087</td>\n",
       "      <td>...</td>\n",
       "      <td>196.301</td>\n",
       "      <td>182.114</td>\n",
       "      <td>353.004024</td>\n",
       "      <td>417.276746</td>\n",
       "      <td>46.159822</td>\n",
       "      <td>-2776.1000</td>\n",
       "      <td>48.7202</td>\n",
       "      <td>111.6970</td>\n",
       "      <td>-1322.250</td>\n",
       "      <td>-1223.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>1</td>\n",
       "      <td>608.309439</td>\n",
       "      <td>61.7327</td>\n",
       "      <td>54363.16913</td>\n",
       "      <td>7218.363257</td>\n",
       "      <td>3201.549891</td>\n",
       "      <td>3777.346252</td>\n",
       "      <td>977.193</td>\n",
       "      <td>526.712</td>\n",
       "      <td>28188.78087</td>\n",
       "      <td>...</td>\n",
       "      <td>743.971</td>\n",
       "      <td>660.761</td>\n",
       "      <td>463.223024</td>\n",
       "      <td>398.156746</td>\n",
       "      <td>114.437822</td>\n",
       "      <td>-4054.0800</td>\n",
       "      <td>36.4182</td>\n",
       "      <td>519.4630</td>\n",
       "      <td>-1283.830</td>\n",
       "      <td>-2112.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>1</td>\n",
       "      <td>820.679439</td>\n",
       "      <td>405.9710</td>\n",
       "      <td>29650.46913</td>\n",
       "      <td>739.363257</td>\n",
       "      <td>1979.429891</td>\n",
       "      <td>3998.986252</td>\n",
       "      <td>1479.330</td>\n",
       "      <td>-204.763</td>\n",
       "      <td>16728.78087</td>\n",
       "      <td>...</td>\n",
       "      <td>-130.505</td>\n",
       "      <td>-246.531</td>\n",
       "      <td>173.609024</td>\n",
       "      <td>-179.789254</td>\n",
       "      <td>-18.633178</td>\n",
       "      <td>-6079.6000</td>\n",
       "      <td>-50.3028</td>\n",
       "      <td>-154.9860</td>\n",
       "      <td>-1916.100</td>\n",
       "      <td>-3089.800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 134 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label    100.9624   103.0613      103.979      105.0359     105.9874     106.9899  108.9946  112.0954     112.9272  ...  238.9339  242.0655    242.9152     249.1192     250.944    253.0242  254.9021  265.0862  268.9947   275.0035\n",
       "0        1 -285.712561   697.6210 -32384.23087  -4211.316744 -1727.094109 -2415.223748  -401.460  -227.813 -55035.61913  ...  -571.724  -357.299 -327.097977  -196.399254 -125.484178  32375.2000  996.6690 -188.5170  5955.980  17333.200\n",
       "1        1  220.929439   145.7970  10911.76913   1480.063257   521.789891  1192.876252   392.283   -60.814 -14662.21913  ...  -232.758   -54.170  -87.760976    46.669746  -40.261178  -2410.0300 -153.4660   10.0724 -1173.150  -1341.590\n",
       "2        1 -545.166561  -644.9410 -27619.33087 -10449.276740 -1567.840109 -2593.703748  -851.215  -566.767 -26117.21913  ...  -682.446  -391.911 -435.817976  -324.594954 -167.244478  -2869.2200 -321.2290 -232.0320  -562.987  -1923.830\n",
       "3        1 -290.889561  -428.6200 -27087.53087  -8542.156744 -1537.310109 -2019.353748  -630.989  -501.868 -26461.61913  ...  -627.868  -376.629 -376.807976  -253.175254 -152.536278     17.6338 -276.4540 -188.5360  -119.657   -296.335\n",
       "4        1  -47.238561  -412.8890   1903.96913  -5049.636744   -23.380109  1005.386252  -374.505  -374.237   1445.78087  ...  -438.624  -353.307 -319.689977  -283.251754 -111.837178  -6269.5300 -245.5420 -200.0390 -1981.940  -3151.190\n",
       "..     ...         ...        ...          ...           ...          ...          ...       ...       ...          ...  ...       ...       ...         ...          ...         ...         ...       ...       ...       ...        ...\n",
       "225      1 -299.254561    55.1307 -19776.73087   8118.463257 -1194.370109 -1008.983748  -238.881  -127.388 -63516.21913  ...  -372.870   -59.663 -277.605976   159.690746 -109.173178  -4151.5600 -225.3170  -61.3006 -1849.740  -1785.280\n",
       "226      1  452.299439  -117.3160  22300.46913   5984.263257  1303.929891  2560.986252   561.653   879.522  14800.78087  ...   548.651   667.471  159.469023  1023.508746   51.685822  -4522.9500  -34.3318  793.2490 -1500.900  -2344.090\n",
       "227      1  894.669439  1153.6000  41763.16913  16484.563260  2581.319891  3502.896252  1895.460   104.617   4259.78087  ...   196.301   182.114  353.004024   417.276746   46.159822  -2776.1000   48.7202  111.6970 -1322.250  -1223.580\n",
       "228      1  608.309439    61.7327  54363.16913   7218.363257  3201.549891  3777.346252   977.193   526.712  28188.78087  ...   743.971   660.761  463.223024   398.156746  114.437822  -4054.0800   36.4182  519.4630 -1283.830  -2112.070\n",
       "229      1  820.679439   405.9710  29650.46913    739.363257  1979.429891  3998.986252  1479.330  -204.763  16728.78087  ...  -130.505  -246.531  173.609024  -179.789254  -18.633178  -6079.6000  -50.3028 -154.9860 -1916.100  -3089.800\n",
       "\n",
       "[230 rows x 134 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[:,:fs_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "437ed5e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================CV[1]========================\n",
      "\n",
      "207 train samples\n",
      "23 test samples\n",
      "\n",
      "Over sampling: 207 -> 208\n",
      "Add train samples: 16\n",
      "224 new train samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\tf28\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1024, 1)      0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " locally_connected1d (LocallyCo  (None, 32, 18)      19008       ['reshape[0][0]']                \n",
      " nnected1D)                                                                                       \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 576, 1)       0           ['locally_connected1d[0][0]']    \n",
      "                                                                                                  \n",
      " locally_connected1d_1 (Locally  (None, 32, 9)       5472        ['reshape_1[0][0]']              \n",
      " Connected1D)                                                                                     \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 288, 1)       0           ['locally_connected1d_1[0][0]']  \n",
      "                                                                                                  \n",
      " locally_connected1d_2 (Locally  (None, 32, 5)       1600        ['reshape_2[0][0]']              \n",
      " Connected1D)                                                                                     \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 160, 1)       0           ['locally_connected1d_2[0][0]']  \n",
      "                                                                                                  \n",
      " locally_connected1d_3 (Locally  (None, 32, 3)       576         ['reshape_3[0][0]']              \n",
      " Connected1D)                                                                                     \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 96)           0           ['locally_connected1d_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 96)           9312        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 96)           9312        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 96)           9312        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 96)           0           ['flatten[0][0]',                \n",
      "                                                                  'dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 96)           0           ['dense[0][0]',                  \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 96)           0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 96)           0           ['multiply[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 96)           0           ['multiply_1[0][0]']             \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 96)           0           ['dropout[0][0]',                \n",
      "                                                                  'dropout_1[0][0]',              \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            194         ['add[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 54,786\n",
      "Trainable params: 54,786\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Learning rate: 0.00000010\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_28484\\4287042063.py:159: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================EPOCH 000001:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp   f1       rfa\n",
      "0    1.0  0.692519  0.495536  0.692947  0.478261  0.765152  0.0  0.396937\n",
      "RFA : 0.39693676070733497\n",
      "7/7 - 10s - loss: 0.6925 - accuracy: 0.4955 - val_loss: 0.6929 - val_accuracy: 0.4783 - lr: 2.0010e-04 - 10s/epoch - 1s/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 2/200\n",
      "======================EPOCH 000002:======================\n",
      "   epoch     loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "1    2.0  0.69419  0.477679  0.691336  0.521739  0.856061  0.685714  0.679427\n",
      "RFA : 0.6794268756563013\n",
      "7/7 - 0s - loss: 0.6942 - accuracy: 0.4777 - val_loss: 0.6913 - val_accuracy: 0.5217 - lr: 4.0010e-04 - 399ms/epoch - 57ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 3/200\n",
      "7/7 - 0s - loss: 0.6928 - accuracy: 0.4955 - val_loss: 0.6897 - val_accuracy: 0.4783 - lr: 6.0009e-04 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 4/200\n",
      "======================EPOCH 000004:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "3    4.0  0.689223  0.575893  0.678031  0.869565  0.977273  0.888889  0.908641\n",
      "RFA : 0.9086407454025867\n",
      "7/7 - 2s - loss: 0.6892 - accuracy: 0.5759 - val_loss: 0.6780 - val_accuracy: 0.8696 - lr: 8.0009e-04 - 2s/epoch - 249ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 5/200\n",
      "======================EPOCH 000005:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "4    5.0  0.677069  0.633929  0.626603  0.869565  0.992424  0.888889  0.913186\n",
      "RFA : 0.9131861999480411\n",
      "7/7 - 0s - loss: 0.6771 - accuracy: 0.6339 - val_loss: 0.6266 - val_accuracy: 0.8696 - lr: 0.0010 - 401ms/epoch - 57ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 6/200\n",
      "7/7 - 1s - loss: 0.6169 - accuracy: 0.6964 - val_loss: 0.4245 - val_accuracy: 0.8696 - lr: 0.0012 - 530ms/epoch - 76ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 7/200\n",
      "======================EPOCH 000007:======================\n",
      "   epoch      loss      acc  val_loss   val_acc      rocp        f1       rfa\n",
      "6    7.0  0.505418  0.78125  0.323738  0.913043  0.977273  0.909091  0.930929\n",
      "RFA : 0.9309288610111583\n",
      "7/7 - 2s - loss: 0.5054 - accuracy: 0.7812 - val_loss: 0.3237 - val_accuracy: 0.9130 - lr: 0.0014 - 2s/epoch - 225ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 8/200\n",
      "7/7 - 0s - loss: 0.4926 - accuracy: 0.7723 - val_loss: 0.2606 - val_accuracy: 0.8696 - lr: 0.0016 - 390ms/epoch - 56ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 9/200\n",
      "======================EPOCH 000009:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "8    9.0  0.436907  0.799107  0.238599  0.913043  0.992424  0.923077  0.940369\n",
      "RFA : 0.9403694204517177\n",
      "7/7 - 0s - loss: 0.4369 - accuracy: 0.7991 - val_loss: 0.2386 - val_accuracy: 0.9130 - lr: 0.0018 - 377ms/epoch - 54ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 10/200\n",
      "7/7 - 0s - loss: 0.3715 - accuracy: 0.8304 - val_loss: 0.2538 - val_accuracy: 0.8696 - lr: 0.0020 - 313ms/epoch - 45ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 11/200\n",
      "7/7 - 1s - loss: 0.3718 - accuracy: 0.8170 - val_loss: 0.2602 - val_accuracy: 0.8696 - lr: 0.0022 - 1s/epoch - 156ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 12/200\n",
      "7/7 - 0s - loss: 0.3937 - accuracy: 0.8304 - val_loss: 0.2523 - val_accuracy: 0.8696 - lr: 0.0024 - 483ms/epoch - 69ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 13/200\n",
      "7/7 - 1s - loss: 0.4088 - accuracy: 0.7946 - val_loss: 0.2389 - val_accuracy: 0.9130 - lr: 0.0026 - 1s/epoch - 193ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 14/200\n",
      "7/7 - 1s - loss: 0.3853 - accuracy: 0.8170 - val_loss: 0.2582 - val_accuracy: 0.8696 - lr: 0.0028 - 508ms/epoch - 73ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 15/200\n",
      "7/7 - 0s - loss: 0.3406 - accuracy: 0.8482 - val_loss: 0.2024 - val_accuracy: 0.9130 - lr: 0.0030 - 313ms/epoch - 45ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 16/200\n",
      "7/7 - 0s - loss: 0.3474 - accuracy: 0.8482 - val_loss: 0.2320 - val_accuracy: 0.9130 - lr: 0.0032 - 322ms/epoch - 46ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 17/200\n",
      "7/7 - 1s - loss: 0.3363 - accuracy: 0.8527 - val_loss: 0.2167 - val_accuracy: 0.9130 - lr: 0.0034 - 1s/epoch - 173ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 18/200\n",
      "7/7 - 0s - loss: 0.3339 - accuracy: 0.8527 - val_loss: 0.2241 - val_accuracy: 0.8696 - lr: 0.0036 - 479ms/epoch - 68ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 19/200\n",
      "7/7 - 0s - loss: 0.3636 - accuracy: 0.8080 - val_loss: 0.2766 - val_accuracy: 0.8696 - lr: 0.0038 - 309ms/epoch - 44ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 20/200\n",
      "7/7 - 0s - loss: 0.3453 - accuracy: 0.8482 - val_loss: 0.2330 - val_accuracy: 0.9130 - lr: 0.0040 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 21/200\n",
      "7/7 - 0s - loss: 0.3317 - accuracy: 0.8705 - val_loss: 0.2559 - val_accuracy: 0.8696 - lr: 0.0042 - 445ms/epoch - 64ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 22/200\n",
      "7/7 - 1s - loss: 0.3866 - accuracy: 0.8304 - val_loss: 0.2852 - val_accuracy: 0.8696 - lr: 0.0044 - 1s/epoch - 173ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 23/200\n",
      "======================EPOCH 000023:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp    f1       rfa\n",
      "22   23.0  0.397742  0.825893  0.204276  0.956522  0.977273  0.96  0.963964\n",
      "RFA : 0.9639644305055791\n",
      "7/7 - 1s - loss: 0.3977 - accuracy: 0.8259 - val_loss: 0.2043 - val_accuracy: 0.9565 - lr: 0.0046 - 1s/epoch - 180ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 24/200\n",
      "7/7 - 0s - loss: 0.3964 - accuracy: 0.7902 - val_loss: 0.2213 - val_accuracy: 0.8696 - lr: 0.0048 - 486ms/epoch - 69ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 25/200\n",
      "7/7 - 0s - loss: 0.3163 - accuracy: 0.8661 - val_loss: 0.2482 - val_accuracy: 0.8696 - lr: 0.0050 - 343ms/epoch - 49ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 26/200\n",
      "7/7 - 1s - loss: 0.4206 - accuracy: 0.7946 - val_loss: 0.2460 - val_accuracy: 0.8696 - lr: 0.0052 - 549ms/epoch - 78ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 27/200\n",
      "7/7 - 0s - loss: 0.3450 - accuracy: 0.8571 - val_loss: 0.1975 - val_accuracy: 0.9130 - lr: 0.0054 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 28/200\n",
      "7/7 - 0s - loss: 0.3060 - accuracy: 0.8571 - val_loss: 0.1758 - val_accuracy: 0.9130 - lr: 0.0056 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 29/200\n",
      "7/7 - 0s - loss: 0.3403 - accuracy: 0.8527 - val_loss: 0.1522 - val_accuracy: 0.9565 - lr: 0.0058 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 30/200\n",
      "7/7 - 1s - loss: 0.3410 - accuracy: 0.8527 - val_loss: 0.3405 - val_accuracy: 0.6957 - lr: 0.0060 - 1s/epoch - 203ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 31/200\n",
      "7/7 - 0s - loss: 0.3547 - accuracy: 0.8438 - val_loss: 0.2648 - val_accuracy: 0.8696 - lr: 0.0062 - 493ms/epoch - 70ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 32/200\n",
      "======================EPOCH 000032:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp    f1       rfa\n",
      "31   32.0  0.355669  0.857143  0.156223  0.956522  0.984848  0.96  0.966237\n",
      "RFA : 0.9662371577783064\n",
      "7/7 - 0s - loss: 0.3557 - accuracy: 0.8571 - val_loss: 0.1562 - val_accuracy: 0.9565 - lr: 0.0064 - 367ms/epoch - 52ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 33/200\n",
      "7/7 - 2s - loss: 0.3346 - accuracy: 0.8571 - val_loss: 0.2118 - val_accuracy: 0.9130 - lr: 0.0066 - 2s/epoch - 233ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 34/200\n",
      "======================EPOCH 000034:======================\n",
      "    epoch      loss       acc  val_loss   val_acc  rocp    f1       rfa\n",
      "33   34.0  0.413173  0.808036  0.186067  0.956522   1.0  0.96  0.970783\n",
      "RFA : 0.970782612323761\n",
      "7/7 - 0s - loss: 0.4132 - accuracy: 0.8080 - val_loss: 0.1861 - val_accuracy: 0.9565 - lr: 0.0068 - 371ms/epoch - 53ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 35/200\n",
      "7/7 - 0s - loss: 0.3340 - accuracy: 0.8571 - val_loss: 0.2232 - val_accuracy: 0.8696 - lr: 0.0070 - 420ms/epoch - 60ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 36/200\n",
      "7/7 - 0s - loss: 0.3406 - accuracy: 0.8616 - val_loss: 0.2154 - val_accuracy: 0.9130 - lr: 0.0072 - 344ms/epoch - 49ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 37/200\n",
      "7/7 - 0s - loss: 0.3621 - accuracy: 0.8527 - val_loss: 0.2044 - val_accuracy: 0.9565 - lr: 0.0074 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 38/200\n",
      "7/7 - 1s - loss: 0.3324 - accuracy: 0.8571 - val_loss: 0.1990 - val_accuracy: 0.8696 - lr: 0.0076 - 1s/epoch - 154ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 39/200\n",
      "7/7 - 0s - loss: 0.3084 - accuracy: 0.8661 - val_loss: 0.1537 - val_accuracy: 0.9565 - lr: 0.0078 - 494ms/epoch - 71ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 40/200\n",
      "7/7 - 1s - loss: 0.3388 - accuracy: 0.8661 - val_loss: 0.1407 - val_accuracy: 0.9565 - lr: 0.0080 - 1s/epoch - 171ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 41/200\n",
      "7/7 - 0s - loss: 0.4496 - accuracy: 0.8259 - val_loss: 0.1978 - val_accuracy: 0.8696 - lr: 0.0082 - 485ms/epoch - 69ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 42/200\n",
      "7/7 - 0s - loss: 0.4884 - accuracy: 0.7991 - val_loss: 0.2368 - val_accuracy: 0.9130 - lr: 0.0084 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 43/200\n",
      "7/7 - 0s - loss: 0.4944 - accuracy: 0.7812 - val_loss: 0.2106 - val_accuracy: 0.9130 - lr: 0.0086 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 44/200\n",
      "7/7 - 1s - loss: 0.4611 - accuracy: 0.7723 - val_loss: 0.1617 - val_accuracy: 0.9565 - lr: 0.0088 - 1s/epoch - 172ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 45/200\n",
      "7/7 - 0s - loss: 0.4269 - accuracy: 0.7902 - val_loss: 0.1689 - val_accuracy: 0.9565 - lr: 0.0090 - 497ms/epoch - 71ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 46/200\n",
      "7/7 - 0s - loss: 0.7881 - accuracy: 0.6964 - val_loss: 1.5471 - val_accuracy: 0.4783 - lr: 0.0092 - 290ms/epoch - 41ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 47/200\n",
      "7/7 - 0s - loss: 14.2289 - accuracy: 0.5446 - val_loss: 38.6236 - val_accuracy: 0.4783 - lr: 0.0094 - 303ms/epoch - 43ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 48/200\n",
      "7/7 - 0s - loss: 1826.2773 - accuracy: 0.4420 - val_loss: 1206.9008 - val_accuracy: 0.5217 - lr: 0.0096 - 302ms/epoch - 43ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 49/200\n",
      "7/7 - 1s - loss: 6657.6841 - accuracy: 0.5045 - val_loss: 208.3369 - val_accuracy: 0.3478 - lr: 0.0098 - 1s/epoch - 200ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 50/200\n",
      "7/7 - 0s - loss: 309.9399 - accuracy: 0.4821 - val_loss: 33.3576 - val_accuracy: 0.5217 - lr: 0.0100 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 51/200\n",
      "7/7 - 0s - loss: 28.4191 - accuracy: 0.5134 - val_loss: 7.8005 - val_accuracy: 0.6087 - lr: 0.0098 - 313ms/epoch - 45ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 52/200\n",
      "7/7 - 0s - loss: 4.7743 - accuracy: 0.5938 - val_loss: 1.8481 - val_accuracy: 0.4348 - lr: 0.0096 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 53/200\n",
      "7/7 - 0s - loss: 2.5004 - accuracy: 0.5536 - val_loss: 0.6662 - val_accuracy: 0.6957 - lr: 0.0094 - 305ms/epoch - 44ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 54/200\n",
      "7/7 - 1s - loss: 1.8510 - accuracy: 0.5536 - val_loss: 0.7039 - val_accuracy: 0.6957 - lr: 0.0092 - 1s/epoch - 194ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 55/200\n",
      "7/7 - 1s - loss: 1.5882 - accuracy: 0.5893 - val_loss: 0.4644 - val_accuracy: 0.8261 - lr: 0.0090 - 504ms/epoch - 72ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 56/200\n",
      "7/7 - 0s - loss: 1.3252 - accuracy: 0.6071 - val_loss: 0.4311 - val_accuracy: 0.8261 - lr: 0.0088 - 304ms/epoch - 43ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 57/200\n",
      "7/7 - 1s - loss: 1.2204 - accuracy: 0.6384 - val_loss: 0.3782 - val_accuracy: 0.8696 - lr: 0.0086 - 1s/epoch - 161ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 58/200\n",
      "7/7 - 0s - loss: 1.0028 - accuracy: 0.6652 - val_loss: 0.3906 - val_accuracy: 0.8261 - lr: 0.0084 - 490ms/epoch - 70ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 59/200\n",
      "7/7 - 0s - loss: 0.9137 - accuracy: 0.6518 - val_loss: 0.3241 - val_accuracy: 0.9130 - lr: 0.0082 - 318ms/epoch - 45ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 60/200\n",
      "7/7 - 0s - loss: 0.9049 - accuracy: 0.6964 - val_loss: 0.5891 - val_accuracy: 0.6957 - lr: 0.0080 - 312ms/epoch - 45ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 61/200\n",
      "7/7 - 0s - loss: 0.8840 - accuracy: 0.7098 - val_loss: 0.4937 - val_accuracy: 0.8261 - lr: 0.0078 - 313ms/epoch - 45ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 62/200\n",
      "7/7 - 1s - loss: 0.7283 - accuracy: 0.7009 - val_loss: 0.3740 - val_accuracy: 0.8696 - lr: 0.0076 - 1s/epoch - 153ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 63/200\n",
      "7/7 - 0s - loss: 0.7348 - accuracy: 0.7188 - val_loss: 0.4472 - val_accuracy: 0.8696 - lr: 0.0074 - 498ms/epoch - 71ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 64/200\n",
      "7/7 - 1s - loss: 0.7218 - accuracy: 0.7277 - val_loss: 0.3401 - val_accuracy: 0.8696 - lr: 0.0072 - 1s/epoch - 149ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 65/200\n",
      "7/7 - 0s - loss: 0.5194 - accuracy: 0.7679 - val_loss: 0.2973 - val_accuracy: 0.9130 - lr: 0.0070 - 500ms/epoch - 71ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 66/200\n",
      "7/7 - 0s - loss: 0.5767 - accuracy: 0.7366 - val_loss: 0.2869 - val_accuracy: 0.9130 - lr: 0.0068 - 300ms/epoch - 43ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 67/200\n",
      "7/7 - 0s - loss: 0.5204 - accuracy: 0.7812 - val_loss: 0.3032 - val_accuracy: 0.8696 - lr: 0.0066 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 68/200\n",
      "7/7 - 0s - loss: 0.4653 - accuracy: 0.8125 - val_loss: 0.3023 - val_accuracy: 0.9130 - lr: 0.0064 - 323ms/epoch - 46ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 69/200\n",
      "7/7 - 1s - loss: 0.4824 - accuracy: 0.8125 - val_loss: 0.2943 - val_accuracy: 0.9130 - lr: 0.0062 - 1s/epoch - 178ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 70/200\n",
      "7/7 - 0s - loss: 0.4666 - accuracy: 0.8259 - val_loss: 0.2902 - val_accuracy: 0.8696 - lr: 0.0060 - 318ms/epoch - 45ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 71/200\n",
      "7/7 - 0s - loss: 0.6827 - accuracy: 0.7589 - val_loss: 0.3482 - val_accuracy: 0.9130 - lr: 0.0058 - 314ms/epoch - 45ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 72/200\n",
      "7/7 - 0s - loss: 0.5559 - accuracy: 0.7679 - val_loss: 0.3364 - val_accuracy: 0.9130 - lr: 0.0056 - 371ms/epoch - 53ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 73/200\n",
      "7/7 - 1s - loss: 0.5502 - accuracy: 0.7857 - val_loss: 0.2780 - val_accuracy: 0.9130 - lr: 0.0054 - 1s/epoch - 147ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 74/200\n",
      "7/7 - 1s - loss: 0.5206 - accuracy: 0.7723 - val_loss: 0.2958 - val_accuracy: 0.9130 - lr: 0.0052 - 1s/epoch - 147ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 75/200\n",
      "7/7 - 0s - loss: 0.4496 - accuracy: 0.8170 - val_loss: 0.3028 - val_accuracy: 0.8696 - lr: 0.0050 - 485ms/epoch - 69ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 76/200\n",
      "7/7 - 0s - loss: 0.5490 - accuracy: 0.7902 - val_loss: 0.3161 - val_accuracy: 0.9130 - lr: 0.0048 - 306ms/epoch - 44ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 77/200\n",
      "7/7 - 1s - loss: 0.4276 - accuracy: 0.8259 - val_loss: 0.2952 - val_accuracy: 0.9130 - lr: 0.0046 - 996ms/epoch - 142ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 78/200\n",
      "7/7 - 1s - loss: 0.5550 - accuracy: 0.7679 - val_loss: 0.2820 - val_accuracy: 0.9130 - lr: 0.0044 - 528ms/epoch - 75ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 79/200\n",
      "7/7 - 0s - loss: 0.4910 - accuracy: 0.8170 - val_loss: 0.2703 - val_accuracy: 0.9130 - lr: 0.0042 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 80/200\n",
      "7/7 - 0s - loss: 0.5305 - accuracy: 0.7991 - val_loss: 0.3285 - val_accuracy: 0.9130 - lr: 0.0040 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 81/200\n",
      "7/7 - 0s - loss: 0.4990 - accuracy: 0.7723 - val_loss: 0.2575 - val_accuracy: 0.9130 - lr: 0.0038 - 320ms/epoch - 46ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 82/200\n",
      "7/7 - 0s - loss: 0.4770 - accuracy: 0.8125 - val_loss: 0.2577 - val_accuracy: 0.9130 - lr: 0.0036 - 358ms/epoch - 51ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 83/200\n",
      "7/7 - 1s - loss: 0.5263 - accuracy: 0.7723 - val_loss: 0.3156 - val_accuracy: 0.9130 - lr: 0.0034 - 1s/epoch - 147ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 84/200\n",
      "7/7 - 0s - loss: 0.4977 - accuracy: 0.8036 - val_loss: 0.2876 - val_accuracy: 0.9130 - lr: 0.0032 - 313ms/epoch - 45ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 85/200\n",
      "7/7 - 0s - loss: 0.4525 - accuracy: 0.8214 - val_loss: 0.2631 - val_accuracy: 0.9130 - lr: 0.0030 - 305ms/epoch - 44ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 86/200\n",
      "7/7 - 1s - loss: 0.3941 - accuracy: 0.8036 - val_loss: 0.2647 - val_accuracy: 0.9130 - lr: 0.0028 - 1s/epoch - 166ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 87/200\n",
      "7/7 - 0s - loss: 0.5276 - accuracy: 0.7812 - val_loss: 0.3591 - val_accuracy: 0.8696 - lr: 0.0026 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 88/200\n",
      "7/7 - 0s - loss: 0.4658 - accuracy: 0.7902 - val_loss: 0.2888 - val_accuracy: 0.9130 - lr: 0.0024 - 291ms/epoch - 42ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 89/200\n",
      "7/7 - 0s - loss: 0.3789 - accuracy: 0.8438 - val_loss: 0.2552 - val_accuracy: 0.9130 - lr: 0.0022 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 90/200\n",
      "7/7 - 0s - loss: 0.3965 - accuracy: 0.8304 - val_loss: 0.2605 - val_accuracy: 0.9130 - lr: 0.0020 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 91/200\n",
      "7/7 - 0s - loss: 0.4414 - accuracy: 0.8036 - val_loss: 0.2525 - val_accuracy: 0.9130 - lr: 0.0018 - 304ms/epoch - 43ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 92/200\n",
      "7/7 - 0s - loss: 0.4615 - accuracy: 0.7946 - val_loss: 0.2526 - val_accuracy: 0.9130 - lr: 0.0016 - 313ms/epoch - 45ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 93/200\n",
      "7/7 - 1s - loss: 0.4321 - accuracy: 0.8214 - val_loss: 0.2614 - val_accuracy: 0.9130 - lr: 0.0014 - 1s/epoch - 145ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 94/200\n",
      "7/7 - 0s - loss: 0.4017 - accuracy: 0.8438 - val_loss: 0.2536 - val_accuracy: 0.9130 - lr: 0.0012 - 482ms/epoch - 69ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 95/200\n",
      "7/7 - 2s - loss: 0.4672 - accuracy: 0.8438 - val_loss: 0.2505 - val_accuracy: 0.9130 - lr: 0.0010 - 2s/epoch - 234ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 96/200\n",
      "7/7 - 0s - loss: 0.4906 - accuracy: 0.7902 - val_loss: 0.2528 - val_accuracy: 0.9130 - lr: 8.0009e-04 - 499ms/epoch - 71ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 97/200\n",
      "7/7 - 0s - loss: 0.4536 - accuracy: 0.8170 - val_loss: 0.2513 - val_accuracy: 0.9130 - lr: 6.0009e-04 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 98/200\n",
      "7/7 - 0s - loss: 0.4619 - accuracy: 0.8125 - val_loss: 0.2536 - val_accuracy: 0.9130 - lr: 4.0010e-04 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 99/200\n",
      "7/7 - 0s - loss: 0.5167 - accuracy: 0.7812 - val_loss: 0.2494 - val_accuracy: 0.9130 - lr: 2.0010e-04 - 444ms/epoch - 63ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 100/200\n",
      "7/7 - 0s - loss: 0.3823 - accuracy: 0.8304 - val_loss: 0.2501 - val_accuracy: 0.9130 - lr: 1.0000e-07 - 316ms/epoch - 45ms/step\n",
      "Learning rate: 0.00000010\n",
      "Epoch 101/200\n",
      "7/7 - 2s - loss: 0.4555 - accuracy: 0.8170 - val_loss: 0.2508 - val_accuracy: 0.9130 - lr: 2.0010e-04 - 2s/epoch - 262ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 102/200\n",
      "7/7 - 0s - loss: 0.3997 - accuracy: 0.8214 - val_loss: 0.2499 - val_accuracy: 0.9130 - lr: 4.0010e-04 - 464ms/epoch - 66ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 103/200\n",
      "7/7 - 0s - loss: 0.4895 - accuracy: 0.7946 - val_loss: 0.2493 - val_accuracy: 0.9130 - lr: 6.0009e-04 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 104/200\n",
      "7/7 - 2s - loss: 0.4030 - accuracy: 0.8482 - val_loss: 0.2464 - val_accuracy: 0.9130 - lr: 8.0009e-04 - 2s/epoch - 259ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 105/200\n",
      "7/7 - 1s - loss: 0.4202 - accuracy: 0.8125 - val_loss: 0.2483 - val_accuracy: 0.9130 - lr: 0.0010 - 520ms/epoch - 74ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 106/200\n",
      "7/7 - 0s - loss: 0.4114 - accuracy: 0.8080 - val_loss: 0.2466 - val_accuracy: 0.9130 - lr: 0.0012 - 427ms/epoch - 61ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 107/200\n",
      "7/7 - 0s - loss: 0.4327 - accuracy: 0.8304 - val_loss: 0.2617 - val_accuracy: 0.9130 - lr: 0.0014 - 360ms/epoch - 51ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 108/200\n",
      "7/7 - 0s - loss: 0.3947 - accuracy: 0.8125 - val_loss: 0.2427 - val_accuracy: 0.9130 - lr: 0.0016 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 109/200\n",
      "7/7 - 0s - loss: 0.3982 - accuracy: 0.8259 - val_loss: 0.2483 - val_accuracy: 0.9130 - lr: 0.0018 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 110/200\n",
      "7/7 - 0s - loss: 0.3738 - accuracy: 0.8527 - val_loss: 0.2478 - val_accuracy: 0.9130 - lr: 0.0020 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 111/200\n",
      "7/7 - 1s - loss: 0.3871 - accuracy: 0.7991 - val_loss: 0.2466 - val_accuracy: 0.9130 - lr: 0.0022 - 525ms/epoch - 75ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 112/200\n",
      "7/7 - 2s - loss: 0.4539 - accuracy: 0.8170 - val_loss: 0.2719 - val_accuracy: 0.9130 - lr: 0.0024 - 2s/epoch - 222ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 113/200\n",
      "7/7 - 0s - loss: 0.4292 - accuracy: 0.8125 - val_loss: 0.2487 - val_accuracy: 0.9130 - lr: 0.0026 - 437ms/epoch - 62ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 114/200\n",
      "7/7 - 0s - loss: 0.4646 - accuracy: 0.7679 - val_loss: 0.2431 - val_accuracy: 0.9130 - lr: 0.0028 - 351ms/epoch - 50ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 115/200\n",
      "7/7 - 0s - loss: 0.4348 - accuracy: 0.8036 - val_loss: 0.2386 - val_accuracy: 0.9130 - lr: 0.0030 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 116/200\n",
      "7/7 - 0s - loss: 0.4275 - accuracy: 0.8214 - val_loss: 0.2724 - val_accuracy: 0.9130 - lr: 0.0032 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 117/200\n",
      "7/7 - 0s - loss: 0.3688 - accuracy: 0.8259 - val_loss: 0.2309 - val_accuracy: 0.9130 - lr: 0.0034 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 118/200\n",
      "7/7 - 0s - loss: 0.3995 - accuracy: 0.8259 - val_loss: 0.2502 - val_accuracy: 0.9130 - lr: 0.0036 - 468ms/epoch - 67ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 119/200\n",
      "7/7 - 2s - loss: 0.5280 - accuracy: 0.8080 - val_loss: 0.2642 - val_accuracy: 0.9130 - lr: 0.0038 - 2s/epoch - 243ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 120/200\n",
      "7/7 - 0s - loss: 0.4410 - accuracy: 0.8080 - val_loss: 0.2380 - val_accuracy: 0.9565 - lr: 0.0040 - 500ms/epoch - 71ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 121/200\n",
      "7/7 - 0s - loss: 0.3972 - accuracy: 0.8214 - val_loss: 0.2589 - val_accuracy: 0.9130 - lr: 0.0042 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 122/200\n",
      "7/7 - 1s - loss: 0.4056 - accuracy: 0.8304 - val_loss: 0.2450 - val_accuracy: 0.9130 - lr: 0.0044 - 1s/epoch - 201ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 123/200\n",
      "7/7 - 0s - loss: 0.3672 - accuracy: 0.8259 - val_loss: 0.2500 - val_accuracy: 0.9130 - lr: 0.0046 - 474ms/epoch - 68ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 124/200\n",
      "7/7 - 0s - loss: 0.5194 - accuracy: 0.7857 - val_loss: 0.2781 - val_accuracy: 0.9565 - lr: 0.0048 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 125/200\n",
      "7/7 - 1s - loss: 0.4291 - accuracy: 0.8170 - val_loss: 0.2449 - val_accuracy: 0.9130 - lr: 0.0050 - 1s/epoch - 180ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 126/200\n",
      "7/7 - 0s - loss: 0.4779 - accuracy: 0.7946 - val_loss: 0.4557 - val_accuracy: 0.8696 - lr: 0.0052 - 494ms/epoch - 71ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 127/200\n",
      "7/7 - 0s - loss: 0.5389 - accuracy: 0.7991 - val_loss: 0.2292 - val_accuracy: 0.9130 - lr: 0.0054 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 128/200\n",
      "7/7 - 1s - loss: 0.3946 - accuracy: 0.8750 - val_loss: 0.3609 - val_accuracy: 0.8261 - lr: 0.0056 - 1s/epoch - 173ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 129/200\n",
      "7/7 - 1s - loss: 0.5614 - accuracy: 0.7500 - val_loss: 0.2712 - val_accuracy: 0.9130 - lr: 0.0058 - 522ms/epoch - 75ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 130/200\n",
      "7/7 - 0s - loss: 0.4532 - accuracy: 0.8125 - val_loss: 0.2160 - val_accuracy: 0.9130 - lr: 0.0060 - 318ms/epoch - 45ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 131/200\n",
      "7/7 - 0s - loss: 0.3453 - accuracy: 0.8527 - val_loss: 0.2165 - val_accuracy: 0.9130 - lr: 0.0062 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 132/200\n",
      "7/7 - 1s - loss: 0.4305 - accuracy: 0.8125 - val_loss: 0.2282 - val_accuracy: 0.9565 - lr: 0.0064 - 1s/epoch - 185ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 133/200\n",
      "7/7 - 0s - loss: 0.4283 - accuracy: 0.8214 - val_loss: 0.2141 - val_accuracy: 0.9130 - lr: 0.0066 - 346ms/epoch - 49ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 134/200\n",
      "7/7 - 1s - loss: 0.3670 - accuracy: 0.8438 - val_loss: 0.2192 - val_accuracy: 0.9565 - lr: 0.0068 - 880ms/epoch - 126ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 135/200\n",
      "7/7 - 1s - loss: 0.4202 - accuracy: 0.8304 - val_loss: 0.2275 - val_accuracy: 0.9565 - lr: 0.0070 - 521ms/epoch - 74ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 136/200\n",
      "7/7 - 0s - loss: 0.4238 - accuracy: 0.8214 - val_loss: 0.2883 - val_accuracy: 0.8696 - lr: 0.0072 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 137/200\n",
      "7/7 - 0s - loss: 0.4682 - accuracy: 0.7991 - val_loss: 0.7426 - val_accuracy: 0.6522 - lr: 0.0074 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 138/200\n",
      "7/7 - 1s - loss: 0.5761 - accuracy: 0.7812 - val_loss: 0.5132 - val_accuracy: 0.6522 - lr: 0.0076 - 1s/epoch - 197ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 139/200\n",
      "7/7 - 0s - loss: 0.4148 - accuracy: 0.8080 - val_loss: 0.2395 - val_accuracy: 0.9565 - lr: 0.0078 - 343ms/epoch - 49ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 140/200\n",
      "7/7 - 0s - loss: 0.4780 - accuracy: 0.7991 - val_loss: 0.2235 - val_accuracy: 0.9565 - lr: 0.0080 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 141/200\n",
      "7/7 - 0s - loss: 0.3660 - accuracy: 0.8348 - val_loss: 0.2118 - val_accuracy: 0.9130 - lr: 0.0082 - 382ms/epoch - 55ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 142/200\n",
      "7/7 - 1s - loss: 0.3739 - accuracy: 0.8527 - val_loss: 0.2153 - val_accuracy: 0.9565 - lr: 0.0084 - 1s/epoch - 144ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 143/200\n",
      "7/7 - 0s - loss: 0.4506 - accuracy: 0.7946 - val_loss: 0.2348 - val_accuracy: 0.9130 - lr: 0.0086 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 144/200\n",
      "7/7 - 0s - loss: 0.4527 - accuracy: 0.8080 - val_loss: 0.2238 - val_accuracy: 0.9130 - lr: 0.0088 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 145/200\n",
      "7/7 - 0s - loss: 0.4574 - accuracy: 0.8259 - val_loss: 0.2461 - val_accuracy: 0.9130 - lr: 0.0090 - 486ms/epoch - 69ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 146/200\n",
      "7/7 - 0s - loss: 0.4134 - accuracy: 0.8214 - val_loss: 0.2616 - val_accuracy: 0.8696 - lr: 0.0092 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 147/200\n",
      "7/7 - 0s - loss: 0.3427 - accuracy: 0.8661 - val_loss: 0.2361 - val_accuracy: 0.9130 - lr: 0.0094 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 148/200\n",
      "7/7 - 1s - loss: 0.3720 - accuracy: 0.8348 - val_loss: 0.2001 - val_accuracy: 0.9130 - lr: 0.0096 - 1s/epoch - 150ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 149/200\n",
      "7/7 - 1s - loss: 0.4888 - accuracy: 0.7946 - val_loss: 0.2055 - val_accuracy: 0.9130 - lr: 0.0098 - 510ms/epoch - 73ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 150/200\n",
      "7/7 - 0s - loss: 0.4548 - accuracy: 0.7946 - val_loss: 0.2093 - val_accuracy: 0.9130 - lr: 0.0100 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 151/200\n",
      "7/7 - 0s - loss: 0.3397 - accuracy: 0.8705 - val_loss: 0.2119 - val_accuracy: 0.9565 - lr: 0.0098 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 152/200\n",
      "7/7 - 0s - loss: 0.3408 - accuracy: 0.8616 - val_loss: 0.2036 - val_accuracy: 0.9565 - lr: 0.0096 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 153/200\n",
      "7/7 - 1s - loss: 0.4251 - accuracy: 0.7946 - val_loss: 0.1877 - val_accuracy: 0.9565 - lr: 0.0094 - 1s/epoch - 180ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 154/200\n",
      "7/7 - 0s - loss: 0.3987 - accuracy: 0.8438 - val_loss: 0.2196 - val_accuracy: 0.9130 - lr: 0.0092 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 155/200\n",
      "7/7 - 0s - loss: 0.3745 - accuracy: 0.8170 - val_loss: 0.2705 - val_accuracy: 0.9130 - lr: 0.0090 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 156/200\n",
      "7/7 - 0s - loss: 0.3446 - accuracy: 0.8393 - val_loss: 0.1936 - val_accuracy: 0.9565 - lr: 0.0088 - 487ms/epoch - 70ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 157/200\n",
      "7/7 - 0s - loss: 0.3870 - accuracy: 0.8393 - val_loss: 0.2086 - val_accuracy: 0.9565 - lr: 0.0086 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 158/200\n",
      "7/7 - 0s - loss: 0.3565 - accuracy: 0.8571 - val_loss: 0.1980 - val_accuracy: 0.9565 - lr: 0.0084 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 159/200\n",
      "7/7 - 0s - loss: 0.3150 - accuracy: 0.8705 - val_loss: 0.2320 - val_accuracy: 0.9565 - lr: 0.0082 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 160/200\n",
      "7/7 - 0s - loss: 0.3986 - accuracy: 0.8393 - val_loss: 0.1873 - val_accuracy: 0.9130 - lr: 0.0080 - 343ms/epoch - 49ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 161/200\n",
      "7/7 - 1s - loss: 0.4601 - accuracy: 0.7991 - val_loss: 0.2317 - val_accuracy: 0.9130 - lr: 0.0078 - 1s/epoch - 175ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 162/200\n",
      "7/7 - 0s - loss: 0.3235 - accuracy: 0.8438 - val_loss: 0.1935 - val_accuracy: 0.9565 - lr: 0.0076 - 360ms/epoch - 51ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 163/200\n",
      "7/7 - 0s - loss: 0.3784 - accuracy: 0.8304 - val_loss: 0.1818 - val_accuracy: 0.9565 - lr: 0.0074 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 164/200\n",
      "7/7 - 0s - loss: 0.3601 - accuracy: 0.8527 - val_loss: 0.1755 - val_accuracy: 0.9565 - lr: 0.0072 - 470ms/epoch - 67ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 165/200\n",
      "7/7 - 1s - loss: 0.3412 - accuracy: 0.8527 - val_loss: 0.2229 - val_accuracy: 0.9565 - lr: 0.0070 - 1s/epoch - 150ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 166/200\n",
      "7/7 - 0s - loss: 0.3800 - accuracy: 0.8259 - val_loss: 0.1856 - val_accuracy: 0.9565 - lr: 0.0068 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 167/200\n",
      "7/7 - 0s - loss: 0.3774 - accuracy: 0.8214 - val_loss: 0.2058 - val_accuracy: 0.9565 - lr: 0.0066 - 346ms/epoch - 49ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 168/200\n",
      "7/7 - 1s - loss: 0.4240 - accuracy: 0.8214 - val_loss: 0.1829 - val_accuracy: 0.9130 - lr: 0.0064 - 1s/epoch - 148ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 169/200\n",
      "7/7 - 1s - loss: 0.3351 - accuracy: 0.8527 - val_loss: 0.1953 - val_accuracy: 0.9565 - lr: 0.0062 - 1000ms/epoch - 143ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 170/200\n",
      "7/7 - 0s - loss: 0.3684 - accuracy: 0.8661 - val_loss: 0.1828 - val_accuracy: 0.9565 - lr: 0.0060 - 488ms/epoch - 70ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 171/200\n",
      "7/7 - 0s - loss: 0.3287 - accuracy: 0.9018 - val_loss: 0.1761 - val_accuracy: 0.9565 - lr: 0.0058 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 172/200\n",
      "7/7 - 0s - loss: 0.3999 - accuracy: 0.8259 - val_loss: 0.1781 - val_accuracy: 0.9565 - lr: 0.0056 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 173/200\n",
      "7/7 - 0s - loss: 0.3368 - accuracy: 0.8393 - val_loss: 0.2126 - val_accuracy: 0.9565 - lr: 0.0054 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 174/200\n",
      "7/7 - 1s - loss: 0.3353 - accuracy: 0.8348 - val_loss: 0.1836 - val_accuracy: 0.9130 - lr: 0.0052 - 502ms/epoch - 72ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 175/200\n",
      "7/7 - 0s - loss: 0.3163 - accuracy: 0.8884 - val_loss: 0.1742 - val_accuracy: 0.9565 - lr: 0.0050 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 176/200\n",
      "7/7 - 0s - loss: 0.3687 - accuracy: 0.8438 - val_loss: 0.1749 - val_accuracy: 0.9130 - lr: 0.0048 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 177/200\n",
      "7/7 - 1s - loss: 0.2906 - accuracy: 0.8839 - val_loss: 0.1862 - val_accuracy: 0.9130 - lr: 0.0046 - 1s/epoch - 161ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 178/200\n",
      "7/7 - 0s - loss: 0.3171 - accuracy: 0.8705 - val_loss: 0.1927 - val_accuracy: 0.9130 - lr: 0.0044 - 492ms/epoch - 70ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 179/200\n",
      "7/7 - 1s - loss: 0.3898 - accuracy: 0.8170 - val_loss: 0.1927 - val_accuracy: 0.9130 - lr: 0.0042 - 1s/epoch - 144ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 180/200\n",
      "7/7 - 1s - loss: 0.3288 - accuracy: 0.8438 - val_loss: 0.1764 - val_accuracy: 0.9565 - lr: 0.0040 - 517ms/epoch - 74ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 181/200\n",
      "7/7 - 0s - loss: 0.2981 - accuracy: 0.8571 - val_loss: 0.1775 - val_accuracy: 0.9130 - lr: 0.0038 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 182/200\n",
      "7/7 - 0s - loss: 0.2960 - accuracy: 0.8884 - val_loss: 0.1690 - val_accuracy: 0.9565 - lr: 0.0036 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 183/200\n",
      "7/7 - 0s - loss: 0.3343 - accuracy: 0.8438 - val_loss: 0.1756 - val_accuracy: 0.9565 - lr: 0.0034 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 184/200\n",
      "7/7 - 0s - loss: 0.3187 - accuracy: 0.8616 - val_loss: 0.1798 - val_accuracy: 0.9130 - lr: 0.0032 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 185/200\n",
      "7/7 - 1s - loss: 0.3229 - accuracy: 0.8482 - val_loss: 0.2122 - val_accuracy: 0.9565 - lr: 0.0030 - 525ms/epoch - 75ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 186/200\n",
      "7/7 - 0s - loss: 0.3306 - accuracy: 0.8705 - val_loss: 0.1721 - val_accuracy: 0.9130 - lr: 0.0028 - 316ms/epoch - 45ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 187/200\n",
      "7/7 - 0s - loss: 0.3006 - accuracy: 0.8795 - val_loss: 0.1670 - val_accuracy: 0.9565 - lr: 0.0026 - 264ms/epoch - 38ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 188/200\n",
      "7/7 - 0s - loss: 0.3371 - accuracy: 0.8482 - val_loss: 0.1703 - val_accuracy: 0.9565 - lr: 0.0024 - 280ms/epoch - 40ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 189/200\n",
      "7/7 - 0s - loss: 0.2710 - accuracy: 0.8795 - val_loss: 0.1760 - val_accuracy: 0.9130 - lr: 0.0022 - 285ms/epoch - 41ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 190/200\n",
      "7/7 - 1s - loss: 0.3124 - accuracy: 0.8616 - val_loss: 0.1659 - val_accuracy: 0.9565 - lr: 0.0020 - 1s/epoch - 163ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 191/200\n",
      "7/7 - 0s - loss: 0.2824 - accuracy: 0.8705 - val_loss: 0.1748 - val_accuracy: 0.9130 - lr: 0.0018 - 357ms/epoch - 51ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 192/200\n",
      "7/7 - 0s - loss: 0.2718 - accuracy: 0.8750 - val_loss: 0.1661 - val_accuracy: 0.9565 - lr: 0.0016 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 193/200\n",
      "7/7 - 1s - loss: 0.3242 - accuracy: 0.8973 - val_loss: 0.1693 - val_accuracy: 0.9130 - lr: 0.0014 - 1s/epoch - 156ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 194/200\n",
      "7/7 - 0s - loss: 0.3047 - accuracy: 0.8661 - val_loss: 0.1676 - val_accuracy: 0.9130 - lr: 0.0012 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 195/200\n",
      "7/7 - 0s - loss: 0.2871 - accuracy: 0.8795 - val_loss: 0.1649 - val_accuracy: 0.9130 - lr: 0.0010 - 351ms/epoch - 50ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 196/200\n",
      "7/7 - 0s - loss: 0.2884 - accuracy: 0.8795 - val_loss: 0.1658 - val_accuracy: 0.9565 - lr: 8.0009e-04 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 197/200\n",
      "7/7 - 0s - loss: 0.3256 - accuracy: 0.8750 - val_loss: 0.1613 - val_accuracy: 0.9130 - lr: 6.0009e-04 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 198/200\n",
      "7/7 - 1s - loss: 0.2680 - accuracy: 0.8973 - val_loss: 0.1631 - val_accuracy: 0.9130 - lr: 4.0010e-04 - 998ms/epoch - 143ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 199/200\n",
      "7/7 - 1s - loss: 0.3232 - accuracy: 0.8438 - val_loss: 0.1627 - val_accuracy: 0.9130 - lr: 2.0010e-04 - 996ms/epoch - 142ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 200/200\n",
      "7/7 - 1s - loss: 0.3090 - accuracy: 0.8616 - val_loss: 0.1619 - val_accuracy: 0.9130 - lr: 1.0000e-07 - 514ms/epoch - 73ms/step\n",
      "\n",
      "======================BEST:======================\n",
      "    epoch      loss       acc  val_loss   val_acc  rocp    f1       rfa\n",
      "33   34.0  0.413173  0.808036  0.186067  0.956522   1.0  0.96  0.970783\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        11\n",
      "           1       0.92      0.92      0.92        12\n",
      "\n",
      "    accuracy                           0.91        23\n",
      "   macro avg       0.91      0.91      0.91        23\n",
      "weighted avg       0.91      0.91      0.91        23\n",
      "\n",
      "[[10  1]\n",
      " [ 1 11]]\n",
      "ACCURACY: 0.9130434782608695\n",
      "PRECISION: 0.9166666666666666\n",
      "RECALL: 0.9166666666666666\n",
      "F1: 0.9166666666666666\n",
      "ROC_AUC(Pr.): 0.9696969696969697\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95        11\n",
      "           1       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.96        23\n",
      "   macro avg       0.96      0.95      0.96        23\n",
      "weighted avg       0.96      0.96      0.96        23\n",
      "\n",
      "[[10  1]\n",
      " [ 0 12]]\n",
      "ACCURACY: 0.9565217391304348\n",
      "PRECISION: 0.9230769230769231\n",
      "RECALL: 1.0\n",
      "F1: 0.9600000000000001\n",
      "ROC_AUC(Pr.): 1.0\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95        11\n",
      "           1       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.96        23\n",
      "   macro avg       0.96      0.95      0.96        23\n",
      "weighted avg       0.96      0.96      0.96        23\n",
      "\n",
      "[[10  1]\n",
      " [ 0 12]]\n",
      "ACCURACY: 0.9565217391304348\n",
      "PRECISION: 0.9230769230769231\n",
      "RECALL: 1.0\n",
      "F1: 0.9600000000000001\n",
      "ROC_AUC(Pr.): 1.0\n",
      "\n",
      "=====================CV[2]========================\n",
      "\n",
      "207 train samples\n",
      "23 test samples\n",
      "\n",
      "Over sampling: 207 -> 208\n",
      "Add train samples: 16\n",
      "224 new train samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\tf28\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.00000010\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_28484\\4287042063.py:159: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================EPOCH 000001:======================\n",
      "   epoch     loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "0    1.0  0.69252  0.566964  0.693071  0.521739  0.469697  0.685714  0.563518\n",
      "RFA : 0.5635177847472104\n",
      "7/7 - 6s - loss: 0.6925 - accuracy: 0.5670 - val_loss: 0.6931 - val_accuracy: 0.5217 - lr: 2.0010e-04 - 6s/epoch - 834ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 2/200\n",
      "7/7 - 2s - loss: 0.6933 - accuracy: 0.4911 - val_loss: 0.6930 - val_accuracy: 0.4783 - lr: 4.0010e-04 - 2s/epoch - 225ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 3/200\n",
      "7/7 - 0s - loss: 0.6917 - accuracy: 0.4866 - val_loss: 0.6922 - val_accuracy: 0.4783 - lr: 6.0009e-04 - 361ms/epoch - 52ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 4/200\n",
      "======================EPOCH 000004:======================\n",
      "   epoch      loss       acc  val_loss   val_acc     rocp        f1       rfa\n",
      "3    4.0  0.688071  0.580357   0.68734  0.521739  0.80303  0.685714  0.663518\n",
      "RFA : 0.6635177847472103\n",
      "7/7 - 0s - loss: 0.6881 - accuracy: 0.5804 - val_loss: 0.6873 - val_accuracy: 0.5217 - lr: 8.0009e-04 - 384ms/epoch - 55ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 5/200\n",
      "======================EPOCH 000005:======================\n",
      "   epoch      loss       acc  val_loss  val_acc      rocp    f1       rfa\n",
      "4    5.0  0.677679  0.638393  0.667668  0.73913  0.863636  0.75  0.780287\n",
      "RFA : 0.7802865621718492\n",
      "7/7 - 1s - loss: 0.6777 - accuracy: 0.6384 - val_loss: 0.6677 - val_accuracy: 0.7391 - lr: 0.0010 - 502ms/epoch - 72ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 6/200\n",
      "======================EPOCH 000006:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "5    6.0  0.613174  0.678571  0.582001  0.782609  0.863636  0.827586  0.822659\n",
      "RFA : 0.8226591222618813\n",
      "7/7 - 1s - loss: 0.6132 - accuracy: 0.6786 - val_loss: 0.5820 - val_accuracy: 0.7826 - lr: 0.0012 - 1s/epoch - 145ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 7/200\n",
      "7/7 - 0s - loss: 0.4995 - accuracy: 0.7768 - val_loss: 0.4604 - val_accuracy: 0.7826 - lr: 0.0014 - 360ms/epoch - 51ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 8/200\n",
      "7/7 - 1s - loss: 0.4767 - accuracy: 0.7455 - val_loss: 0.4174 - val_accuracy: 0.7391 - lr: 0.0016 - 1s/epoch - 162ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 9/200\n",
      "======================EPOCH 000009:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp   f1       rfa\n",
      "8    9.0  0.428225  0.816964  0.386154  0.782609  0.931818  0.8  0.833458\n",
      "RFA : 0.8334584953026338\n",
      "7/7 - 1s - loss: 0.4282 - accuracy: 0.8170 - val_loss: 0.3862 - val_accuracy: 0.7826 - lr: 0.0018 - 668ms/epoch - 95ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 10/200\n",
      "======================EPOCH 000010:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "9   10.0  0.369438  0.825893  0.350144  0.782609  0.931818  0.814815  0.838644\n",
      "RFA : 0.8386436804878189\n",
      "7/7 - 0s - loss: 0.3694 - accuracy: 0.8259 - val_loss: 0.3501 - val_accuracy: 0.7826 - lr: 0.0020 - 394ms/epoch - 56ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 11/200\n",
      "7/7 - 0s - loss: 0.4106 - accuracy: 0.7991 - val_loss: 0.3886 - val_accuracy: 0.7391 - lr: 0.0022 - 381ms/epoch - 54ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 12/200\n",
      "======================EPOCH 000012:======================\n",
      "    epoch      loss       acc  val_loss   val_acc     rocp        f1       rfa\n",
      "11   12.0  0.458664  0.794643  0.398854  0.782609  0.94697  0.827586  0.847659\n",
      "RFA : 0.8476591222618814\n",
      "7/7 - 2s - loss: 0.4587 - accuracy: 0.7946 - val_loss: 0.3989 - val_accuracy: 0.7826 - lr: 0.0024 - 2s/epoch - 223ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 13/200\n",
      "======================EPOCH 000013:======================\n",
      "    epoch      loss       acc  val_loss   val_acc     rocp        f1       rfa\n",
      "12   13.0  0.412685  0.790179  0.341532  0.826087  0.94697  0.846154  0.869375\n",
      "RFA : 0.8693751836781733\n",
      "7/7 - 1s - loss: 0.4127 - accuracy: 0.7902 - val_loss: 0.3415 - val_accuracy: 0.8261 - lr: 0.0026 - 519ms/epoch - 74ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 14/200\n",
      "7/7 - 1s - loss: 0.3677 - accuracy: 0.8214 - val_loss: 0.3364 - val_accuracy: 0.7826 - lr: 0.0028 - 1s/epoch - 161ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 15/200\n",
      "======================EPOCH 000015:======================\n",
      "    epoch      loss       acc  val_loss   val_acc     rocp        f1       rfa\n",
      "14   15.0  0.334122  0.879464  0.331461  0.869565  0.94697  0.869565  0.892787\n",
      "RFA : 0.8927865512875228\n",
      "7/7 - 1s - loss: 0.3341 - accuracy: 0.8795 - val_loss: 0.3315 - val_accuracy: 0.8696 - lr: 0.0030 - 516ms/epoch - 74ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 16/200\n",
      "7/7 - 0s - loss: 0.4145 - accuracy: 0.7991 - val_loss: 0.5083 - val_accuracy: 0.7826 - lr: 0.0032 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 17/200\n",
      "======================EPOCH 000017:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "16   17.0  0.382907  0.799107  0.320122  0.913043  0.954545  0.916667  0.926762\n",
      "RFA : 0.9267621943444917\n",
      "7/7 - 0s - loss: 0.3829 - accuracy: 0.7991 - val_loss: 0.3201 - val_accuracy: 0.9130 - lr: 0.0034 - 382ms/epoch - 55ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 18/200\n",
      "7/7 - 0s - loss: 0.3363 - accuracy: 0.8393 - val_loss: 0.3629 - val_accuracy: 0.8261 - lr: 0.0036 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 19/200\n",
      "7/7 - 1s - loss: 0.3843 - accuracy: 0.8080 - val_loss: 0.3864 - val_accuracy: 0.7826 - lr: 0.0038 - 554ms/epoch - 79ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 20/200\n",
      "======================EPOCH 000020:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "19   20.0  0.378352  0.834821  0.302782  0.913043  0.984848  0.909091  0.933202\n",
      "RFA : 0.9332015882838856\n",
      "7/7 - 1s - loss: 0.3784 - accuracy: 0.8348 - val_loss: 0.3028 - val_accuracy: 0.9130 - lr: 0.0040 - 663ms/epoch - 95ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 21/200\n",
      "7/7 - 0s - loss: 0.3394 - accuracy: 0.8571 - val_loss: 0.2851 - val_accuracy: 0.8261 - lr: 0.0042 - 355ms/epoch - 51ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 22/200\n",
      "7/7 - 0s - loss: 0.4079 - accuracy: 0.7812 - val_loss: 0.3665 - val_accuracy: 0.8696 - lr: 0.0044 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 23/200\n",
      "======================EPOCH 000023:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "22   23.0  0.368627  0.821429   0.30132  0.913043  0.984848  0.923077  0.938097\n",
      "RFA : 0.9380966931789905\n",
      "7/7 - 1s - loss: 0.3686 - accuracy: 0.8214 - val_loss: 0.3013 - val_accuracy: 0.9130 - lr: 0.0046 - 554ms/epoch - 79ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 24/200\n",
      "7/7 - 0s - loss: 0.3294 - accuracy: 0.8705 - val_loss: 0.2604 - val_accuracy: 0.8696 - lr: 0.0048 - 355ms/epoch - 51ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 25/200\n",
      "7/7 - 0s - loss: 0.3148 - accuracy: 0.8661 - val_loss: 1.3212 - val_accuracy: 0.6087 - lr: 0.0050 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 26/200\n",
      "7/7 - 0s - loss: 0.4875 - accuracy: 0.8348 - val_loss: 0.3226 - val_accuracy: 0.9130 - lr: 0.0052 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 27/200\n",
      "7/7 - 0s - loss: 0.3758 - accuracy: 0.8214 - val_loss: 0.2893 - val_accuracy: 0.9130 - lr: 0.0054 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 28/200\n",
      "7/7 - 0s - loss: 0.3431 - accuracy: 0.8482 - val_loss: 0.2895 - val_accuracy: 0.9130 - lr: 0.0056 - 449ms/epoch - 64ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 29/200\n",
      "7/7 - 1s - loss: 0.3303 - accuracy: 0.8705 - val_loss: 0.3510 - val_accuracy: 0.8261 - lr: 0.0058 - 1s/epoch - 148ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 30/200\n",
      "7/7 - 0s - loss: 0.3110 - accuracy: 0.8482 - val_loss: 0.2653 - val_accuracy: 0.8696 - lr: 0.0060 - 331ms/epoch - 47ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 31/200\n",
      "7/7 - 1s - loss: 0.2758 - accuracy: 0.8929 - val_loss: 0.3493 - val_accuracy: 0.8696 - lr: 0.0062 - 526ms/epoch - 75ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 32/200\n",
      "7/7 - 1s - loss: 0.3192 - accuracy: 0.8705 - val_loss: 0.3418 - val_accuracy: 0.8696 - lr: 0.0064 - 785ms/epoch - 112ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 33/200\n",
      "7/7 - 0s - loss: 0.3275 - accuracy: 0.8348 - val_loss: 0.3025 - val_accuracy: 0.8261 - lr: 0.0066 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 34/200\n",
      "7/7 - 2s - loss: 0.3491 - accuracy: 0.8482 - val_loss: 0.2905 - val_accuracy: 0.9130 - lr: 0.0068 - 2s/epoch - 252ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 35/200\n",
      "7/7 - 1s - loss: 0.3358 - accuracy: 0.8616 - val_loss: 0.2530 - val_accuracy: 0.9130 - lr: 0.0070 - 508ms/epoch - 73ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 36/200\n",
      "7/7 - 1s - loss: 0.2917 - accuracy: 0.8482 - val_loss: 0.3350 - val_accuracy: 0.8261 - lr: 0.0072 - 507ms/epoch - 72ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 37/200\n",
      "======================EPOCH 000037:======================\n",
      "    epoch      loss      acc  val_loss   val_acc  rocp    f1       rfa\n",
      "36   37.0  0.350316  0.84375  0.232462  0.956522   1.0  0.96  0.970783\n",
      "RFA : 0.970782612323761\n",
      "7/7 - 0s - loss: 0.3503 - accuracy: 0.8438 - val_loss: 0.2325 - val_accuracy: 0.9565 - lr: 0.0074 - 384ms/epoch - 55ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 38/200\n",
      "7/7 - 0s - loss: 0.3304 - accuracy: 0.8750 - val_loss: 0.2942 - val_accuracy: 0.7826 - lr: 0.0076 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 39/200\n",
      "7/7 - 0s - loss: 0.4215 - accuracy: 0.8125 - val_loss: 0.3533 - val_accuracy: 0.7826 - lr: 0.0078 - 493ms/epoch - 70ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 40/200\n",
      "7/7 - 0s - loss: 0.3074 - accuracy: 0.8661 - val_loss: 0.5766 - val_accuracy: 0.6087 - lr: 0.0080 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 41/200\n",
      "7/7 - 2s - loss: 0.4313 - accuracy: 0.8170 - val_loss: 0.3521 - val_accuracy: 0.8261 - lr: 0.0082 - 2s/epoch - 255ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 42/200\n",
      "7/7 - 1s - loss: 0.3606 - accuracy: 0.8170 - val_loss: 0.2481 - val_accuracy: 0.9565 - lr: 0.0084 - 513ms/epoch - 73ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 43/200\n",
      "7/7 - 0s - loss: 0.3236 - accuracy: 0.8259 - val_loss: 0.3631 - val_accuracy: 0.8696 - lr: 0.0086 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 44/200\n",
      "7/7 - 0s - loss: 0.3975 - accuracy: 0.8438 - val_loss: 0.3080 - val_accuracy: 0.8261 - lr: 0.0088 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 45/200\n",
      "7/7 - 1s - loss: 0.6831 - accuracy: 0.7232 - val_loss: 0.4863 - val_accuracy: 0.6957 - lr: 0.0090 - 1s/epoch - 196ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 46/200\n",
      "7/7 - 0s - loss: 0.4733 - accuracy: 0.8214 - val_loss: 0.3356 - val_accuracy: 0.8696 - lr: 0.0092 - 485ms/epoch - 69ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 47/200\n",
      "7/7 - 0s - loss: 0.8147 - accuracy: 0.7232 - val_loss: 1.1975 - val_accuracy: 0.6957 - lr: 0.0094 - 331ms/epoch - 47ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 48/200\n",
      "7/7 - 0s - loss: 8.5881 - accuracy: 0.6071 - val_loss: 9.1974 - val_accuracy: 0.5217 - lr: 0.0096 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 49/200\n",
      "7/7 - 1s - loss: 421.6171 - accuracy: 0.5223 - val_loss: 320.9254 - val_accuracy: 0.4783 - lr: 0.0098 - 828ms/epoch - 118ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 50/200\n",
      "7/7 - 0s - loss: 371.4646 - accuracy: 0.5089 - val_loss: 55.6201 - val_accuracy: 0.5217 - lr: 0.0100 - 323ms/epoch - 46ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 51/200\n",
      "7/7 - 0s - loss: 18.8846 - accuracy: 0.5268 - val_loss: 2.4650 - val_accuracy: 0.5217 - lr: 0.0098 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 52/200\n",
      "7/7 - 1s - loss: 3.6084 - accuracy: 0.6116 - val_loss: 2.2210 - val_accuracy: 0.6522 - lr: 0.0096 - 559ms/epoch - 80ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 53/200\n",
      "7/7 - 0s - loss: 1.7021 - accuracy: 0.6339 - val_loss: 1.1629 - val_accuracy: 0.6087 - lr: 0.0094 - 382ms/epoch - 55ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 54/200\n",
      "7/7 - 0s - loss: 0.9382 - accuracy: 0.6875 - val_loss: 0.4095 - val_accuracy: 0.8261 - lr: 0.0092 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 55/200\n",
      "7/7 - 1s - loss: 0.6678 - accuracy: 0.7232 - val_loss: 0.4919 - val_accuracy: 0.7826 - lr: 0.0090 - 1s/epoch - 188ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 56/200\n",
      "7/7 - 1s - loss: 0.5834 - accuracy: 0.7500 - val_loss: 0.4122 - val_accuracy: 0.8261 - lr: 0.0088 - 514ms/epoch - 73ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 57/200\n",
      "7/7 - 0s - loss: 0.4918 - accuracy: 0.7857 - val_loss: 0.4189 - val_accuracy: 0.8261 - lr: 0.0086 - 321ms/epoch - 46ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 58/200\n",
      "7/7 - 0s - loss: 0.4118 - accuracy: 0.8304 - val_loss: 0.3004 - val_accuracy: 0.9130 - lr: 0.0084 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 59/200\n",
      "7/7 - 1s - loss: 0.4527 - accuracy: 0.7946 - val_loss: 0.3850 - val_accuracy: 0.7826 - lr: 0.0082 - 1s/epoch - 201ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 60/200\n",
      "7/7 - 1s - loss: 0.5132 - accuracy: 0.7902 - val_loss: 0.2703 - val_accuracy: 0.9130 - lr: 0.0080 - 574ms/epoch - 82ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 61/200\n",
      "7/7 - 2s - loss: 0.3643 - accuracy: 0.8482 - val_loss: 0.3912 - val_accuracy: 0.8261 - lr: 0.0078 - 2s/epoch - 227ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 62/200\n",
      "7/7 - 1s - loss: 0.3747 - accuracy: 0.8304 - val_loss: 0.2387 - val_accuracy: 0.9130 - lr: 0.0076 - 504ms/epoch - 72ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 63/200\n",
      "7/7 - 1s - loss: 0.3719 - accuracy: 0.8170 - val_loss: 0.2165 - val_accuracy: 0.9130 - lr: 0.0074 - 1s/epoch - 186ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 64/200\n",
      "7/7 - 0s - loss: 0.3339 - accuracy: 0.8705 - val_loss: 0.2497 - val_accuracy: 0.9130 - lr: 0.0072 - 497ms/epoch - 71ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 65/200\n",
      "7/7 - 0s - loss: 0.3703 - accuracy: 0.8750 - val_loss: 0.2539 - val_accuracy: 0.9130 - lr: 0.0070 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 66/200\n",
      "7/7 - 1s - loss: 0.4003 - accuracy: 0.8125 - val_loss: 0.2520 - val_accuracy: 0.9130 - lr: 0.0068 - 818ms/epoch - 117ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 67/200\n",
      "7/7 - 0s - loss: 0.3451 - accuracy: 0.8527 - val_loss: 0.2291 - val_accuracy: 0.9565 - lr: 0.0066 - 491ms/epoch - 70ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 68/200\n",
      "7/7 - 0s - loss: 0.3880 - accuracy: 0.8348 - val_loss: 0.2330 - val_accuracy: 0.9565 - lr: 0.0064 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 69/200\n",
      "7/7 - 1s - loss: 0.3393 - accuracy: 0.8482 - val_loss: 0.2168 - val_accuracy: 0.9565 - lr: 0.0062 - 1s/epoch - 184ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 70/200\n",
      "7/7 - 0s - loss: 0.3830 - accuracy: 0.8348 - val_loss: 0.2119 - val_accuracy: 0.9565 - lr: 0.0060 - 490ms/epoch - 70ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 71/200\n",
      "7/7 - 0s - loss: 0.3322 - accuracy: 0.8393 - val_loss: 0.3565 - val_accuracy: 0.8261 - lr: 0.0058 - 398ms/epoch - 57ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 72/200\n",
      "7/7 - 0s - loss: 0.3841 - accuracy: 0.8482 - val_loss: 0.2160 - val_accuracy: 0.9565 - lr: 0.0056 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 73/200\n",
      "7/7 - 1s - loss: 0.3575 - accuracy: 0.8304 - val_loss: 0.2522 - val_accuracy: 0.8696 - lr: 0.0054 - 889ms/epoch - 127ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 74/200\n",
      "7/7 - 0s - loss: 0.3195 - accuracy: 0.8616 - val_loss: 0.2429 - val_accuracy: 0.9130 - lr: 0.0052 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 75/200\n",
      "7/7 - 0s - loss: 0.2643 - accuracy: 0.8884 - val_loss: 0.2848 - val_accuracy: 0.8696 - lr: 0.0050 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 76/200\n",
      "7/7 - 0s - loss: 0.2892 - accuracy: 0.8750 - val_loss: 0.2649 - val_accuracy: 0.8696 - lr: 0.0048 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 77/200\n",
      "7/7 - 1s - loss: 0.3731 - accuracy: 0.8214 - val_loss: 0.1913 - val_accuracy: 0.9565 - lr: 0.0046 - 1s/epoch - 198ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 78/200\n",
      "7/7 - 0s - loss: 0.3079 - accuracy: 0.8571 - val_loss: 0.1982 - val_accuracy: 0.9565 - lr: 0.0044 - 460ms/epoch - 66ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 79/200\n",
      "7/7 - 0s - loss: 0.3049 - accuracy: 0.8661 - val_loss: 0.3614 - val_accuracy: 0.8261 - lr: 0.0042 - 371ms/epoch - 53ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 80/200\n",
      "7/7 - 0s - loss: 0.3484 - accuracy: 0.8616 - val_loss: 0.2062 - val_accuracy: 0.9565 - lr: 0.0040 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 81/200\n",
      "7/7 - 0s - loss: 0.2942 - accuracy: 0.8482 - val_loss: 0.1961 - val_accuracy: 0.9565 - lr: 0.0038 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 82/200\n",
      "7/7 - 0s - loss: 0.3496 - accuracy: 0.8438 - val_loss: 0.1902 - val_accuracy: 0.9565 - lr: 0.0036 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 83/200\n",
      "7/7 - 1s - loss: 0.3311 - accuracy: 0.8438 - val_loss: 0.2437 - val_accuracy: 0.8696 - lr: 0.0034 - 1s/epoch - 180ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 84/200\n",
      "7/7 - 1s - loss: 0.2655 - accuracy: 0.8839 - val_loss: 0.1655 - val_accuracy: 0.9565 - lr: 0.0032 - 513ms/epoch - 73ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 85/200\n",
      "7/7 - 1s - loss: 0.2859 - accuracy: 0.8705 - val_loss: 0.1828 - val_accuracy: 0.9565 - lr: 0.0030 - 831ms/epoch - 119ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 86/200\n",
      "7/7 - 1s - loss: 0.3138 - accuracy: 0.8616 - val_loss: 0.1663 - val_accuracy: 0.9565 - lr: 0.0028 - 502ms/epoch - 72ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 87/200\n",
      "7/7 - 0s - loss: 0.2683 - accuracy: 0.8929 - val_loss: 0.1720 - val_accuracy: 0.9565 - lr: 0.0026 - 331ms/epoch - 47ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 88/200\n",
      "7/7 - 0s - loss: 0.3125 - accuracy: 0.8661 - val_loss: 0.1965 - val_accuracy: 0.9565 - lr: 0.0024 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 89/200\n",
      "7/7 - 0s - loss: 0.2631 - accuracy: 0.8795 - val_loss: 0.1996 - val_accuracy: 0.9130 - lr: 0.0022 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 90/200\n",
      "7/7 - 1s - loss: 0.2809 - accuracy: 0.8884 - val_loss: 0.1833 - val_accuracy: 0.9565 - lr: 0.0020 - 945ms/epoch - 135ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 91/200\n",
      "7/7 - 0s - loss: 0.2604 - accuracy: 0.8929 - val_loss: 0.1981 - val_accuracy: 0.9130 - lr: 0.0018 - 489ms/epoch - 70ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 92/200\n",
      "7/7 - 1s - loss: 0.2926 - accuracy: 0.8839 - val_loss: 0.1760 - val_accuracy: 0.9565 - lr: 0.0016 - 927ms/epoch - 132ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 93/200\n",
      "7/7 - 1s - loss: 0.2481 - accuracy: 0.8929 - val_loss: 0.1793 - val_accuracy: 0.9565 - lr: 0.0014 - 542ms/epoch - 77ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 94/200\n",
      "7/7 - 2s - loss: 0.2330 - accuracy: 0.9152 - val_loss: 0.1783 - val_accuracy: 0.9565 - lr: 0.0012 - 2s/epoch - 228ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 95/200\n",
      "7/7 - 0s - loss: 0.3375 - accuracy: 0.8661 - val_loss: 0.1752 - val_accuracy: 0.9565 - lr: 0.0010 - 488ms/epoch - 70ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 96/200\n",
      "7/7 - 2s - loss: 0.3407 - accuracy: 0.8795 - val_loss: 0.1760 - val_accuracy: 0.9565 - lr: 8.0009e-04 - 2s/epoch - 236ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 97/200\n",
      "7/7 - 0s - loss: 0.3116 - accuracy: 0.8482 - val_loss: 0.1781 - val_accuracy: 0.9565 - lr: 6.0009e-04 - 471ms/epoch - 67ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 98/200\n",
      "7/7 - 0s - loss: 0.3214 - accuracy: 0.8795 - val_loss: 0.1994 - val_accuracy: 0.9130 - lr: 4.0010e-04 - 331ms/epoch - 47ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 99/200\n",
      "7/7 - 1s - loss: 0.2301 - accuracy: 0.8973 - val_loss: 0.1991 - val_accuracy: 0.9565 - lr: 2.0010e-04 - 1s/epoch - 165ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 100/200\n",
      "7/7 - 1s - loss: 0.2728 - accuracy: 0.8973 - val_loss: 0.1941 - val_accuracy: 0.9565 - lr: 1.0000e-07 - 506ms/epoch - 72ms/step\n",
      "Learning rate: 0.00000010\n",
      "Epoch 101/200\n",
      "7/7 - 0s - loss: 0.2871 - accuracy: 0.8839 - val_loss: 0.1901 - val_accuracy: 0.9565 - lr: 2.0010e-04 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 102/200\n",
      "7/7 - 1s - loss: 0.2703 - accuracy: 0.8661 - val_loss: 0.1866 - val_accuracy: 0.9565 - lr: 4.0010e-04 - 878ms/epoch - 125ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 103/200\n",
      "7/7 - 0s - loss: 0.3440 - accuracy: 0.8438 - val_loss: 0.1986 - val_accuracy: 0.9565 - lr: 6.0009e-04 - 480ms/epoch - 69ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 104/200\n",
      "7/7 - 0s - loss: 0.2659 - accuracy: 0.9018 - val_loss: 0.1919 - val_accuracy: 0.9565 - lr: 8.0009e-04 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 105/200\n",
      "7/7 - 0s - loss: 0.2518 - accuracy: 0.9107 - val_loss: 0.2219 - val_accuracy: 0.9130 - lr: 0.0010 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 106/200\n",
      "7/7 - 0s - loss: 0.2681 - accuracy: 0.9062 - val_loss: 0.1780 - val_accuracy: 0.9565 - lr: 0.0012 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 107/200\n",
      "7/7 - 1s - loss: 0.3004 - accuracy: 0.8616 - val_loss: 0.1855 - val_accuracy: 0.9565 - lr: 0.0014 - 1s/epoch - 164ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 108/200\n",
      "7/7 - 0s - loss: 0.3409 - accuracy: 0.8616 - val_loss: 0.1980 - val_accuracy: 0.9565 - lr: 0.0016 - 353ms/epoch - 50ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 109/200\n",
      "7/7 - 1s - loss: 0.2532 - accuracy: 0.8839 - val_loss: 0.2015 - val_accuracy: 0.9130 - lr: 0.0018 - 1s/epoch - 169ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 110/200\n",
      "7/7 - 0s - loss: 0.2496 - accuracy: 0.8929 - val_loss: 0.2095 - val_accuracy: 0.9130 - lr: 0.0020 - 492ms/epoch - 70ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 111/200\n",
      "7/7 - 0s - loss: 0.2605 - accuracy: 0.8750 - val_loss: 0.1896 - val_accuracy: 0.9130 - lr: 0.0022 - 321ms/epoch - 46ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 112/200\n",
      "7/7 - 0s - loss: 0.3034 - accuracy: 0.8839 - val_loss: 0.2028 - val_accuracy: 0.9565 - lr: 0.0024 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 113/200\n",
      "7/7 - 0s - loss: 0.2915 - accuracy: 0.8884 - val_loss: 0.2230 - val_accuracy: 0.9130 - lr: 0.0026 - 479ms/epoch - 68ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 114/200\n",
      "7/7 - 1s - loss: 0.2950 - accuracy: 0.8884 - val_loss: 0.2164 - val_accuracy: 0.9130 - lr: 0.0028 - 1s/epoch - 155ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 115/200\n",
      "7/7 - 2s - loss: 0.2181 - accuracy: 0.9018 - val_loss: 0.2300 - val_accuracy: 0.8696 - lr: 0.0030 - 2s/epoch - 221ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 116/200\n",
      "7/7 - 1s - loss: 0.2539 - accuracy: 0.8884 - val_loss: 0.1762 - val_accuracy: 0.9565 - lr: 0.0032 - 542ms/epoch - 77ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 117/200\n",
      "7/7 - 1s - loss: 0.2611 - accuracy: 0.8973 - val_loss: 0.2659 - val_accuracy: 0.8696 - lr: 0.0034 - 697ms/epoch - 100ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 118/200\n",
      "7/7 - 0s - loss: 0.3847 - accuracy: 0.8661 - val_loss: 0.2177 - val_accuracy: 0.9130 - lr: 0.0036 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 119/200\n",
      "7/7 - 0s - loss: 0.2601 - accuracy: 0.8929 - val_loss: 0.1757 - val_accuracy: 0.9565 - lr: 0.0038 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 120/200\n",
      "7/7 - 0s - loss: 0.2611 - accuracy: 0.9018 - val_loss: 0.2430 - val_accuracy: 0.8696 - lr: 0.0040 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 121/200\n",
      "7/7 - 0s - loss: 0.3344 - accuracy: 0.8482 - val_loss: 0.3792 - val_accuracy: 0.8261 - lr: 0.0042 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 122/200\n",
      "7/7 - 0s - loss: 0.3993 - accuracy: 0.8348 - val_loss: 0.2252 - val_accuracy: 0.9130 - lr: 0.0044 - 497ms/epoch - 71ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 123/200\n",
      "7/7 - 1s - loss: 0.2786 - accuracy: 0.8929 - val_loss: 0.2632 - val_accuracy: 0.9130 - lr: 0.0046 - 1s/epoch - 195ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 124/200\n",
      "7/7 - 0s - loss: 0.2780 - accuracy: 0.8705 - val_loss: 0.2480 - val_accuracy: 0.8696 - lr: 0.0048 - 393ms/epoch - 56ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 125/200\n",
      "7/7 - 0s - loss: 0.3259 - accuracy: 0.8750 - val_loss: 0.2145 - val_accuracy: 0.9130 - lr: 0.0050 - 344ms/epoch - 49ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 126/200\n",
      "7/7 - 0s - loss: 0.2799 - accuracy: 0.9152 - val_loss: 0.1741 - val_accuracy: 0.9565 - lr: 0.0052 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 127/200\n",
      "7/7 - 0s - loss: 0.3185 - accuracy: 0.8661 - val_loss: 0.2101 - val_accuracy: 0.9565 - lr: 0.0054 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 128/200\n",
      "7/7 - 0s - loss: 0.2485 - accuracy: 0.9107 - val_loss: 0.2209 - val_accuracy: 0.9130 - lr: 0.0056 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 129/200\n",
      "7/7 - 1s - loss: 0.3579 - accuracy: 0.8705 - val_loss: 0.2526 - val_accuracy: 0.8696 - lr: 0.0058 - 506ms/epoch - 72ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 130/200\n",
      "7/7 - 1s - loss: 0.3438 - accuracy: 0.8304 - val_loss: 0.2383 - val_accuracy: 0.8696 - lr: 0.0060 - 1s/epoch - 205ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 131/200\n",
      "7/7 - 1s - loss: 0.3169 - accuracy: 0.8571 - val_loss: 0.2000 - val_accuracy: 0.9565 - lr: 0.0062 - 514ms/epoch - 73ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 132/200\n",
      "7/7 - 1s - loss: 0.3066 - accuracy: 0.8750 - val_loss: 0.2720 - val_accuracy: 0.9130 - lr: 0.0064 - 643ms/epoch - 92ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 133/200\n",
      "7/7 - 0s - loss: 0.2653 - accuracy: 0.8795 - val_loss: 0.2397 - val_accuracy: 0.9130 - lr: 0.0066 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 134/200\n",
      "7/7 - 1s - loss: 0.3126 - accuracy: 0.8571 - val_loss: 0.2113 - val_accuracy: 0.9130 - lr: 0.0068 - 965ms/epoch - 138ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 135/200\n",
      "7/7 - 1s - loss: 0.2840 - accuracy: 0.8750 - val_loss: 0.2305 - val_accuracy: 0.9130 - lr: 0.0070 - 528ms/epoch - 75ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 136/200\n",
      "7/7 - 2s - loss: 0.3032 - accuracy: 0.8571 - val_loss: 0.2037 - val_accuracy: 0.9130 - lr: 0.0072 - 2s/epoch - 219ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 137/200\n",
      "7/7 - 0s - loss: 0.2795 - accuracy: 0.8884 - val_loss: 0.1708 - val_accuracy: 0.9565 - lr: 0.0074 - 489ms/epoch - 70ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 138/200\n",
      "7/7 - 0s - loss: 0.2487 - accuracy: 0.9196 - val_loss: 0.3092 - val_accuracy: 0.8696 - lr: 0.0076 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 139/200\n",
      "7/7 - 0s - loss: 0.2868 - accuracy: 0.9018 - val_loss: 0.2337 - val_accuracy: 0.9130 - lr: 0.0078 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 140/200\n",
      "7/7 - 1s - loss: 0.2475 - accuracy: 0.9018 - val_loss: 0.2961 - val_accuracy: 0.8696 - lr: 0.0080 - 656ms/epoch - 94ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 141/200\n",
      "7/7 - 0s - loss: 0.2728 - accuracy: 0.8884 - val_loss: 0.1494 - val_accuracy: 0.9565 - lr: 0.0082 - 456ms/epoch - 65ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 142/200\n",
      "7/7 - 0s - loss: 0.2969 - accuracy: 0.8884 - val_loss: 0.2740 - val_accuracy: 0.8696 - lr: 0.0084 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 143/200\n",
      "7/7 - 0s - loss: 0.2908 - accuracy: 0.8750 - val_loss: 0.3146 - val_accuracy: 0.8696 - lr: 0.0086 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 144/200\n",
      "7/7 - 1s - loss: 0.2852 - accuracy: 0.8661 - val_loss: 0.2008 - val_accuracy: 0.9130 - lr: 0.0088 - 1s/epoch - 209ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 145/200\n",
      "7/7 - 0s - loss: 0.3063 - accuracy: 0.8795 - val_loss: 0.1924 - val_accuracy: 0.9565 - lr: 0.0090 - 419ms/epoch - 60ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 146/200\n",
      "7/7 - 0s - loss: 0.3427 - accuracy: 0.8482 - val_loss: 0.2141 - val_accuracy: 0.9565 - lr: 0.0092 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 147/200\n",
      "7/7 - 0s - loss: 0.2741 - accuracy: 0.8839 - val_loss: 0.2324 - val_accuracy: 0.9130 - lr: 0.0094 - 356ms/epoch - 51ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 148/200\n",
      "7/7 - 1s - loss: 0.3241 - accuracy: 0.8616 - val_loss: 0.2108 - val_accuracy: 0.9130 - lr: 0.0096 - 622ms/epoch - 89ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 149/200\n",
      "7/7 - 0s - loss: 0.2573 - accuracy: 0.9018 - val_loss: 0.1482 - val_accuracy: 0.9565 - lr: 0.0098 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 150/200\n",
      "7/7 - 0s - loss: 0.2584 - accuracy: 0.8929 - val_loss: 0.2117 - val_accuracy: 0.9565 - lr: 0.0100 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 151/200\n",
      "7/7 - 0s - loss: 0.2761 - accuracy: 0.8973 - val_loss: 0.3659 - val_accuracy: 0.8261 - lr: 0.0098 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 152/200\n",
      "7/7 - 1s - loss: 0.2533 - accuracy: 0.9107 - val_loss: 0.1822 - val_accuracy: 0.9565 - lr: 0.0096 - 1s/epoch - 158ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 153/200\n",
      "7/7 - 0s - loss: 0.2838 - accuracy: 0.8795 - val_loss: 0.1906 - val_accuracy: 0.9565 - lr: 0.0094 - 364ms/epoch - 52ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 154/200\n",
      "7/7 - 0s - loss: 0.2414 - accuracy: 0.9107 - val_loss: 0.1686 - val_accuracy: 0.9565 - lr: 0.0092 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 155/200\n",
      "7/7 - 0s - loss: 0.2675 - accuracy: 0.8705 - val_loss: 0.2042 - val_accuracy: 0.9130 - lr: 0.0090 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 156/200\n",
      "7/7 - 1s - loss: 0.2946 - accuracy: 0.8750 - val_loss: 0.2080 - val_accuracy: 0.9130 - lr: 0.0088 - 831ms/epoch - 119ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 157/200\n",
      "7/7 - 0s - loss: 0.3223 - accuracy: 0.8750 - val_loss: 0.2210 - val_accuracy: 0.9130 - lr: 0.0086 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 158/200\n",
      "7/7 - 0s - loss: 0.2547 - accuracy: 0.8884 - val_loss: 0.1841 - val_accuracy: 0.9565 - lr: 0.0084 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 159/200\n",
      "7/7 - 0s - loss: 0.2528 - accuracy: 0.9062 - val_loss: 0.1798 - val_accuracy: 0.9130 - lr: 0.0082 - 359ms/epoch - 51ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 160/200\n",
      "7/7 - 2s - loss: 0.3723 - accuracy: 0.8571 - val_loss: 0.2700 - val_accuracy: 0.8696 - lr: 0.0080 - 2s/epoch - 214ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 161/200\n",
      "7/7 - 0s - loss: 0.2762 - accuracy: 0.8795 - val_loss: 0.2870 - val_accuracy: 0.8261 - lr: 0.0078 - 466ms/epoch - 67ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 162/200\n",
      "7/7 - 0s - loss: 0.2118 - accuracy: 0.9107 - val_loss: 0.1972 - val_accuracy: 0.9565 - lr: 0.0076 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 163/200\n",
      "7/7 - 0s - loss: 0.2117 - accuracy: 0.9241 - val_loss: 0.1937 - val_accuracy: 0.9130 - lr: 0.0074 - 323ms/epoch - 46ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 164/200\n",
      "7/7 - 1s - loss: 0.2857 - accuracy: 0.8929 - val_loss: 0.1761 - val_accuracy: 0.9565 - lr: 0.0072 - 1s/epoch - 205ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 165/200\n",
      "7/7 - 0s - loss: 0.2614 - accuracy: 0.8929 - val_loss: 0.1977 - val_accuracy: 0.9565 - lr: 0.0070 - 484ms/epoch - 69ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 166/200\n",
      "7/7 - 0s - loss: 0.2801 - accuracy: 0.8839 - val_loss: 0.1617 - val_accuracy: 0.9565 - lr: 0.0068 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 167/200\n",
      "7/7 - 0s - loss: 0.2368 - accuracy: 0.9018 - val_loss: 0.2061 - val_accuracy: 0.9130 - lr: 0.0066 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 168/200\n",
      "7/7 - 1s - loss: 0.2605 - accuracy: 0.8795 - val_loss: 0.2536 - val_accuracy: 0.8696 - lr: 0.0064 - 1s/epoch - 196ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 169/200\n",
      "7/7 - 0s - loss: 0.2509 - accuracy: 0.8973 - val_loss: 0.1732 - val_accuracy: 0.9565 - lr: 0.0062 - 342ms/epoch - 49ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 170/200\n",
      "7/7 - 0s - loss: 0.2884 - accuracy: 0.8661 - val_loss: 0.2401 - val_accuracy: 0.9130 - lr: 0.0060 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 171/200\n",
      "7/7 - 0s - loss: 0.2522 - accuracy: 0.8795 - val_loss: 0.1473 - val_accuracy: 0.9565 - lr: 0.0058 - 361ms/epoch - 52ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 172/200\n",
      "7/7 - 1s - loss: 0.2538 - accuracy: 0.9018 - val_loss: 0.2258 - val_accuracy: 0.9130 - lr: 0.0056 - 584ms/epoch - 83ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 173/200\n",
      "7/7 - 0s - loss: 0.2280 - accuracy: 0.9062 - val_loss: 0.1792 - val_accuracy: 0.9565 - lr: 0.0054 - 346ms/epoch - 49ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 174/200\n",
      "7/7 - 0s - loss: 0.2385 - accuracy: 0.9018 - val_loss: 0.3100 - val_accuracy: 0.8261 - lr: 0.0052 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 175/200\n",
      "7/7 - 0s - loss: 0.2213 - accuracy: 0.9107 - val_loss: 0.1600 - val_accuracy: 0.9565 - lr: 0.0050 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 176/200\n",
      "7/7 - 0s - loss: 0.2504 - accuracy: 0.9018 - val_loss: 0.1704 - val_accuracy: 0.9130 - lr: 0.0048 - 482ms/epoch - 69ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 177/200\n",
      "7/7 - 0s - loss: 0.2208 - accuracy: 0.9241 - val_loss: 0.2138 - val_accuracy: 0.9130 - lr: 0.0046 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 178/200\n",
      "7/7 - 0s - loss: 0.2399 - accuracy: 0.9062 - val_loss: 0.2136 - val_accuracy: 0.9130 - lr: 0.0044 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 179/200\n",
      "7/7 - 0s - loss: 0.2415 - accuracy: 0.8973 - val_loss: 0.2009 - val_accuracy: 0.9130 - lr: 0.0042 - 323ms/epoch - 46ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 180/200\n",
      "7/7 - 0s - loss: 0.1941 - accuracy: 0.9196 - val_loss: 0.1695 - val_accuracy: 0.9565 - lr: 0.0040 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 181/200\n",
      "7/7 - 0s - loss: 0.2818 - accuracy: 0.8795 - val_loss: 0.1613 - val_accuracy: 0.9565 - lr: 0.0038 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 182/200\n",
      "7/7 - 1s - loss: 0.2438 - accuracy: 0.9196 - val_loss: 0.1858 - val_accuracy: 0.9565 - lr: 0.0036 - 1s/epoch - 179ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 183/200\n",
      "7/7 - 0s - loss: 0.2300 - accuracy: 0.8929 - val_loss: 0.1907 - val_accuracy: 0.9130 - lr: 0.0034 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 184/200\n",
      "7/7 - 0s - loss: 0.2474 - accuracy: 0.8929 - val_loss: 0.1427 - val_accuracy: 0.9565 - lr: 0.0032 - 358ms/epoch - 51ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 185/200\n",
      "7/7 - 0s - loss: 0.2239 - accuracy: 0.9018 - val_loss: 0.1716 - val_accuracy: 0.9565 - lr: 0.0030 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 186/200\n",
      "7/7 - 0s - loss: 0.2106 - accuracy: 0.9062 - val_loss: 0.1748 - val_accuracy: 0.9565 - lr: 0.0028 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 187/200\n",
      "7/7 - 0s - loss: 0.2490 - accuracy: 0.8973 - val_loss: 0.1746 - val_accuracy: 0.9130 - lr: 0.0026 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 188/200\n",
      "7/7 - 0s - loss: 0.2573 - accuracy: 0.8884 - val_loss: 0.1860 - val_accuracy: 0.9130 - lr: 0.0024 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 189/200\n",
      "7/7 - 0s - loss: 0.2213 - accuracy: 0.9196 - val_loss: 0.1561 - val_accuracy: 0.9565 - lr: 0.0022 - 450ms/epoch - 64ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 190/200\n",
      "7/7 - 1s - loss: 0.2257 - accuracy: 0.8884 - val_loss: 0.1617 - val_accuracy: 0.9565 - lr: 0.0020 - 1s/epoch - 189ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 191/200\n",
      "7/7 - 1s - loss: 0.2169 - accuracy: 0.9286 - val_loss: 0.1671 - val_accuracy: 0.9565 - lr: 0.0018 - 1s/epoch - 189ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 192/200\n",
      "7/7 - 0s - loss: 0.2388 - accuracy: 0.9062 - val_loss: 0.1778 - val_accuracy: 0.9565 - lr: 0.0016 - 499ms/epoch - 71ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 193/200\n",
      "7/7 - 0s - loss: 0.2575 - accuracy: 0.8929 - val_loss: 0.1778 - val_accuracy: 0.9130 - lr: 0.0014 - 343ms/epoch - 49ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 194/200\n",
      "7/7 - 0s - loss: 0.2103 - accuracy: 0.9196 - val_loss: 0.1692 - val_accuracy: 0.9565 - lr: 0.0012 - 369ms/epoch - 53ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 195/200\n",
      "7/7 - 1s - loss: 0.2698 - accuracy: 0.8929 - val_loss: 0.1593 - val_accuracy: 0.9565 - lr: 0.0010 - 1s/epoch - 209ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 196/200\n",
      "7/7 - 0s - loss: 0.2730 - accuracy: 0.8973 - val_loss: 0.1605 - val_accuracy: 0.9565 - lr: 8.0009e-04 - 483ms/epoch - 69ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 197/200\n",
      "7/7 - 0s - loss: 0.2396 - accuracy: 0.9107 - val_loss: 0.1564 - val_accuracy: 0.9565 - lr: 6.0009e-04 - 346ms/epoch - 49ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 198/200\n",
      "7/7 - 0s - loss: 0.2469 - accuracy: 0.8839 - val_loss: 0.1620 - val_accuracy: 0.9565 - lr: 4.0010e-04 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 199/200\n",
      "7/7 - 0s - loss: 0.2413 - accuracy: 0.9062 - val_loss: 0.1610 - val_accuracy: 0.9565 - lr: 2.0010e-04 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 200/200\n",
      "7/7 - 0s - loss: 0.1829 - accuracy: 0.9196 - val_loss: 0.1602 - val_accuracy: 0.9565 - lr: 1.0000e-07 - 336ms/epoch - 48ms/step\n",
      "\n",
      "======================BEST:======================\n",
      "    epoch      loss      acc  val_loss   val_acc  rocp    f1       rfa\n",
      "36   37.0  0.350316  0.84375  0.232462  0.956522   1.0  0.96  0.970783\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95        11\n",
      "           1       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.96        23\n",
      "   macro avg       0.96      0.95      0.96        23\n",
      "weighted avg       0.96      0.96      0.96        23\n",
      "\n",
      "[[10  1]\n",
      " [ 0 12]]\n",
      "ACCURACY: 0.9565217391304348\n",
      "PRECISION: 0.9230769230769231\n",
      "RECALL: 1.0\n",
      "F1: 0.9600000000000001\n",
      "ROC_AUC(Pr.): 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95        11\n",
      "           1       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.96        23\n",
      "   macro avg       0.96      0.95      0.96        23\n",
      "weighted avg       0.96      0.96      0.96        23\n",
      "\n",
      "[[10  1]\n",
      " [ 0 12]]\n",
      "ACCURACY: 0.9565217391304348\n",
      "PRECISION: 0.9230769230769231\n",
      "RECALL: 1.0\n",
      "F1: 0.9600000000000001\n",
      "ROC_AUC(Pr.): 1.0\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95        11\n",
      "           1       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.96        23\n",
      "   macro avg       0.96      0.95      0.96        23\n",
      "weighted avg       0.96      0.96      0.96        23\n",
      "\n",
      "[[10  1]\n",
      " [ 0 12]]\n",
      "ACCURACY: 0.9565217391304348\n",
      "PRECISION: 0.9230769230769231\n",
      "RECALL: 1.0\n",
      "F1: 0.9600000000000001\n",
      "ROC_AUC(Pr.): 1.0\n",
      "\n",
      "=====================CV[3]========================\n",
      "\n",
      "207 train samples\n",
      "23 test samples\n",
      "\n",
      "Over sampling: 207 -> 208\n",
      "Add train samples: 16\n",
      "224 new train samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\tf28\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.00000010\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_28484\\4287042063.py:159: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================EPOCH 000001:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "0    1.0  0.693825  0.477679   0.69272  0.521739  0.704545  0.421053  0.541341\n",
      "RFA : 0.5413407512543875\n",
      "7/7 - 6s - loss: 0.6938 - accuracy: 0.4777 - val_loss: 0.6927 - val_accuracy: 0.5217 - lr: 2.0010e-04 - 6s/epoch - 856ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 2/200\n",
      "7/7 - 0s - loss: 0.6935 - accuracy: 0.4866 - val_loss: 0.6930 - val_accuracy: 0.4783 - lr: 4.0010e-04 - 359ms/epoch - 51ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 3/200\n",
      "======================EPOCH 000003:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "2    3.0  0.690654  0.566964  0.689867  0.565217  0.833333  0.705882  0.694885\n",
      "RFA : 0.6948849050437702\n",
      "7/7 - 0s - loss: 0.6907 - accuracy: 0.5670 - val_loss: 0.6899 - val_accuracy: 0.5652 - lr: 6.0009e-04 - 392ms/epoch - 56ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 4/200\n",
      "7/7 - 0s - loss: 0.6879 - accuracy: 0.5714 - val_loss: 0.6845 - val_accuracy: 0.5217 - lr: 8.0009e-04 - 374ms/epoch - 53ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 5/200\n",
      "======================EPOCH 000005:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp    f1       rfa\n",
      "4    5.0  0.672896  0.642857  0.649796  0.652174  0.833333  0.75  0.740761\n",
      "RFA : 0.7407608777284622\n",
      "7/7 - 0s - loss: 0.6729 - accuracy: 0.6429 - val_loss: 0.6498 - val_accuracy: 0.6522 - lr: 0.0010 - 479ms/epoch - 68ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 6/200\n",
      "======================EPOCH 000006:======================\n",
      "   epoch      loss       acc  val_loss   val_acc     rocp        f1       rfa\n",
      "5    6.0  0.594798  0.767857  0.559091  0.826087  0.80303  0.846154  0.826193\n",
      "RFA : 0.8261933654963554\n",
      "7/7 - 1s - loss: 0.5948 - accuracy: 0.7679 - val_loss: 0.5591 - val_accuracy: 0.8261 - lr: 0.0012 - 1s/epoch - 177ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 7/200\n",
      "7/7 - 1s - loss: 0.4817 - accuracy: 0.8036 - val_loss: 0.5154 - val_accuracy: 0.7826 - lr: 0.0014 - 1s/epoch - 208ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 8/200\n",
      "7/7 - 0s - loss: 0.4566 - accuracy: 0.7768 - val_loss: 0.4813 - val_accuracy: 0.7391 - lr: 0.0016 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 9/200\n",
      "7/7 - 0s - loss: 0.4071 - accuracy: 0.7991 - val_loss: 0.4853 - val_accuracy: 0.7826 - lr: 0.0018 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 10/200\n",
      "7/7 - 1s - loss: 0.3714 - accuracy: 0.8304 - val_loss: 0.4504 - val_accuracy: 0.7391 - lr: 0.0020 - 1s/epoch - 164ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 11/200\n",
      "======================EPOCH 000011:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "10   11.0  0.367304  0.821429  0.528653  0.782609  0.901515  0.827586  0.834023\n",
      "RFA : 0.8340227586255178\n",
      "7/7 - 0s - loss: 0.3673 - accuracy: 0.8214 - val_loss: 0.5287 - val_accuracy: 0.7826 - lr: 0.0022 - 363ms/epoch - 52ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 12/200\n",
      "7/7 - 0s - loss: 0.3473 - accuracy: 0.8393 - val_loss: 0.7498 - val_accuracy: 0.7391 - lr: 0.0024 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 13/200\n",
      "7/7 - 1s - loss: 0.4041 - accuracy: 0.8036 - val_loss: 0.7106 - val_accuracy: 0.6957 - lr: 0.0026 - 512ms/epoch - 73ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 14/200\n",
      "7/7 - 1s - loss: 0.3877 - accuracy: 0.8393 - val_loss: 0.5496 - val_accuracy: 0.7391 - lr: 0.0028 - 1s/epoch - 177ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 15/200\n",
      "7/7 - 0s - loss: 0.3036 - accuracy: 0.8438 - val_loss: 0.4811 - val_accuracy: 0.7391 - lr: 0.0030 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 16/200\n",
      "7/7 - 0s - loss: 0.2871 - accuracy: 0.8571 - val_loss: 0.9707 - val_accuracy: 0.6957 - lr: 0.0032 - 351ms/epoch - 50ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 17/200\n",
      "7/7 - 2s - loss: 0.3845 - accuracy: 0.8125 - val_loss: 0.6977 - val_accuracy: 0.7391 - lr: 0.0034 - 2s/epoch - 226ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 18/200\n",
      "7/7 - 0s - loss: 0.3159 - accuracy: 0.8527 - val_loss: 0.4775 - val_accuracy: 0.7826 - lr: 0.0036 - 354ms/epoch - 51ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 19/200\n",
      "======================EPOCH 000019:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "18   19.0  0.322161  0.866071  0.464015  0.826087  0.901515  0.857143  0.859585\n",
      "RFA : 0.8595849738879637\n",
      "7/7 - 0s - loss: 0.3222 - accuracy: 0.8661 - val_loss: 0.4640 - val_accuracy: 0.8261 - lr: 0.0038 - 377ms/epoch - 54ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 20/200\n",
      "7/7 - 0s - loss: 0.2338 - accuracy: 0.9196 - val_loss: 0.4788 - val_accuracy: 0.7826 - lr: 0.0040 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 21/200\n",
      "7/7 - 1s - loss: 0.5243 - accuracy: 0.7812 - val_loss: 0.4577 - val_accuracy: 0.7826 - lr: 0.0042 - 923ms/epoch - 132ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 22/200\n",
      "7/7 - 1s - loss: 0.3620 - accuracy: 0.8348 - val_loss: 0.4350 - val_accuracy: 0.7391 - lr: 0.0044 - 502ms/epoch - 72ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 23/200\n",
      "7/7 - 0s - loss: 0.2925 - accuracy: 0.8795 - val_loss: 0.4916 - val_accuracy: 0.7826 - lr: 0.0046 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 24/200\n",
      "7/7 - 0s - loss: 0.3132 - accuracy: 0.8705 - val_loss: 0.4601 - val_accuracy: 0.7391 - lr: 0.0048 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 25/200\n",
      "7/7 - 0s - loss: 0.3241 - accuracy: 0.8438 - val_loss: 0.7272 - val_accuracy: 0.6957 - lr: 0.0050 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 26/200\n",
      "7/7 - 0s - loss: 0.3326 - accuracy: 0.8482 - val_loss: 0.5152 - val_accuracy: 0.7391 - lr: 0.0052 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 27/200\n",
      "7/7 - 0s - loss: 0.2850 - accuracy: 0.8795 - val_loss: 0.4216 - val_accuracy: 0.7826 - lr: 0.0054 - 481ms/epoch - 69ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 28/200\n",
      "7/7 - 0s - loss: 0.2670 - accuracy: 0.8795 - val_loss: 0.5982 - val_accuracy: 0.7391 - lr: 0.0056 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 29/200\n",
      "7/7 - 0s - loss: 0.3072 - accuracy: 0.8795 - val_loss: 0.6183 - val_accuracy: 0.7391 - lr: 0.0058 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 30/200\n",
      "7/7 - 2s - loss: 0.3291 - accuracy: 0.8705 - val_loss: 0.5295 - val_accuracy: 0.7391 - lr: 0.0060 - 2s/epoch - 268ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 31/200\n",
      "7/7 - 0s - loss: 0.3758 - accuracy: 0.8125 - val_loss: 0.5223 - val_accuracy: 0.7391 - lr: 0.0062 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 32/200\n",
      "7/7 - 0s - loss: 0.3142 - accuracy: 0.8393 - val_loss: 0.6583 - val_accuracy: 0.7391 - lr: 0.0064 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 33/200\n",
      "7/7 - 2s - loss: 0.2906 - accuracy: 0.8705 - val_loss: 0.6373 - val_accuracy: 0.6957 - lr: 0.0066 - 2s/epoch - 221ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 34/200\n",
      "7/7 - 0s - loss: 0.3392 - accuracy: 0.8482 - val_loss: 0.3971 - val_accuracy: 0.7826 - lr: 0.0068 - 364ms/epoch - 52ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 35/200\n",
      "7/7 - 0s - loss: 0.2775 - accuracy: 0.8705 - val_loss: 0.6525 - val_accuracy: 0.7391 - lr: 0.0070 - 396ms/epoch - 57ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 36/200\n",
      "7/7 - 0s - loss: 0.2918 - accuracy: 0.8839 - val_loss: 0.5396 - val_accuracy: 0.7826 - lr: 0.0072 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 37/200\n",
      "7/7 - 0s - loss: 0.3269 - accuracy: 0.8438 - val_loss: 0.3537 - val_accuracy: 0.7826 - lr: 0.0074 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 38/200\n",
      "7/7 - 0s - loss: 0.2698 - accuracy: 0.9018 - val_loss: 0.4664 - val_accuracy: 0.7391 - lr: 0.0076 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 39/200\n",
      "7/7 - 0s - loss: 0.2235 - accuracy: 0.9152 - val_loss: 0.5194 - val_accuracy: 0.7826 - lr: 0.0078 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 40/200\n",
      "7/7 - 2s - loss: 0.2464 - accuracy: 0.8973 - val_loss: 0.4708 - val_accuracy: 0.7391 - lr: 0.0080 - 2s/epoch - 255ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 41/200\n",
      "7/7 - 0s - loss: 0.2855 - accuracy: 0.8973 - val_loss: 0.6590 - val_accuracy: 0.7391 - lr: 0.0082 - 353ms/epoch - 50ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 42/200\n",
      "7/7 - 0s - loss: 0.4685 - accuracy: 0.8348 - val_loss: 0.4240 - val_accuracy: 0.7826 - lr: 0.0084 - 486ms/epoch - 69ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 43/200\n",
      "7/7 - 0s - loss: 0.3819 - accuracy: 0.8348 - val_loss: 0.5112 - val_accuracy: 0.7391 - lr: 0.0086 - 357ms/epoch - 51ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 44/200\n",
      "7/7 - 0s - loss: 0.3410 - accuracy: 0.8839 - val_loss: 0.3978 - val_accuracy: 0.7826 - lr: 0.0088 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 45/200\n",
      "7/7 - 0s - loss: 0.3242 - accuracy: 0.8571 - val_loss: 0.5457 - val_accuracy: 0.7391 - lr: 0.0090 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 46/200\n",
      "7/7 - 0s - loss: 0.2771 - accuracy: 0.8750 - val_loss: 0.3803 - val_accuracy: 0.7826 - lr: 0.0092 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 47/200\n",
      "7/7 - 0s - loss: 0.3415 - accuracy: 0.8571 - val_loss: 0.7278 - val_accuracy: 0.7826 - lr: 0.0094 - 341ms/epoch - 49ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 48/200\n",
      "7/7 - 1s - loss: 0.3163 - accuracy: 0.8750 - val_loss: 0.4664 - val_accuracy: 0.7826 - lr: 0.0096 - 1s/epoch - 203ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 49/200\n",
      "7/7 - 1s - loss: 0.2745 - accuracy: 0.8750 - val_loss: 0.5584 - val_accuracy: 0.7826 - lr: 0.0098 - 1s/epoch - 169ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 50/200\n",
      "7/7 - 0s - loss: 0.3278 - accuracy: 0.8839 - val_loss: 0.3819 - val_accuracy: 0.7826 - lr: 0.0100 - 352ms/epoch - 50ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 51/200\n",
      "7/7 - 0s - loss: 0.2756 - accuracy: 0.8973 - val_loss: 0.4430 - val_accuracy: 0.7826 - lr: 0.0098 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 52/200\n",
      "7/7 - 1s - loss: 0.3272 - accuracy: 0.8527 - val_loss: 0.5653 - val_accuracy: 0.7391 - lr: 0.0096 - 1s/epoch - 157ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 53/200\n",
      "7/7 - 0s - loss: 3.5274 - accuracy: 0.6607 - val_loss: 2.4820 - val_accuracy: 0.4783 - lr: 0.0094 - 360ms/epoch - 51ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 54/200\n",
      "7/7 - 0s - loss: 100.4346 - accuracy: 0.5491 - val_loss: 58.5721 - val_accuracy: 0.4783 - lr: 0.0092 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 55/200\n",
      "7/7 - 1s - loss: 827.1754 - accuracy: 0.5580 - val_loss: 2089.1191 - val_accuracy: 0.5217 - lr: 0.0090 - 1s/epoch - 174ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 56/200\n",
      "7/7 - 0s - loss: 811.6978 - accuracy: 0.5089 - val_loss: 67.3766 - val_accuracy: 0.4783 - lr: 0.0088 - 490ms/epoch - 70ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 57/200\n",
      "7/7 - 1s - loss: 42.4276 - accuracy: 0.4911 - val_loss: 2.2694 - val_accuracy: 0.6957 - lr: 0.0086 - 507ms/epoch - 72ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 58/200\n",
      "7/7 - 0s - loss: 3.8453 - accuracy: 0.6652 - val_loss: 1.2210 - val_accuracy: 0.7391 - lr: 0.0084 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 59/200\n",
      "7/7 - 0s - loss: 1.6773 - accuracy: 0.6473 - val_loss: 1.3653 - val_accuracy: 0.7826 - lr: 0.0082 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 60/200\n",
      "7/7 - 0s - loss: 0.9874 - accuracy: 0.7723 - val_loss: 1.1453 - val_accuracy: 0.7391 - lr: 0.0080 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 61/200\n",
      "7/7 - 1s - loss: 0.6540 - accuracy: 0.7679 - val_loss: 1.5426 - val_accuracy: 0.6957 - lr: 0.0078 - 548ms/epoch - 78ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 62/200\n",
      "7/7 - 0s - loss: 0.7570 - accuracy: 0.7857 - val_loss: 0.8316 - val_accuracy: 0.7391 - lr: 0.0076 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 63/200\n",
      "======================EPOCH 000063:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1      rfa\n",
      "62   63.0  0.550116  0.785714  0.702336  0.826087  0.916667  0.857143  0.86413\n",
      "RFA : 0.8641304284334181\n",
      "7/7 - 2s - loss: 0.5501 - accuracy: 0.7857 - val_loss: 0.7023 - val_accuracy: 0.8261 - lr: 0.0074 - 2s/epoch - 262ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 64/200\n",
      "======================EPOCH 000064:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "63   64.0  0.448505  0.834821  0.503057  0.826087  0.924242  0.857143  0.866403\n",
      "RFA : 0.8664031557061453\n",
      "7/7 - 1s - loss: 0.4485 - accuracy: 0.8348 - val_loss: 0.5031 - val_accuracy: 0.8261 - lr: 0.0072 - 708ms/epoch - 101ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 65/200\n",
      "======================EPOCH 000065:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "64   65.0  0.473686  0.808036  0.655589  0.826087  0.931818  0.857143  0.868676\n",
      "RFA : 0.8686758829788728\n",
      "7/7 - 0s - loss: 0.4737 - accuracy: 0.8080 - val_loss: 0.6556 - val_accuracy: 0.8261 - lr: 0.0070 - 362ms/epoch - 52ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 66/200\n",
      "7/7 - 1s - loss: 0.4394 - accuracy: 0.8393 - val_loss: 0.3801 - val_accuracy: 0.7391 - lr: 0.0068 - 572ms/epoch - 82ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 67/200\n",
      "7/7 - 0s - loss: 0.4314 - accuracy: 0.8170 - val_loss: 0.3953 - val_accuracy: 0.7391 - lr: 0.0066 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 68/200\n",
      "7/7 - 0s - loss: 0.4401 - accuracy: 0.8125 - val_loss: 0.4504 - val_accuracy: 0.7826 - lr: 0.0064 - 341ms/epoch - 49ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 69/200\n",
      "7/7 - 1s - loss: 0.4312 - accuracy: 0.8125 - val_loss: 0.4047 - val_accuracy: 0.7391 - lr: 0.0062 - 646ms/epoch - 92ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 70/200\n",
      "7/7 - 0s - loss: 0.4190 - accuracy: 0.8393 - val_loss: 0.4254 - val_accuracy: 0.7391 - lr: 0.0060 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 71/200\n",
      "7/7 - 0s - loss: 0.4827 - accuracy: 0.8036 - val_loss: 0.4344 - val_accuracy: 0.7826 - lr: 0.0058 - 351ms/epoch - 50ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 72/200\n",
      "7/7 - 0s - loss: 0.3818 - accuracy: 0.8527 - val_loss: 0.7008 - val_accuracy: 0.7826 - lr: 0.0056 - 498ms/epoch - 71ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 73/200\n",
      "7/7 - 1s - loss: 0.3336 - accuracy: 0.8393 - val_loss: 0.4729 - val_accuracy: 0.8261 - lr: 0.0054 - 1s/epoch - 164ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 74/200\n",
      "7/7 - 0s - loss: 0.3928 - accuracy: 0.8304 - val_loss: 0.5867 - val_accuracy: 0.8261 - lr: 0.0052 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 75/200\n",
      "7/7 - 1s - loss: 0.4102 - accuracy: 0.8616 - val_loss: 0.4799 - val_accuracy: 0.8261 - lr: 0.0050 - 673ms/epoch - 96ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 76/200\n",
      "7/7 - 0s - loss: 0.4219 - accuracy: 0.8393 - val_loss: 0.7818 - val_accuracy: 0.6957 - lr: 0.0048 - 370ms/epoch - 53ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 77/200\n",
      "7/7 - 0s - loss: 0.4275 - accuracy: 0.8348 - val_loss: 0.5502 - val_accuracy: 0.8261 - lr: 0.0046 - 358ms/epoch - 51ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 78/200\n",
      "7/7 - 0s - loss: 0.3617 - accuracy: 0.8750 - val_loss: 0.3844 - val_accuracy: 0.7391 - lr: 0.0044 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 79/200\n",
      "======================EPOCH 000079:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "78   79.0  0.361356  0.861607  0.603079  0.826087  0.939394  0.857143  0.870949\n",
      "RFA : 0.8709486102516\n",
      "7/7 - 1s - loss: 0.3614 - accuracy: 0.8616 - val_loss: 0.6031 - val_accuracy: 0.8261 - lr: 0.0042 - 555ms/epoch - 79ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 80/200\n",
      "7/7 - 1s - loss: 0.3383 - accuracy: 0.8795 - val_loss: 0.3768 - val_accuracy: 0.7826 - lr: 0.0040 - 614ms/epoch - 88ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 81/200\n",
      "7/7 - 1s - loss: 0.2769 - accuracy: 0.8839 - val_loss: 0.6191 - val_accuracy: 0.8261 - lr: 0.0038 - 702ms/epoch - 100ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 82/200\n",
      "7/7 - 1s - loss: 0.3399 - accuracy: 0.8527 - val_loss: 0.4574 - val_accuracy: 0.8261 - lr: 0.0036 - 655ms/epoch - 94ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 83/200\n",
      "7/7 - 1s - loss: 0.4342 - accuracy: 0.8170 - val_loss: 0.4928 - val_accuracy: 0.8261 - lr: 0.0034 - 518ms/epoch - 74ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 84/200\n",
      "7/7 - 1s - loss: 0.3927 - accuracy: 0.8438 - val_loss: 0.4798 - val_accuracy: 0.8261 - lr: 0.0032 - 576ms/epoch - 82ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 85/200\n",
      "7/7 - 0s - loss: 0.3565 - accuracy: 0.8214 - val_loss: 0.5361 - val_accuracy: 0.8261 - lr: 0.0030 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 86/200\n",
      "7/7 - 0s - loss: 0.3336 - accuracy: 0.8482 - val_loss: 0.4970 - val_accuracy: 0.8261 - lr: 0.0028 - 393ms/epoch - 56ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 87/200\n",
      "7/7 - 1s - loss: 0.3748 - accuracy: 0.8348 - val_loss: 0.5394 - val_accuracy: 0.8261 - lr: 0.0026 - 592ms/epoch - 85ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 88/200\n",
      "7/7 - 1s - loss: 0.4529 - accuracy: 0.7991 - val_loss: 0.3793 - val_accuracy: 0.7391 - lr: 0.0024 - 646ms/epoch - 92ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 89/200\n",
      "7/7 - 1s - loss: 0.3814 - accuracy: 0.8393 - val_loss: 0.5794 - val_accuracy: 0.8261 - lr: 0.0022 - 508ms/epoch - 73ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 90/200\n",
      "7/7 - 0s - loss: 0.3925 - accuracy: 0.8348 - val_loss: 0.4769 - val_accuracy: 0.8261 - lr: 0.0020 - 484ms/epoch - 69ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 91/200\n",
      "7/7 - 1s - loss: 0.3023 - accuracy: 0.8661 - val_loss: 0.5875 - val_accuracy: 0.8261 - lr: 0.0018 - 1s/epoch - 177ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 92/200\n",
      "7/7 - 1s - loss: 0.3873 - accuracy: 0.8125 - val_loss: 0.4831 - val_accuracy: 0.8261 - lr: 0.0016 - 585ms/epoch - 84ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 93/200\n",
      "7/7 - 0s - loss: 0.3777 - accuracy: 0.8661 - val_loss: 0.5597 - val_accuracy: 0.8261 - lr: 0.0014 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 94/200\n",
      "7/7 - 0s - loss: 0.3618 - accuracy: 0.8616 - val_loss: 0.5094 - val_accuracy: 0.8261 - lr: 0.0012 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 95/200\n",
      "7/7 - 1s - loss: 0.3525 - accuracy: 0.8661 - val_loss: 0.4577 - val_accuracy: 0.8261 - lr: 0.0010 - 975ms/epoch - 139ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 96/200\n",
      "7/7 - 0s - loss: 0.3173 - accuracy: 0.8527 - val_loss: 0.4968 - val_accuracy: 0.8261 - lr: 8.0009e-04 - 491ms/epoch - 70ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 97/200\n",
      "7/7 - 0s - loss: 0.3446 - accuracy: 0.8571 - val_loss: 0.4649 - val_accuracy: 0.8261 - lr: 6.0009e-04 - 351ms/epoch - 50ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 98/200\n",
      "7/7 - 0s - loss: 0.3771 - accuracy: 0.8348 - val_loss: 0.4700 - val_accuracy: 0.8261 - lr: 4.0010e-04 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 99/200\n",
      "7/7 - 0s - loss: 0.2946 - accuracy: 0.8705 - val_loss: 0.4889 - val_accuracy: 0.8261 - lr: 2.0010e-04 - 360ms/epoch - 51ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 100/200\n",
      "7/7 - 0s - loss: 0.3114 - accuracy: 0.8705 - val_loss: 0.4940 - val_accuracy: 0.8261 - lr: 1.0000e-07 - 357ms/epoch - 51ms/step\n",
      "Learning rate: 0.00000010\n",
      "Epoch 101/200\n",
      "7/7 - 1s - loss: 0.3497 - accuracy: 0.8527 - val_loss: 0.5000 - val_accuracy: 0.8261 - lr: 2.0010e-04 - 791ms/epoch - 113ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 102/200\n",
      "7/7 - 0s - loss: 0.3355 - accuracy: 0.8571 - val_loss: 0.5287 - val_accuracy: 0.8261 - lr: 4.0010e-04 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 103/200\n",
      "7/7 - 0s - loss: 0.3108 - accuracy: 0.8795 - val_loss: 0.5390 - val_accuracy: 0.8261 - lr: 6.0009e-04 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 104/200\n",
      "7/7 - 0s - loss: 0.3903 - accuracy: 0.8527 - val_loss: 0.4447 - val_accuracy: 0.8261 - lr: 8.0009e-04 - 353ms/epoch - 50ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 105/200\n",
      "7/7 - 1s - loss: 0.3644 - accuracy: 0.8482 - val_loss: 0.4439 - val_accuracy: 0.8261 - lr: 0.0010 - 613ms/epoch - 88ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 106/200\n",
      "7/7 - 0s - loss: 0.3065 - accuracy: 0.8884 - val_loss: 0.4313 - val_accuracy: 0.8261 - lr: 0.0012 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 107/200\n",
      "======================EPOCH 000107:======================\n",
      "     epoch      loss    acc  val_loss   val_acc     rocp        f1       rfa\n",
      "106  107.0  0.342823  0.875  0.486746  0.826087  0.94697  0.857143  0.873221\n",
      "RFA : 0.8732213375243274\n",
      "7/7 - 0s - loss: 0.3428 - accuracy: 0.8750 - val_loss: 0.4867 - val_accuracy: 0.8261 - lr: 0.0014 - 361ms/epoch - 52ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 108/200\n",
      "7/7 - 1s - loss: 0.4173 - accuracy: 0.8527 - val_loss: 0.5025 - val_accuracy: 0.8261 - lr: 0.0016 - 537ms/epoch - 77ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 109/200\n",
      "7/7 - 2s - loss: 0.2782 - accuracy: 0.8839 - val_loss: 0.4782 - val_accuracy: 0.8261 - lr: 0.0018 - 2s/epoch - 216ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 110/200\n",
      "7/7 - 0s - loss: 0.3527 - accuracy: 0.8393 - val_loss: 0.4479 - val_accuracy: 0.8261 - lr: 0.0020 - 447ms/epoch - 64ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 111/200\n",
      "======================EPOCH 000111:======================\n",
      "     epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "110  111.0  0.338087  0.861607   0.48364  0.826087  0.954545  0.857143  0.875494\n",
      "RFA : 0.8754940647970546\n",
      "7/7 - 0s - loss: 0.3381 - accuracy: 0.8616 - val_loss: 0.4836 - val_accuracy: 0.8261 - lr: 0.0022 - 380ms/epoch - 54ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 112/200\n",
      "7/7 - 0s - loss: 0.3180 - accuracy: 0.8438 - val_loss: 0.4018 - val_accuracy: 0.8261 - lr: 0.0024 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 113/200\n",
      "7/7 - 1s - loss: 0.3683 - accuracy: 0.8482 - val_loss: 0.4860 - val_accuracy: 0.8261 - lr: 0.0026 - 1s/epoch - 161ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 114/200\n",
      "7/7 - 0s - loss: 0.4093 - accuracy: 0.8616 - val_loss: 0.3623 - val_accuracy: 0.7826 - lr: 0.0028 - 477ms/epoch - 68ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 115/200\n",
      "7/7 - 0s - loss: 0.3345 - accuracy: 0.8705 - val_loss: 0.6537 - val_accuracy: 0.7391 - lr: 0.0030 - 323ms/epoch - 46ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 116/200\n",
      "7/7 - 0s - loss: 0.3140 - accuracy: 0.8527 - val_loss: 0.4123 - val_accuracy: 0.8261 - lr: 0.0032 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 117/200\n",
      "7/7 - 0s - loss: 0.3746 - accuracy: 0.8527 - val_loss: 0.3676 - val_accuracy: 0.7826 - lr: 0.0034 - 263ms/epoch - 38ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 118/200\n",
      "7/7 - 0s - loss: 0.3251 - accuracy: 0.8348 - val_loss: 0.4489 - val_accuracy: 0.8261 - lr: 0.0036 - 271ms/epoch - 39ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 119/200\n",
      "7/7 - 0s - loss: 0.2818 - accuracy: 0.8839 - val_loss: 0.4150 - val_accuracy: 0.8261 - lr: 0.0038 - 420ms/epoch - 60ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 120/200\n",
      "7/7 - 0s - loss: 0.3086 - accuracy: 0.8571 - val_loss: 0.5525 - val_accuracy: 0.8261 - lr: 0.0040 - 451ms/epoch - 64ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 121/200\n",
      "7/7 - 0s - loss: 0.3683 - accuracy: 0.8438 - val_loss: 0.3872 - val_accuracy: 0.8261 - lr: 0.0042 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 122/200\n",
      "7/7 - 0s - loss: 0.2601 - accuracy: 0.8839 - val_loss: 0.3700 - val_accuracy: 0.8261 - lr: 0.0044 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 123/200\n",
      "7/7 - 1s - loss: 0.3518 - accuracy: 0.8438 - val_loss: 0.7175 - val_accuracy: 0.7391 - lr: 0.0046 - 548ms/epoch - 78ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 124/200\n",
      "7/7 - 1s - loss: 0.3377 - accuracy: 0.8750 - val_loss: 0.3557 - val_accuracy: 0.7826 - lr: 0.0048 - 855ms/epoch - 122ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 125/200\n",
      "7/7 - 0s - loss: 0.2892 - accuracy: 0.8750 - val_loss: 0.6170 - val_accuracy: 0.7826 - lr: 0.0050 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 126/200\n",
      "7/7 - 0s - loss: 0.2873 - accuracy: 0.8839 - val_loss: 0.3718 - val_accuracy: 0.7826 - lr: 0.0052 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 127/200\n",
      "7/7 - 0s - loss: 0.3030 - accuracy: 0.8616 - val_loss: 0.7254 - val_accuracy: 0.7391 - lr: 0.0054 - 380ms/epoch - 54ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 128/200\n",
      "7/7 - 1s - loss: 0.4288 - accuracy: 0.8170 - val_loss: 0.5024 - val_accuracy: 0.8261 - lr: 0.0056 - 679ms/epoch - 97ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 129/200\n",
      "7/7 - 1s - loss: 0.3667 - accuracy: 0.8616 - val_loss: 0.4580 - val_accuracy: 0.8261 - lr: 0.0058 - 720ms/epoch - 103ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 130/200\n",
      "7/7 - 1s - loss: 0.3135 - accuracy: 0.8750 - val_loss: 0.3812 - val_accuracy: 0.7826 - lr: 0.0060 - 505ms/epoch - 72ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 131/200\n",
      "7/7 - 0s - loss: 0.3100 - accuracy: 0.8571 - val_loss: 0.3407 - val_accuracy: 0.7826 - lr: 0.0062 - 491ms/epoch - 70ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 132/200\n",
      "7/7 - 0s - loss: 0.3172 - accuracy: 0.8616 - val_loss: 0.4147 - val_accuracy: 0.8261 - lr: 0.0064 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 133/200\n",
      "7/7 - 0s - loss: 0.2999 - accuracy: 0.8616 - val_loss: 0.4006 - val_accuracy: 0.7391 - lr: 0.0066 - 481ms/epoch - 69ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 134/200\n",
      "7/7 - 2s - loss: 0.3280 - accuracy: 0.8393 - val_loss: 0.6129 - val_accuracy: 0.8261 - lr: 0.0068 - 2s/epoch - 227ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 135/200\n",
      "7/7 - 0s - loss: 0.3012 - accuracy: 0.8973 - val_loss: 0.3990 - val_accuracy: 0.8261 - lr: 0.0070 - 484ms/epoch - 69ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 136/200\n",
      "7/7 - 1s - loss: 0.2817 - accuracy: 0.8438 - val_loss: 0.4899 - val_accuracy: 0.8261 - lr: 0.0072 - 604ms/epoch - 86ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 137/200\n",
      "7/7 - 0s - loss: 0.3371 - accuracy: 0.8527 - val_loss: 0.6869 - val_accuracy: 0.7391 - lr: 0.0074 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 138/200\n",
      "7/7 - 0s - loss: 0.3008 - accuracy: 0.8616 - val_loss: 0.3927 - val_accuracy: 0.8261 - lr: 0.0076 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 139/200\n",
      "7/7 - 0s - loss: 0.2946 - accuracy: 0.8750 - val_loss: 0.5090 - val_accuracy: 0.8261 - lr: 0.0078 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 140/200\n",
      "7/7 - 0s - loss: 0.2948 - accuracy: 0.8750 - val_loss: 0.3376 - val_accuracy: 0.8261 - lr: 0.0080 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 141/200\n",
      "7/7 - 0s - loss: 0.2934 - accuracy: 0.8750 - val_loss: 0.5721 - val_accuracy: 0.8261 - lr: 0.0082 - 358ms/epoch - 51ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 142/200\n",
      "7/7 - 2s - loss: 0.3893 - accuracy: 0.8036 - val_loss: 0.3352 - val_accuracy: 0.7826 - lr: 0.0084 - 2s/epoch - 226ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 143/200\n",
      "7/7 - 0s - loss: 0.2742 - accuracy: 0.8929 - val_loss: 1.1064 - val_accuracy: 0.6957 - lr: 0.0086 - 378ms/epoch - 54ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 144/200\n",
      "7/7 - 0s - loss: 0.3611 - accuracy: 0.8214 - val_loss: 0.6538 - val_accuracy: 0.7391 - lr: 0.0088 - 354ms/epoch - 51ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 145/200\n",
      "7/7 - 0s - loss: 0.2703 - accuracy: 0.8661 - val_loss: 0.3521 - val_accuracy: 0.7826 - lr: 0.0090 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 146/200\n",
      "7/7 - 0s - loss: 0.3195 - accuracy: 0.8616 - val_loss: 0.7185 - val_accuracy: 0.7391 - lr: 0.0092 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 147/200\n",
      "7/7 - 0s - loss: 0.4160 - accuracy: 0.8125 - val_loss: 0.7197 - val_accuracy: 0.7391 - lr: 0.0094 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 148/200\n",
      "7/7 - 1s - loss: 0.3215 - accuracy: 0.8795 - val_loss: 0.5993 - val_accuracy: 0.7826 - lr: 0.0096 - 863ms/epoch - 123ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 149/200\n",
      "7/7 - 0s - loss: 0.3412 - accuracy: 0.8616 - val_loss: 0.8131 - val_accuracy: 0.7391 - lr: 0.0098 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 150/200\n",
      "7/7 - 0s - loss: 0.3180 - accuracy: 0.8839 - val_loss: 0.3489 - val_accuracy: 0.7826 - lr: 0.0100 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 151/200\n",
      "7/7 - 0s - loss: 0.3152 - accuracy: 0.8527 - val_loss: 0.4564 - val_accuracy: 0.8261 - lr: 0.0098 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 152/200\n",
      "7/7 - 0s - loss: 0.2726 - accuracy: 0.9018 - val_loss: 0.5697 - val_accuracy: 0.8261 - lr: 0.0096 - 490ms/epoch - 70ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 153/200\n",
      "7/7 - 1s - loss: 0.2540 - accuracy: 0.8795 - val_loss: 0.4364 - val_accuracy: 0.8261 - lr: 0.0094 - 1s/epoch - 188ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 154/200\n",
      "7/7 - 1s - loss: 0.3332 - accuracy: 0.8616 - val_loss: 0.4980 - val_accuracy: 0.8261 - lr: 0.0092 - 949ms/epoch - 136ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 155/200\n",
      "7/7 - 0s - loss: 0.3006 - accuracy: 0.8929 - val_loss: 0.5012 - val_accuracy: 0.8261 - lr: 0.0090 - 493ms/epoch - 70ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 156/200\n",
      "7/7 - 0s - loss: 0.3742 - accuracy: 0.8438 - val_loss: 0.3373 - val_accuracy: 0.7826 - lr: 0.0088 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 157/200\n",
      "7/7 - 0s - loss: 0.2607 - accuracy: 0.8929 - val_loss: 0.6325 - val_accuracy: 0.7391 - lr: 0.0086 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 158/200\n",
      "======================EPOCH 000158:======================\n",
      "     epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "157  158.0  0.366865  0.825893  0.390032  0.826087  0.962121  0.857143  0.877767\n",
      "RFA : 0.8777667920697818\n",
      "7/7 - 0s - loss: 0.3669 - accuracy: 0.8259 - val_loss: 0.3900 - val_accuracy: 0.8261 - lr: 0.0084 - 404ms/epoch - 58ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 159/200\n",
      "7/7 - 1s - loss: 0.3195 - accuracy: 0.8705 - val_loss: 0.3003 - val_accuracy: 0.8261 - lr: 0.0082 - 1s/epoch - 148ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 160/200\n",
      "7/7 - 0s - loss: 0.2903 - accuracy: 0.8839 - val_loss: 0.3282 - val_accuracy: 0.7826 - lr: 0.0080 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 161/200\n",
      "======================EPOCH 000161:======================\n",
      "     epoch      loss       acc  val_loss   val_acc     rocp    f1       rfa\n",
      "160  161.0  0.281892  0.879464  0.296719  0.869565  0.94697  0.88  0.896439\n",
      "RFA : 0.8964387252005663\n",
      "7/7 - 1s - loss: 0.2819 - accuracy: 0.8795 - val_loss: 0.2967 - val_accuracy: 0.8696 - lr: 0.0078 - 534ms/epoch - 76ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 162/200\n",
      "7/7 - 1s - loss: 0.2566 - accuracy: 0.8929 - val_loss: 0.3403 - val_accuracy: 0.8261 - lr: 0.0076 - 1s/epoch - 153ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 163/200\n",
      "7/7 - 1s - loss: 0.2816 - accuracy: 0.8884 - val_loss: 0.2894 - val_accuracy: 0.8261 - lr: 0.0074 - 954ms/epoch - 136ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 164/200\n",
      "7/7 - 0s - loss: 0.3579 - accuracy: 0.8259 - val_loss: 0.5667 - val_accuracy: 0.7826 - lr: 0.0072 - 489ms/epoch - 70ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 165/200\n",
      "7/7 - 1s - loss: 0.2611 - accuracy: 0.8884 - val_loss: 0.4467 - val_accuracy: 0.8261 - lr: 0.0070 - 1s/epoch - 162ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 166/200\n",
      "7/7 - 0s - loss: 0.2864 - accuracy: 0.8929 - val_loss: 0.3920 - val_accuracy: 0.8261 - lr: 0.0068 - 476ms/epoch - 68ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 167/200\n",
      "7/7 - 0s - loss: 0.2529 - accuracy: 0.9196 - val_loss: 0.3560 - val_accuracy: 0.8261 - lr: 0.0066 - 426ms/epoch - 61ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 168/200\n",
      "7/7 - 0s - loss: 0.2742 - accuracy: 0.8929 - val_loss: 0.3207 - val_accuracy: 0.7826 - lr: 0.0064 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 169/200\n",
      "7/7 - 1s - loss: 0.3213 - accuracy: 0.8839 - val_loss: 0.4973 - val_accuracy: 0.7826 - lr: 0.0062 - 526ms/epoch - 75ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 170/200\n",
      "7/7 - 1s - loss: 0.2831 - accuracy: 0.8839 - val_loss: 0.3081 - val_accuracy: 0.7826 - lr: 0.0060 - 562ms/epoch - 80ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 171/200\n",
      "7/7 - 0s - loss: 0.2829 - accuracy: 0.8795 - val_loss: 0.4156 - val_accuracy: 0.8261 - lr: 0.0058 - 378ms/epoch - 54ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 172/200\n",
      "7/7 - 0s - loss: 0.2822 - accuracy: 0.8795 - val_loss: 0.3493 - val_accuracy: 0.7826 - lr: 0.0056 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 173/200\n",
      "7/7 - 0s - loss: 0.2625 - accuracy: 0.8839 - val_loss: 0.4005 - val_accuracy: 0.8261 - lr: 0.0054 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 174/200\n",
      "7/7 - 1s - loss: 0.2548 - accuracy: 0.9196 - val_loss: 0.4854 - val_accuracy: 0.8261 - lr: 0.0052 - 596ms/epoch - 85ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 175/200\n",
      "7/7 - 2s - loss: 0.2326 - accuracy: 0.8973 - val_loss: 0.3997 - val_accuracy: 0.8261 - lr: 0.0050 - 2s/epoch - 222ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 176/200\n",
      "7/7 - 0s - loss: 0.3085 - accuracy: 0.8884 - val_loss: 0.3396 - val_accuracy: 0.7826 - lr: 0.0048 - 401ms/epoch - 57ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 177/200\n",
      "7/7 - 0s - loss: 0.2713 - accuracy: 0.8705 - val_loss: 0.4198 - val_accuracy: 0.8261 - lr: 0.0046 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 178/200\n",
      "7/7 - 2s - loss: 0.2490 - accuracy: 0.8973 - val_loss: 0.4434 - val_accuracy: 0.8261 - lr: 0.0044 - 2s/epoch - 219ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 179/200\n",
      "7/7 - 0s - loss: 0.2511 - accuracy: 0.9062 - val_loss: 0.3196 - val_accuracy: 0.7826 - lr: 0.0042 - 487ms/epoch - 70ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 180/200\n",
      "7/7 - 0s - loss: 0.2331 - accuracy: 0.9241 - val_loss: 0.3543 - val_accuracy: 0.8261 - lr: 0.0040 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 181/200\n",
      "7/7 - 0s - loss: 0.2748 - accuracy: 0.8884 - val_loss: 0.3660 - val_accuracy: 0.7826 - lr: 0.0038 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 182/200\n",
      "7/7 - 0s - loss: 0.2791 - accuracy: 0.8839 - val_loss: 0.4440 - val_accuracy: 0.8261 - lr: 0.0036 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 183/200\n",
      "7/7 - 0s - loss: 0.2578 - accuracy: 0.8973 - val_loss: 0.4006 - val_accuracy: 0.8261 - lr: 0.0034 - 427ms/epoch - 61ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 184/200\n",
      "7/7 - 1s - loss: 0.2365 - accuracy: 0.9062 - val_loss: 0.3892 - val_accuracy: 0.8261 - lr: 0.0032 - 1s/epoch - 212ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 185/200\n",
      "7/7 - 0s - loss: 0.2077 - accuracy: 0.9286 - val_loss: 0.3713 - val_accuracy: 0.8261 - lr: 0.0030 - 470ms/epoch - 67ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 186/200\n",
      "7/7 - 0s - loss: 0.2223 - accuracy: 0.9062 - val_loss: 0.3929 - val_accuracy: 0.8261 - lr: 0.0028 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 187/200\n",
      "7/7 - 1s - loss: 0.2521 - accuracy: 0.9107 - val_loss: 0.3590 - val_accuracy: 0.7826 - lr: 0.0026 - 1s/epoch - 198ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 188/200\n",
      "7/7 - 0s - loss: 0.2326 - accuracy: 0.9196 - val_loss: 0.3880 - val_accuracy: 0.8261 - lr: 0.0024 - 492ms/epoch - 70ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 189/200\n",
      "7/7 - 0s - loss: 0.2717 - accuracy: 0.9062 - val_loss: 0.3608 - val_accuracy: 0.7826 - lr: 0.0022 - 342ms/epoch - 49ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 190/200\n",
      "7/7 - 0s - loss: 0.2663 - accuracy: 0.8929 - val_loss: 0.4169 - val_accuracy: 0.8261 - lr: 0.0020 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 191/200\n",
      "7/7 - 0s - loss: 0.2490 - accuracy: 0.8929 - val_loss: 0.4010 - val_accuracy: 0.8261 - lr: 0.0018 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 192/200\n",
      "7/7 - 1s - loss: 0.2644 - accuracy: 0.8839 - val_loss: 0.3686 - val_accuracy: 0.8261 - lr: 0.0016 - 552ms/epoch - 79ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 193/200\n",
      "7/7 - 1s - loss: 0.2060 - accuracy: 0.9241 - val_loss: 0.3563 - val_accuracy: 0.7826 - lr: 0.0014 - 1s/epoch - 170ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 194/200\n",
      "7/7 - 0s - loss: 0.2268 - accuracy: 0.9152 - val_loss: 0.3403 - val_accuracy: 0.7826 - lr: 0.0012 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 195/200\n",
      "7/7 - 0s - loss: 0.2301 - accuracy: 0.8973 - val_loss: 0.4206 - val_accuracy: 0.8261 - lr: 0.0010 - 395ms/epoch - 56ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 196/200\n",
      "7/7 - 1s - loss: 0.3299 - accuracy: 0.8929 - val_loss: 0.4375 - val_accuracy: 0.8261 - lr: 8.0009e-04 - 1s/epoch - 169ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 197/200\n",
      "7/7 - 1s - loss: 0.2425 - accuracy: 0.9107 - val_loss: 0.3845 - val_accuracy: 0.8261 - lr: 6.0009e-04 - 1s/epoch - 168ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 198/200\n",
      "7/7 - 0s - loss: 0.2718 - accuracy: 0.8884 - val_loss: 0.3922 - val_accuracy: 0.8261 - lr: 4.0010e-04 - 487ms/epoch - 70ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 199/200\n",
      "7/7 - 0s - loss: 0.2541 - accuracy: 0.8929 - val_loss: 0.3833 - val_accuracy: 0.8261 - lr: 2.0010e-04 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 200/200\n",
      "7/7 - 2s - loss: 0.1977 - accuracy: 0.9196 - val_loss: 0.3815 - val_accuracy: 0.8261 - lr: 1.0000e-07 - 2s/epoch - 326ms/step\n",
      "\n",
      "======================BEST:======================\n",
      "     epoch      loss       acc  val_loss   val_acc     rocp    f1       rfa\n",
      "160  161.0  0.281892  0.879464  0.296719  0.869565  0.94697  0.88  0.896439\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.64      0.78        11\n",
      "           1       0.75      1.00      0.86        12\n",
      "\n",
      "    accuracy                           0.83        23\n",
      "   macro avg       0.88      0.82      0.82        23\n",
      "weighted avg       0.87      0.83      0.82        23\n",
      "\n",
      "[[ 7  4]\n",
      " [ 0 12]]\n",
      "ACCURACY: 0.8260869565217391\n",
      "PRECISION: 0.75\n",
      "RECALL: 1.0\n",
      "F1: 0.8571428571428571\n",
      "ROC_AUC(Pr.): 0.946969696969697\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       0.85      0.92      0.88        12\n",
      "\n",
      "    accuracy                           0.87        23\n",
      "   macro avg       0.87      0.87      0.87        23\n",
      "weighted avg       0.87      0.87      0.87        23\n",
      "\n",
      "[[ 9  2]\n",
      " [ 1 11]]\n",
      "ACCURACY: 0.8695652173913043\n",
      "PRECISION: 0.8461538461538461\n",
      "RECALL: 0.9166666666666666\n",
      "F1: 0.8799999999999999\n",
      "ROC_AUC(Pr.): 0.946969696969697\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       0.85      0.92      0.88        12\n",
      "\n",
      "    accuracy                           0.87        23\n",
      "   macro avg       0.87      0.87      0.87        23\n",
      "weighted avg       0.87      0.87      0.87        23\n",
      "\n",
      "[[ 9  2]\n",
      " [ 1 11]]\n",
      "ACCURACY: 0.8695652173913043\n",
      "PRECISION: 0.8461538461538461\n",
      "RECALL: 0.9166666666666666\n",
      "F1: 0.8799999999999999\n",
      "ROC_AUC(Pr.): 0.946969696969697\n",
      "\n",
      "=====================CV[4]========================\n",
      "\n",
      "207 train samples\n",
      "23 test samples\n",
      "\n",
      "Over sampling: 207 -> 208\n",
      "Add train samples: 16\n",
      "224 new train samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\tf28\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.00000010\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_28484\\4287042063.py:159: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================EPOCH 000001:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "0    1.0  0.693656  0.504464  0.692779  0.565217  0.727273  0.642857  0.641008\n",
      "RFA : 0.6410078996961767\n",
      "7/7 - 6s - loss: 0.6937 - accuracy: 0.5045 - val_loss: 0.6928 - val_accuracy: 0.5652 - lr: 2.0010e-04 - 6s/epoch - 825ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 2/200\n",
      "======================EPOCH 000002:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "1    2.0  0.691998  0.549107  0.691739  0.695652  0.840909  0.774194  0.766719\n",
      "RFA : 0.7667187346129123\n",
      "7/7 - 0s - loss: 0.6920 - accuracy: 0.5491 - val_loss: 0.6917 - val_accuracy: 0.6957 - lr: 4.0010e-04 - 383ms/epoch - 55ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 3/200\n",
      "7/7 - 0s - loss: 0.6922 - accuracy: 0.4732 - val_loss: 0.6898 - val_accuracy: 0.4783 - lr: 6.0009e-04 - 364ms/epoch - 52ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 4/200\n",
      "======================EPOCH 000004:======================\n",
      "   epoch      loss       acc  val_loss  val_acc      rocp        f1       rfa\n",
      "3    4.0  0.689812  0.598214  0.678201  0.73913  0.848485  0.785714  0.788241\n",
      "RFA : 0.7882411076263948\n",
      "7/7 - 1s - loss: 0.6898 - accuracy: 0.5982 - val_loss: 0.6782 - val_accuracy: 0.7391 - lr: 8.0009e-04 - 1s/epoch - 173ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 5/200\n",
      "7/7 - 0s - loss: 0.6696 - accuracy: 0.6830 - val_loss: 0.6245 - val_accuracy: 0.7391 - lr: 0.0010 - 383ms/epoch - 55ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 6/200\n",
      "======================EPOCH 000006:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "5    6.0  0.594631  0.754464  0.490883  0.782609  0.840909  0.827586  0.815841\n",
      "RFA : 0.8158409404436996\n",
      "7/7 - 0s - loss: 0.5946 - accuracy: 0.7545 - val_loss: 0.4909 - val_accuracy: 0.7826 - lr: 0.0012 - 387ms/epoch - 55ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 7/200\n",
      "======================EPOCH 000007:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "6    7.0  0.544824  0.714286  0.467287  0.782609  0.848485  0.827586  0.818114\n",
      "RFA : 0.8181136677164269\n",
      "7/7 - 1s - loss: 0.5448 - accuracy: 0.7143 - val_loss: 0.4673 - val_accuracy: 0.7826 - lr: 0.0014 - 1s/epoch - 185ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 8/200\n",
      "7/7 - 1s - loss: 0.4829 - accuracy: 0.7411 - val_loss: 0.4550 - val_accuracy: 0.6957 - lr: 0.0016 - 552ms/epoch - 79ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 9/200\n",
      "======================EPOCH 000009:======================\n",
      "   epoch     loss     acc  val_loss   val_acc      rocp        f1       rfa\n",
      "8    9.0  0.43656  0.8125  0.396573  0.782609  0.871212  0.827586  0.824932\n",
      "RFA : 0.8249318495346086\n",
      "7/7 - 0s - loss: 0.4366 - accuracy: 0.8125 - val_loss: 0.3966 - val_accuracy: 0.7826 - lr: 0.0018 - 361ms/epoch - 52ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 10/200\n",
      "======================EPOCH 000010:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "9   10.0  0.417739  0.794643    0.3705  0.826087  0.893939  0.846154  0.853466\n",
      "RFA : 0.8534660927690825\n",
      "7/7 - 1s - loss: 0.4177 - accuracy: 0.7946 - val_loss: 0.3705 - val_accuracy: 0.8261 - lr: 0.0020 - 545ms/epoch - 78ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 11/200\n",
      "======================EPOCH 000011:======================\n",
      "    epoch      loss     acc  val_loss   val_acc      rocp        f1       rfa\n",
      "10   11.0  0.378373  0.8125   0.36227  0.826087  0.909091  0.846154  0.858012\n",
      "RFA : 0.8580115473145371\n",
      "7/7 - 1s - loss: 0.3784 - accuracy: 0.8125 - val_loss: 0.3623 - val_accuracy: 0.8261 - lr: 0.0022 - 1s/epoch - 167ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 12/200\n",
      "======================EPOCH 000012:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "11   12.0  0.337541  0.834821  0.348504  0.826087  0.909091  0.857143  0.861858\n",
      "RFA : 0.8618577011606909\n",
      "7/7 - 1s - loss: 0.3375 - accuracy: 0.8348 - val_loss: 0.3485 - val_accuracy: 0.8261 - lr: 0.0024 - 689ms/epoch - 98ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 13/200\n",
      "7/7 - 1s - loss: 0.3472 - accuracy: 0.8571 - val_loss: 0.3752 - val_accuracy: 0.6957 - lr: 0.0026 - 503ms/epoch - 72ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 14/200\n",
      "7/7 - 0s - loss: 0.3662 - accuracy: 0.8482 - val_loss: 0.4748 - val_accuracy: 0.8261 - lr: 0.0028 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 15/200\n",
      "7/7 - 1s - loss: 0.3509 - accuracy: 0.8616 - val_loss: 0.4328 - val_accuracy: 0.8261 - lr: 0.0030 - 825ms/epoch - 118ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 16/200\n",
      "7/7 - 0s - loss: 0.3560 - accuracy: 0.8259 - val_loss: 0.4448 - val_accuracy: 0.8261 - lr: 0.0032 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 17/200\n",
      "7/7 - 0s - loss: 0.2948 - accuracy: 0.8750 - val_loss: 0.4305 - val_accuracy: 0.6957 - lr: 0.0034 - 342ms/epoch - 49ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 18/200\n",
      "7/7 - 0s - loss: 0.2876 - accuracy: 0.9062 - val_loss: 0.3844 - val_accuracy: 0.7826 - lr: 0.0036 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 19/200\n",
      "7/7 - 0s - loss: 0.3157 - accuracy: 0.8482 - val_loss: 0.4959 - val_accuracy: 0.7826 - lr: 0.0038 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 20/200\n",
      "7/7 - 0s - loss: 0.3303 - accuracy: 0.8750 - val_loss: 0.3979 - val_accuracy: 0.6957 - lr: 0.0040 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 21/200\n",
      "7/7 - 0s - loss: 0.2849 - accuracy: 0.8795 - val_loss: 0.4552 - val_accuracy: 0.7826 - lr: 0.0042 - 461ms/epoch - 66ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 22/200\n",
      "7/7 - 2s - loss: 0.2783 - accuracy: 0.8661 - val_loss: 0.6929 - val_accuracy: 0.8261 - lr: 0.0044 - 2s/epoch - 220ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 23/200\n",
      "7/7 - 0s - loss: 0.3568 - accuracy: 0.8571 - val_loss: 0.4772 - val_accuracy: 0.7826 - lr: 0.0046 - 413ms/epoch - 59ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 24/200\n",
      "7/7 - 0s - loss: 0.2961 - accuracy: 0.9107 - val_loss: 0.4665 - val_accuracy: 0.7391 - lr: 0.0048 - 360ms/epoch - 51ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 25/200\n",
      "7/7 - 0s - loss: 0.3667 - accuracy: 0.8482 - val_loss: 0.4157 - val_accuracy: 0.7826 - lr: 0.0050 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 26/200\n",
      "7/7 - 0s - loss: 0.2989 - accuracy: 0.8973 - val_loss: 0.4434 - val_accuracy: 0.7391 - lr: 0.0052 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 27/200\n",
      "7/7 - 1s - loss: 0.3366 - accuracy: 0.8438 - val_loss: 0.4020 - val_accuracy: 0.7826 - lr: 0.0054 - 560ms/epoch - 80ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 28/200\n",
      "7/7 - 1s - loss: 0.4448 - accuracy: 0.7768 - val_loss: 0.4245 - val_accuracy: 0.6957 - lr: 0.0056 - 520ms/epoch - 74ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 29/200\n",
      "7/7 - 0s - loss: 0.3627 - accuracy: 0.8259 - val_loss: 0.4288 - val_accuracy: 0.8261 - lr: 0.0058 - 321ms/epoch - 46ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 30/200\n",
      "7/7 - 0s - loss: 0.2830 - accuracy: 0.8795 - val_loss: 0.4720 - val_accuracy: 0.7826 - lr: 0.0060 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 31/200\n",
      "7/7 - 0s - loss: 0.2758 - accuracy: 0.8795 - val_loss: 0.6390 - val_accuracy: 0.7826 - lr: 0.0062 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 32/200\n",
      "7/7 - 2s - loss: 0.3022 - accuracy: 0.8839 - val_loss: 0.4613 - val_accuracy: 0.7826 - lr: 0.0064 - 2s/epoch - 257ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 33/200\n",
      "7/7 - 0s - loss: 0.3060 - accuracy: 0.8839 - val_loss: 0.5681 - val_accuracy: 0.7391 - lr: 0.0066 - 486ms/epoch - 69ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 34/200\n",
      "7/7 - 0s - loss: 0.4398 - accuracy: 0.7902 - val_loss: 0.4330 - val_accuracy: 0.7826 - lr: 0.0068 - 437ms/epoch - 62ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 35/200\n",
      "7/7 - 0s - loss: 0.3294 - accuracy: 0.8571 - val_loss: 0.4493 - val_accuracy: 0.7826 - lr: 0.0070 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 36/200\n",
      "7/7 - 0s - loss: 0.2584 - accuracy: 0.8929 - val_loss: 0.5387 - val_accuracy: 0.7826 - lr: 0.0072 - 346ms/epoch - 49ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 37/200\n",
      "7/7 - 2s - loss: 0.3115 - accuracy: 0.8705 - val_loss: 0.7831 - val_accuracy: 0.6957 - lr: 0.0074 - 2s/epoch - 230ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 38/200\n",
      "7/7 - 0s - loss: 0.3722 - accuracy: 0.8214 - val_loss: 0.4186 - val_accuracy: 0.7826 - lr: 0.0076 - 494ms/epoch - 71ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 39/200\n",
      "7/7 - 0s - loss: 0.2825 - accuracy: 0.8973 - val_loss: 0.5225 - val_accuracy: 0.7826 - lr: 0.0078 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 40/200\n",
      "7/7 - 1s - loss: 0.2956 - accuracy: 0.8705 - val_loss: 0.4907 - val_accuracy: 0.7391 - lr: 0.0080 - 563ms/epoch - 80ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 41/200\n",
      "7/7 - 1s - loss: 0.2170 - accuracy: 0.9196 - val_loss: 0.7641 - val_accuracy: 0.8261 - lr: 0.0082 - 1s/epoch - 214ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 42/200\n",
      "7/7 - 0s - loss: 0.4231 - accuracy: 0.8304 - val_loss: 0.3785 - val_accuracy: 0.7391 - lr: 0.0084 - 466ms/epoch - 67ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 43/200\n",
      "7/7 - 0s - loss: 0.2707 - accuracy: 0.8973 - val_loss: 0.5123 - val_accuracy: 0.7391 - lr: 0.0086 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 44/200\n",
      "7/7 - 0s - loss: 0.3624 - accuracy: 0.8661 - val_loss: 1.4123 - val_accuracy: 0.6957 - lr: 0.0088 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 45/200\n",
      "7/7 - 1s - loss: 1.1224 - accuracy: 0.7232 - val_loss: 1.7946 - val_accuracy: 0.7391 - lr: 0.0090 - 1s/epoch - 171ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 46/200\n",
      "7/7 - 0s - loss: 13.9672 - accuracy: 0.5938 - val_loss: 79.7348 - val_accuracy: 0.4783 - lr: 0.0092 - 486ms/epoch - 69ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 47/200\n",
      "7/7 - 0s - loss: 1280.0508 - accuracy: 0.5000 - val_loss: 1557.1949 - val_accuracy: 0.5217 - lr: 0.0094 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 48/200\n",
      "7/7 - 0s - loss: 1442.7528 - accuracy: 0.5134 - val_loss: 319.1358 - val_accuracy: 0.4783 - lr: 0.0096 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 49/200\n",
      "7/7 - 0s - loss: 60.1353 - accuracy: 0.4509 - val_loss: 5.5425 - val_accuracy: 0.5217 - lr: 0.0098 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 50/200\n",
      "7/7 - 0s - loss: 3.7601 - accuracy: 0.4911 - val_loss: 0.6492 - val_accuracy: 0.6522 - lr: 0.0100 - 369ms/epoch - 53ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 51/200\n",
      "7/7 - 1s - loss: 1.0812 - accuracy: 0.5848 - val_loss: 0.5867 - val_accuracy: 0.7391 - lr: 0.0098 - 993ms/epoch - 142ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 52/200\n",
      "7/7 - 1s - loss: 0.7355 - accuracy: 0.6652 - val_loss: 0.4356 - val_accuracy: 0.7826 - lr: 0.0096 - 1s/epoch - 176ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 53/200\n",
      "7/7 - 0s - loss: 0.5667 - accuracy: 0.7098 - val_loss: 0.4458 - val_accuracy: 0.7826 - lr: 0.0094 - 490ms/epoch - 70ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 54/200\n",
      "7/7 - 0s - loss: 0.5104 - accuracy: 0.7679 - val_loss: 0.4159 - val_accuracy: 0.7826 - lr: 0.0092 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 55/200\n",
      "7/7 - 0s - loss: 0.5352 - accuracy: 0.7634 - val_loss: 0.3769 - val_accuracy: 0.8261 - lr: 0.0090 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 56/200\n",
      "7/7 - 0s - loss: 0.5155 - accuracy: 0.8036 - val_loss: 0.4558 - val_accuracy: 0.7391 - lr: 0.0088 - 448ms/epoch - 64ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 57/200\n",
      "7/7 - 2s - loss: 0.5307 - accuracy: 0.7366 - val_loss: 0.3788 - val_accuracy: 0.8261 - lr: 0.0086 - 2s/epoch - 221ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 58/200\n",
      "7/7 - 0s - loss: 0.4906 - accuracy: 0.7589 - val_loss: 0.4259 - val_accuracy: 0.7826 - lr: 0.0084 - 389ms/epoch - 56ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 59/200\n",
      "7/7 - 0s - loss: 0.4489 - accuracy: 0.7812 - val_loss: 0.4627 - val_accuracy: 0.7826 - lr: 0.0082 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 60/200\n",
      "======================EPOCH 000060:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "59   60.0  0.490004  0.776786  0.365661  0.826087  0.931818  0.857143  0.868676\n",
      "RFA : 0.8686758829788728\n",
      "7/7 - 0s - loss: 0.4900 - accuracy: 0.7768 - val_loss: 0.3657 - val_accuracy: 0.8261 - lr: 0.0080 - 371ms/epoch - 53ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 61/200\n",
      "7/7 - 0s - loss: 0.4162 - accuracy: 0.7902 - val_loss: 0.3980 - val_accuracy: 0.7826 - lr: 0.0078 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 62/200\n",
      "======================EPOCH 000062:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp    f1       rfa\n",
      "61   62.0  0.416425  0.816964  0.332134  0.869565  0.939394  0.88  0.894166\n",
      "RFA : 0.8941659979278391\n",
      "7/7 - 0s - loss: 0.4164 - accuracy: 0.8170 - val_loss: 0.3321 - val_accuracy: 0.8696 - lr: 0.0076 - 372ms/epoch - 53ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 63/200\n",
      "7/7 - 2s - loss: 0.3909 - accuracy: 0.8036 - val_loss: 0.3366 - val_accuracy: 0.8261 - lr: 0.0074 - 2s/epoch - 262ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 64/200\n",
      "7/7 - 1s - loss: 0.4515 - accuracy: 0.7812 - val_loss: 0.3839 - val_accuracy: 0.7826 - lr: 0.0072 - 521ms/epoch - 74ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 65/200\n",
      "7/7 - 0s - loss: 0.4229 - accuracy: 0.7902 - val_loss: 0.3443 - val_accuracy: 0.8696 - lr: 0.0070 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 66/200\n",
      "7/7 - 0s - loss: 0.4050 - accuracy: 0.7946 - val_loss: 0.3657 - val_accuracy: 0.8261 - lr: 0.0068 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 67/200\n",
      "7/7 - 0s - loss: 0.4564 - accuracy: 0.7991 - val_loss: 0.4818 - val_accuracy: 0.7826 - lr: 0.0066 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 68/200\n",
      "7/7 - 0s - loss: 0.5064 - accuracy: 0.7768 - val_loss: 0.3639 - val_accuracy: 0.8696 - lr: 0.0064 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 69/200\n",
      "7/7 - 1s - loss: 0.3734 - accuracy: 0.8080 - val_loss: 0.3459 - val_accuracy: 0.8261 - lr: 0.0062 - 1s/epoch - 169ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 70/200\n",
      "7/7 - 0s - loss: 0.3814 - accuracy: 0.8170 - val_loss: 0.3444 - val_accuracy: 0.8696 - lr: 0.0060 - 351ms/epoch - 50ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 71/200\n",
      "7/7 - 0s - loss: 0.4294 - accuracy: 0.8170 - val_loss: 0.3454 - val_accuracy: 0.8261 - lr: 0.0058 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 72/200\n",
      "7/7 - 1s - loss: 0.3497 - accuracy: 0.8571 - val_loss: 0.3611 - val_accuracy: 0.8261 - lr: 0.0056 - 568ms/epoch - 81ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 73/200\n",
      "7/7 - 1s - loss: 0.3710 - accuracy: 0.8259 - val_loss: 0.3454 - val_accuracy: 0.8261 - lr: 0.0054 - 664ms/epoch - 95ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 74/200\n",
      "7/7 - 0s - loss: 0.3884 - accuracy: 0.8438 - val_loss: 0.3736 - val_accuracy: 0.8261 - lr: 0.0052 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 75/200\n",
      "7/7 - 0s - loss: 0.3396 - accuracy: 0.8616 - val_loss: 0.3377 - val_accuracy: 0.7826 - lr: 0.0050 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 76/200\n",
      "7/7 - 1s - loss: 0.3266 - accuracy: 0.8661 - val_loss: 0.3574 - val_accuracy: 0.8261 - lr: 0.0048 - 529ms/epoch - 76ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 77/200\n",
      "7/7 - 1s - loss: 0.4066 - accuracy: 0.7946 - val_loss: 0.3324 - val_accuracy: 0.7826 - lr: 0.0046 - 1s/epoch - 184ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 78/200\n",
      "7/7 - 1s - loss: 0.3250 - accuracy: 0.8571 - val_loss: 0.3327 - val_accuracy: 0.7826 - lr: 0.0044 - 1s/epoch - 148ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 79/200\n",
      "7/7 - 0s - loss: 0.3078 - accuracy: 0.8527 - val_loss: 0.3913 - val_accuracy: 0.8261 - lr: 0.0042 - 476ms/epoch - 68ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 80/200\n",
      "7/7 - 0s - loss: 0.3320 - accuracy: 0.8527 - val_loss: 0.3363 - val_accuracy: 0.7391 - lr: 0.0040 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 81/200\n",
      "7/7 - 1s - loss: 0.3402 - accuracy: 0.8661 - val_loss: 0.3844 - val_accuracy: 0.8261 - lr: 0.0038 - 701ms/epoch - 100ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 82/200\n",
      "7/7 - 0s - loss: 0.3963 - accuracy: 0.8482 - val_loss: 0.3331 - val_accuracy: 0.7391 - lr: 0.0036 - 490ms/epoch - 70ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 83/200\n",
      "7/7 - 0s - loss: 0.3495 - accuracy: 0.8438 - val_loss: 0.3518 - val_accuracy: 0.7826 - lr: 0.0034 - 319ms/epoch - 46ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 84/200\n",
      "7/7 - 0s - loss: 0.3275 - accuracy: 0.8661 - val_loss: 0.3348 - val_accuracy: 0.7391 - lr: 0.0032 - 260ms/epoch - 37ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 85/200\n",
      "7/7 - 0s - loss: 0.3554 - accuracy: 0.8304 - val_loss: 0.3423 - val_accuracy: 0.8261 - lr: 0.0030 - 263ms/epoch - 38ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 86/200\n",
      "7/7 - 0s - loss: 0.3216 - accuracy: 0.8571 - val_loss: 0.3561 - val_accuracy: 0.7826 - lr: 0.0028 - 262ms/epoch - 37ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 87/200\n",
      "7/7 - 0s - loss: 0.3820 - accuracy: 0.8795 - val_loss: 0.3539 - val_accuracy: 0.7826 - lr: 0.0026 - 285ms/epoch - 41ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 88/200\n",
      "7/7 - 1s - loss: 0.3214 - accuracy: 0.8839 - val_loss: 0.3347 - val_accuracy: 0.8261 - lr: 0.0024 - 635ms/epoch - 91ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 89/200\n",
      "7/7 - 1s - loss: 0.3351 - accuracy: 0.8527 - val_loss: 0.3395 - val_accuracy: 0.7826 - lr: 0.0022 - 652ms/epoch - 93ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 90/200\n",
      "7/7 - 0s - loss: 0.2935 - accuracy: 0.8571 - val_loss: 0.3395 - val_accuracy: 0.7826 - lr: 0.0020 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 91/200\n",
      "7/7 - 0s - loss: 0.3637 - accuracy: 0.8170 - val_loss: 0.3396 - val_accuracy: 0.7826 - lr: 0.0018 - 498ms/epoch - 71ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 92/200\n",
      "7/7 - 0s - loss: 0.2703 - accuracy: 0.8929 - val_loss: 0.3647 - val_accuracy: 0.7826 - lr: 0.0016 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 93/200\n",
      "7/7 - 0s - loss: 0.3305 - accuracy: 0.8616 - val_loss: 0.3283 - val_accuracy: 0.7826 - lr: 0.0014 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 94/200\n",
      "7/7 - 0s - loss: 0.3161 - accuracy: 0.8616 - val_loss: 0.3452 - val_accuracy: 0.7826 - lr: 0.0012 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 95/200\n",
      "7/7 - 0s - loss: 0.2772 - accuracy: 0.9018 - val_loss: 0.3554 - val_accuracy: 0.7826 - lr: 0.0010 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 96/200\n",
      "7/7 - 0s - loss: 0.3360 - accuracy: 0.8616 - val_loss: 0.3316 - val_accuracy: 0.7391 - lr: 8.0009e-04 - 476ms/epoch - 68ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 97/200\n",
      "7/7 - 0s - loss: 0.3383 - accuracy: 0.8616 - val_loss: 0.3432 - val_accuracy: 0.7826 - lr: 6.0009e-04 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 98/200\n",
      "7/7 - 1s - loss: 0.2665 - accuracy: 0.8795 - val_loss: 0.3445 - val_accuracy: 0.7826 - lr: 4.0010e-04 - 557ms/epoch - 80ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 99/200\n",
      "7/7 - 0s - loss: 0.2923 - accuracy: 0.9018 - val_loss: 0.3465 - val_accuracy: 0.7826 - lr: 2.0010e-04 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 100/200\n",
      "7/7 - 2s - loss: 0.3076 - accuracy: 0.8705 - val_loss: 0.3459 - val_accuracy: 0.7826 - lr: 1.0000e-07 - 2s/epoch - 239ms/step\n",
      "Learning rate: 0.00000010\n",
      "Epoch 101/200\n",
      "7/7 - 0s - loss: 0.3368 - accuracy: 0.8705 - val_loss: 0.3472 - val_accuracy: 0.7826 - lr: 2.0010e-04 - 342ms/epoch - 49ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 102/200\n",
      "7/7 - 0s - loss: 0.2839 - accuracy: 0.8839 - val_loss: 0.3432 - val_accuracy: 0.7826 - lr: 4.0010e-04 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 103/200\n",
      "7/7 - 0s - loss: 0.2736 - accuracy: 0.8795 - val_loss: 0.3516 - val_accuracy: 0.7826 - lr: 6.0009e-04 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 104/200\n",
      "7/7 - 0s - loss: 0.3560 - accuracy: 0.8438 - val_loss: 0.3464 - val_accuracy: 0.7826 - lr: 8.0009e-04 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 105/200\n",
      "7/7 - 0s - loss: 0.3000 - accuracy: 0.8616 - val_loss: 0.3437 - val_accuracy: 0.7826 - lr: 0.0010 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 106/200\n",
      "7/7 - 0s - loss: 0.3450 - accuracy: 0.8661 - val_loss: 0.3346 - val_accuracy: 0.7391 - lr: 0.0012 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 107/200\n",
      "7/7 - 2s - loss: 0.3411 - accuracy: 0.8616 - val_loss: 0.3503 - val_accuracy: 0.7826 - lr: 0.0014 - 2s/epoch - 219ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 108/200\n",
      "7/7 - 0s - loss: 0.3274 - accuracy: 0.8795 - val_loss: 0.3336 - val_accuracy: 0.7826 - lr: 0.0016 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 109/200\n",
      "7/7 - 1s - loss: 0.3050 - accuracy: 0.8616 - val_loss: 0.3248 - val_accuracy: 0.7826 - lr: 0.0018 - 512ms/epoch - 73ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 110/200\n",
      "7/7 - 0s - loss: 0.2754 - accuracy: 0.8795 - val_loss: 0.3567 - val_accuracy: 0.7826 - lr: 0.0020 - 403ms/epoch - 58ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 111/200\n",
      "7/7 - 0s - loss: 0.3153 - accuracy: 0.8571 - val_loss: 0.3467 - val_accuracy: 0.7826 - lr: 0.0022 - 331ms/epoch - 47ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 112/200\n",
      "7/7 - 0s - loss: 0.3140 - accuracy: 0.8616 - val_loss: 0.3502 - val_accuracy: 0.7826 - lr: 0.0024 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 113/200\n",
      "7/7 - 0s - loss: 0.3060 - accuracy: 0.8929 - val_loss: 0.3631 - val_accuracy: 0.7826 - lr: 0.0026 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 114/200\n",
      "7/7 - 0s - loss: 0.2897 - accuracy: 0.8839 - val_loss: 0.3234 - val_accuracy: 0.7826 - lr: 0.0028 - 432ms/epoch - 62ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 115/200\n",
      "7/7 - 2s - loss: 0.2707 - accuracy: 0.8705 - val_loss: 0.3531 - val_accuracy: 0.7826 - lr: 0.0030 - 2s/epoch - 215ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 116/200\n",
      "7/7 - 0s - loss: 0.2479 - accuracy: 0.9062 - val_loss: 0.3264 - val_accuracy: 0.7826 - lr: 0.0032 - 447ms/epoch - 64ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 117/200\n",
      "7/7 - 0s - loss: 0.2629 - accuracy: 0.8884 - val_loss: 0.4838 - val_accuracy: 0.8261 - lr: 0.0034 - 377ms/epoch - 54ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 118/200\n",
      "7/7 - 0s - loss: 0.3929 - accuracy: 0.8304 - val_loss: 0.3330 - val_accuracy: 0.7826 - lr: 0.0036 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 119/200\n",
      "7/7 - 0s - loss: 0.3134 - accuracy: 0.8750 - val_loss: 0.3337 - val_accuracy: 0.8261 - lr: 0.0038 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 120/200\n",
      "7/7 - 0s - loss: 0.2451 - accuracy: 0.9018 - val_loss: 0.3378 - val_accuracy: 0.7826 - lr: 0.0040 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 121/200\n",
      "7/7 - 0s - loss: 0.2963 - accuracy: 0.8571 - val_loss: 0.4123 - val_accuracy: 0.7826 - lr: 0.0042 - 424ms/epoch - 61ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 122/200\n",
      "7/7 - 1s - loss: 0.3247 - accuracy: 0.8616 - val_loss: 0.3657 - val_accuracy: 0.7826 - lr: 0.0044 - 549ms/epoch - 78ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 123/200\n",
      "7/7 - 0s - loss: 0.2860 - accuracy: 0.8705 - val_loss: 0.3537 - val_accuracy: 0.7826 - lr: 0.0046 - 391ms/epoch - 56ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 124/200\n",
      "7/7 - 0s - loss: 0.2791 - accuracy: 0.8929 - val_loss: 0.3494 - val_accuracy: 0.8261 - lr: 0.0048 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 125/200\n",
      "7/7 - 1s - loss: 0.2978 - accuracy: 0.8616 - val_loss: 0.3419 - val_accuracy: 0.7826 - lr: 0.0050 - 1s/epoch - 203ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 126/200\n",
      "7/7 - 1s - loss: 0.3121 - accuracy: 0.8884 - val_loss: 0.3510 - val_accuracy: 0.7826 - lr: 0.0052 - 513ms/epoch - 73ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 127/200\n",
      "7/7 - 1s - loss: 0.3021 - accuracy: 0.8571 - val_loss: 0.3458 - val_accuracy: 0.8261 - lr: 0.0054 - 522ms/epoch - 75ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 128/200\n",
      "7/7 - 0s - loss: 0.3766 - accuracy: 0.8527 - val_loss: 0.3381 - val_accuracy: 0.8261 - lr: 0.0056 - 483ms/epoch - 69ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 129/200\n",
      "7/7 - 0s - loss: 0.2895 - accuracy: 0.8705 - val_loss: 0.3962 - val_accuracy: 0.7826 - lr: 0.0058 - 440ms/epoch - 63ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 130/200\n",
      "7/7 - 1s - loss: 0.2868 - accuracy: 0.8527 - val_loss: 0.3519 - val_accuracy: 0.7391 - lr: 0.0060 - 733ms/epoch - 105ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 131/200\n",
      "7/7 - 0s - loss: 0.2868 - accuracy: 0.8884 - val_loss: 0.3965 - val_accuracy: 0.7826 - lr: 0.0062 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 132/200\n",
      "7/7 - 0s - loss: 0.2815 - accuracy: 0.8795 - val_loss: 0.3733 - val_accuracy: 0.7826 - lr: 0.0064 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 133/200\n",
      "7/7 - 0s - loss: 0.2712 - accuracy: 0.8750 - val_loss: 0.3661 - val_accuracy: 0.8261 - lr: 0.0066 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 134/200\n",
      "7/7 - 0s - loss: 0.2463 - accuracy: 0.9062 - val_loss: 0.3749 - val_accuracy: 0.7826 - lr: 0.0068 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 135/200\n",
      "7/7 - 1s - loss: 0.2645 - accuracy: 0.8973 - val_loss: 0.3575 - val_accuracy: 0.7826 - lr: 0.0070 - 523ms/epoch - 75ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 136/200\n",
      "7/7 - 0s - loss: 0.2599 - accuracy: 0.8973 - val_loss: 0.3601 - val_accuracy: 0.7826 - lr: 0.0072 - 471ms/epoch - 67ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 137/200\n",
      "7/7 - 0s - loss: 0.3113 - accuracy: 0.8616 - val_loss: 0.3697 - val_accuracy: 0.7826 - lr: 0.0074 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 138/200\n",
      "7/7 - 0s - loss: 0.3064 - accuracy: 0.8884 - val_loss: 0.3732 - val_accuracy: 0.7826 - lr: 0.0076 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 139/200\n",
      "7/7 - 0s - loss: 0.2818 - accuracy: 0.8795 - val_loss: 0.5915 - val_accuracy: 0.8261 - lr: 0.0078 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 140/200\n",
      "7/7 - 0s - loss: 0.3177 - accuracy: 0.8839 - val_loss: 0.3296 - val_accuracy: 0.7826 - lr: 0.0080 - 344ms/epoch - 49ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 141/200\n",
      "7/7 - 0s - loss: 0.2951 - accuracy: 0.8750 - val_loss: 0.3679 - val_accuracy: 0.8261 - lr: 0.0082 - 497ms/epoch - 71ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 142/200\n",
      "7/7 - 1s - loss: 0.3052 - accuracy: 0.8661 - val_loss: 0.4829 - val_accuracy: 0.8261 - lr: 0.0084 - 1s/epoch - 154ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 143/200\n",
      "7/7 - 0s - loss: 0.3766 - accuracy: 0.8482 - val_loss: 0.3511 - val_accuracy: 0.8261 - lr: 0.0086 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 144/200\n",
      "7/7 - 0s - loss: 0.2587 - accuracy: 0.8973 - val_loss: 0.3518 - val_accuracy: 0.7826 - lr: 0.0088 - 361ms/epoch - 52ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 145/200\n",
      "7/7 - 0s - loss: 0.2853 - accuracy: 0.8661 - val_loss: 0.4472 - val_accuracy: 0.7826 - lr: 0.0090 - 413ms/epoch - 59ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 146/200\n",
      "7/7 - 0s - loss: 0.2500 - accuracy: 0.8839 - val_loss: 0.3612 - val_accuracy: 0.7826 - lr: 0.0092 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 147/200\n",
      "7/7 - 0s - loss: 0.2497 - accuracy: 0.8929 - val_loss: 0.3428 - val_accuracy: 0.8261 - lr: 0.0094 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 148/200\n",
      "7/7 - 1s - loss: 0.2258 - accuracy: 0.8929 - val_loss: 0.4488 - val_accuracy: 0.7826 - lr: 0.0096 - 579ms/epoch - 83ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 149/200\n",
      "7/7 - 0s - loss: 0.3111 - accuracy: 0.8571 - val_loss: 0.4645 - val_accuracy: 0.8261 - lr: 0.0098 - 484ms/epoch - 69ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 150/200\n",
      "7/7 - 0s - loss: 0.3407 - accuracy: 0.8750 - val_loss: 0.3764 - val_accuracy: 0.7826 - lr: 0.0100 - 343ms/epoch - 49ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 151/200\n",
      "7/7 - 0s - loss: 0.2967 - accuracy: 0.8661 - val_loss: 0.3700 - val_accuracy: 0.8261 - lr: 0.0098 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 152/200\n",
      "7/7 - 0s - loss: 0.2606 - accuracy: 0.9107 - val_loss: 0.4719 - val_accuracy: 0.7826 - lr: 0.0096 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 153/200\n",
      "7/7 - 1s - loss: 0.2944 - accuracy: 0.8795 - val_loss: 0.4009 - val_accuracy: 0.7826 - lr: 0.0094 - 503ms/epoch - 72ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 154/200\n",
      "7/7 - 1s - loss: 0.2438 - accuracy: 0.9062 - val_loss: 0.3998 - val_accuracy: 0.7826 - lr: 0.0092 - 768ms/epoch - 110ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 155/200\n",
      "7/7 - 1s - loss: 0.2579 - accuracy: 0.9018 - val_loss: 0.3746 - val_accuracy: 0.7826 - lr: 0.0090 - 733ms/epoch - 105ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 156/200\n",
      "7/7 - 0s - loss: 0.2476 - accuracy: 0.8973 - val_loss: 0.3486 - val_accuracy: 0.8261 - lr: 0.0088 - 411ms/epoch - 59ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 157/200\n",
      "7/7 - 0s - loss: 0.3428 - accuracy: 0.8750 - val_loss: 0.3707 - val_accuracy: 0.7826 - lr: 0.0086 - 352ms/epoch - 50ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 158/200\n",
      "7/7 - 1s - loss: 0.2210 - accuracy: 0.9286 - val_loss: 0.3534 - val_accuracy: 0.8261 - lr: 0.0084 - 850ms/epoch - 121ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 159/200\n",
      "7/7 - 0s - loss: 0.2648 - accuracy: 0.9062 - val_loss: 0.4289 - val_accuracy: 0.8261 - lr: 0.0082 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 160/200\n",
      "7/7 - 0s - loss: 0.3389 - accuracy: 0.8482 - val_loss: 0.3997 - val_accuracy: 0.8261 - lr: 0.0080 - 356ms/epoch - 51ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 161/200\n",
      "7/7 - 1s - loss: 0.2449 - accuracy: 0.9062 - val_loss: 0.4238 - val_accuracy: 0.7826 - lr: 0.0078 - 813ms/epoch - 116ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 162/200\n",
      "7/7 - 0s - loss: 0.2530 - accuracy: 0.9152 - val_loss: 0.3747 - val_accuracy: 0.7826 - lr: 0.0076 - 479ms/epoch - 68ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 163/200\n",
      "7/7 - 0s - loss: 0.2063 - accuracy: 0.9286 - val_loss: 0.3566 - val_accuracy: 0.7826 - lr: 0.0074 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 164/200\n",
      "7/7 - 0s - loss: 0.2725 - accuracy: 0.8884 - val_loss: 0.3554 - val_accuracy: 0.8261 - lr: 0.0072 - 491ms/epoch - 70ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 165/200\n",
      "7/7 - 0s - loss: 0.2472 - accuracy: 0.8929 - val_loss: 0.3892 - val_accuracy: 0.8261 - lr: 0.0070 - 331ms/epoch - 47ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 166/200\n",
      "7/7 - 0s - loss: 0.2124 - accuracy: 0.9107 - val_loss: 0.3968 - val_accuracy: 0.8261 - lr: 0.0068 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 167/200\n",
      "7/7 - 0s - loss: 0.2346 - accuracy: 0.9107 - val_loss: 0.4175 - val_accuracy: 0.7826 - lr: 0.0066 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 168/200\n",
      "7/7 - 1s - loss: 0.2551 - accuracy: 0.9152 - val_loss: 0.3862 - val_accuracy: 0.8261 - lr: 0.0064 - 1s/epoch - 173ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 169/200\n",
      "7/7 - 0s - loss: 0.2628 - accuracy: 0.8884 - val_loss: 0.3702 - val_accuracy: 0.7826 - lr: 0.0062 - 489ms/epoch - 70ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 170/200\n",
      "7/7 - 1s - loss: 0.2277 - accuracy: 0.9196 - val_loss: 0.4221 - val_accuracy: 0.7826 - lr: 0.0060 - 661ms/epoch - 94ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 171/200\n",
      "7/7 - 0s - loss: 0.2403 - accuracy: 0.9107 - val_loss: 0.3812 - val_accuracy: 0.7826 - lr: 0.0058 - 488ms/epoch - 70ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 172/200\n",
      "7/7 - 0s - loss: 0.2657 - accuracy: 0.9062 - val_loss: 0.4292 - val_accuracy: 0.7826 - lr: 0.0056 - 354ms/epoch - 51ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 173/200\n",
      "7/7 - 0s - loss: 0.2436 - accuracy: 0.9062 - val_loss: 0.3591 - val_accuracy: 0.8261 - lr: 0.0054 - 351ms/epoch - 50ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 174/200\n",
      "7/7 - 0s - loss: 0.2262 - accuracy: 0.9018 - val_loss: 0.3821 - val_accuracy: 0.8261 - lr: 0.0052 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 175/200\n",
      "7/7 - 2s - loss: 0.2226 - accuracy: 0.9062 - val_loss: 0.3621 - val_accuracy: 0.8261 - lr: 0.0050 - 2s/epoch - 279ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 176/200\n",
      "7/7 - 1s - loss: 0.2267 - accuracy: 0.9196 - val_loss: 0.3646 - val_accuracy: 0.8261 - lr: 0.0048 - 529ms/epoch - 76ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 177/200\n",
      "7/7 - 0s - loss: 0.2544 - accuracy: 0.9152 - val_loss: 0.3558 - val_accuracy: 0.8261 - lr: 0.0046 - 364ms/epoch - 52ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 178/200\n",
      "7/7 - 0s - loss: 0.2193 - accuracy: 0.9062 - val_loss: 0.3576 - val_accuracy: 0.8261 - lr: 0.0044 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 179/200\n",
      "7/7 - 0s - loss: 0.2240 - accuracy: 0.9062 - val_loss: 0.3472 - val_accuracy: 0.8261 - lr: 0.0042 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 180/200\n",
      "7/7 - 0s - loss: 0.2303 - accuracy: 0.9196 - val_loss: 0.3457 - val_accuracy: 0.8261 - lr: 0.0040 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 181/200\n",
      "7/7 - 0s - loss: 0.2466 - accuracy: 0.9107 - val_loss: 0.3697 - val_accuracy: 0.7826 - lr: 0.0038 - 499ms/epoch - 71ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 182/200\n",
      "7/7 - 1s - loss: 0.2752 - accuracy: 0.9062 - val_loss: 0.3663 - val_accuracy: 0.7826 - lr: 0.0036 - 1s/epoch - 188ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 183/200\n",
      "7/7 - 1s - loss: 0.2654 - accuracy: 0.8884 - val_loss: 0.3448 - val_accuracy: 0.8696 - lr: 0.0034 - 1s/epoch - 151ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 184/200\n",
      "7/7 - 1s - loss: 0.2363 - accuracy: 0.9286 - val_loss: 0.3627 - val_accuracy: 0.8261 - lr: 0.0032 - 514ms/epoch - 73ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 185/200\n",
      "7/7 - 0s - loss: 0.2225 - accuracy: 0.9107 - val_loss: 0.3906 - val_accuracy: 0.7826 - lr: 0.0030 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 186/200\n",
      "7/7 - 0s - loss: 0.2166 - accuracy: 0.9062 - val_loss: 0.3613 - val_accuracy: 0.8261 - lr: 0.0028 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 187/200\n",
      "7/7 - 1s - loss: 0.2082 - accuracy: 0.9152 - val_loss: 0.3637 - val_accuracy: 0.8261 - lr: 0.0026 - 518ms/epoch - 74ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 188/200\n",
      "7/7 - 0s - loss: 0.2852 - accuracy: 0.8973 - val_loss: 0.3563 - val_accuracy: 0.8261 - lr: 0.0024 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 189/200\n",
      "7/7 - 0s - loss: 0.2382 - accuracy: 0.9241 - val_loss: 0.3481 - val_accuracy: 0.8696 - lr: 0.0022 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 190/200\n",
      "7/7 - 1s - loss: 0.2775 - accuracy: 0.8616 - val_loss: 0.3597 - val_accuracy: 0.8261 - lr: 0.0020 - 812ms/epoch - 116ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 191/200\n",
      "7/7 - 0s - loss: 0.2041 - accuracy: 0.9107 - val_loss: 0.3394 - val_accuracy: 0.8696 - lr: 0.0018 - 490ms/epoch - 70ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 192/200\n",
      "7/7 - 0s - loss: 0.1840 - accuracy: 0.9241 - val_loss: 0.3532 - val_accuracy: 0.8261 - lr: 0.0016 - 331ms/epoch - 47ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 193/200\n",
      "7/7 - 0s - loss: 0.2384 - accuracy: 0.9107 - val_loss: 0.3550 - val_accuracy: 0.8261 - lr: 0.0014 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 194/200\n",
      "7/7 - 1s - loss: 0.2506 - accuracy: 0.9286 - val_loss: 0.3641 - val_accuracy: 0.8261 - lr: 0.0012 - 655ms/epoch - 94ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 195/200\n",
      "7/7 - 0s - loss: 0.2816 - accuracy: 0.9107 - val_loss: 0.3630 - val_accuracy: 0.8261 - lr: 0.0010 - 494ms/epoch - 71ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 196/200\n",
      "7/7 - 0s - loss: 0.2458 - accuracy: 0.9062 - val_loss: 0.3662 - val_accuracy: 0.8261 - lr: 8.0009e-04 - 402ms/epoch - 57ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 197/200\n",
      "7/7 - 0s - loss: 0.2495 - accuracy: 0.9152 - val_loss: 0.3692 - val_accuracy: 0.8261 - lr: 6.0009e-04 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 198/200\n",
      "7/7 - 0s - loss: 0.2306 - accuracy: 0.9152 - val_loss: 0.3611 - val_accuracy: 0.8261 - lr: 4.0010e-04 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 199/200\n",
      "7/7 - 1s - loss: 0.2109 - accuracy: 0.9107 - val_loss: 0.3585 - val_accuracy: 0.8261 - lr: 2.0010e-04 - 849ms/epoch - 121ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 200/200\n",
      "7/7 - 0s - loss: 0.2102 - accuracy: 0.9375 - val_loss: 0.3578 - val_accuracy: 0.8261 - lr: 1.0000e-07 - 355ms/epoch - 51ms/step\n",
      "\n",
      "======================BEST:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp    f1       rfa\n",
      "61   62.0  0.416425  0.816964  0.332134  0.869565  0.939394  0.88  0.894166\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.73      0.80        11\n",
      "           1       0.79      0.92      0.85        12\n",
      "\n",
      "    accuracy                           0.83        23\n",
      "   macro avg       0.84      0.82      0.82        23\n",
      "weighted avg       0.84      0.83      0.82        23\n",
      "\n",
      "[[ 8  3]\n",
      " [ 1 11]]\n",
      "ACCURACY: 0.8260869565217391\n",
      "PRECISION: 0.7857142857142857\n",
      "RECALL: 0.9166666666666666\n",
      "F1: 0.8461538461538461\n",
      "ROC_AUC(Pr.): 0.9318181818181819\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       0.85      0.92      0.88        12\n",
      "\n",
      "    accuracy                           0.87        23\n",
      "   macro avg       0.87      0.87      0.87        23\n",
      "weighted avg       0.87      0.87      0.87        23\n",
      "\n",
      "[[ 9  2]\n",
      " [ 1 11]]\n",
      "ACCURACY: 0.8695652173913043\n",
      "PRECISION: 0.8461538461538461\n",
      "RECALL: 0.9166666666666666\n",
      "F1: 0.8799999999999999\n",
      "ROC_AUC(Pr.): 0.9393939393939393\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       0.85      0.92      0.88        12\n",
      "\n",
      "    accuracy                           0.87        23\n",
      "   macro avg       0.87      0.87      0.87        23\n",
      "weighted avg       0.87      0.87      0.87        23\n",
      "\n",
      "[[ 9  2]\n",
      " [ 1 11]]\n",
      "ACCURACY: 0.8695652173913043\n",
      "PRECISION: 0.8461538461538461\n",
      "RECALL: 0.9166666666666666\n",
      "F1: 0.8799999999999999\n",
      "ROC_AUC(Pr.): 0.9393939393939393\n",
      "\n",
      "=====================CV[5]========================\n",
      "\n",
      "207 train samples\n",
      "23 test samples\n",
      "\n",
      "Over sampling: 207 -> 208\n",
      "Add train samples: 16\n",
      "224 new train samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\tf28\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.00000010\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_28484\\4287042063.py:159: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================EPOCH 000001:======================\n",
      "   epoch      loss      acc  val_loss   val_acc      rocp   f1       rfa\n",
      "0    1.0  0.692494  0.53125  0.693251  0.478261  0.606061  0.0  0.349209\n",
      "RFA : 0.3492094879800623\n",
      "7/7 - 7s - loss: 0.6925 - accuracy: 0.5312 - val_loss: 0.6933 - val_accuracy: 0.4783 - lr: 2.0010e-04 - 7s/epoch - 1s/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 2/200\n",
      "======================EPOCH 000002:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "1    2.0  0.692294  0.540179  0.692043  0.521739  0.757576  0.685714  0.649881\n",
      "RFA : 0.6498814211108467\n",
      "7/7 - 1s - loss: 0.6923 - accuracy: 0.5402 - val_loss: 0.6920 - val_accuracy: 0.5217 - lr: 4.0010e-04 - 1s/epoch - 191ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 3/200\n",
      "======================EPOCH 000003:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1      rfa\n",
      "2    3.0  0.692497  0.540179  0.691534  0.695652  0.772727  0.666667  0.70863\n",
      "RFA : 0.7086297805562163\n",
      "7/7 - 0s - loss: 0.6925 - accuracy: 0.5402 - val_loss: 0.6915 - val_accuracy: 0.6957 - lr: 6.0009e-04 - 389ms/epoch - 56ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 4/200\n",
      "======================EPOCH 000004:======================\n",
      "   epoch      loss       acc  val_loss  val_acc      rocp        f1       rfa\n",
      "3    4.0  0.687771  0.598214  0.686427  0.73913  0.818182  0.769231  0.773381\n",
      "RFA : 0.7733809677662548\n",
      "7/7 - 0s - loss: 0.6878 - accuracy: 0.5982 - val_loss: 0.6864 - val_accuracy: 0.7391 - lr: 8.0009e-04 - 406ms/epoch - 58ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 5/200\n",
      "7/7 - 1s - loss: 0.6707 - accuracy: 0.6295 - val_loss: 0.6568 - val_accuracy: 0.6957 - lr: 0.0010 - 1s/epoch - 211ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 6/200\n",
      "7/7 - 0s - loss: 0.6021 - accuracy: 0.7321 - val_loss: 0.5528 - val_accuracy: 0.6957 - lr: 0.0012 - 363ms/epoch - 52ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 7/200\n",
      "7/7 - 0s - loss: 0.4848 - accuracy: 0.7723 - val_loss: 0.4844 - val_accuracy: 0.6957 - lr: 0.0014 - 359ms/epoch - 51ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 8/200\n",
      "7/7 - 0s - loss: 0.4843 - accuracy: 0.7545 - val_loss: 0.5045 - val_accuracy: 0.6522 - lr: 0.0016 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 9/200\n",
      "======================EPOCH 000009:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "8    9.0  0.455479  0.754464  0.420011  0.913043  0.954545  0.909091  0.924111\n",
      "RFA : 0.9241106791929765\n",
      "7/7 - 2s - loss: 0.4555 - accuracy: 0.7545 - val_loss: 0.4200 - val_accuracy: 0.9130 - lr: 0.0018 - 2s/epoch - 228ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 10/200\n",
      "7/7 - 1s - loss: 0.3955 - accuracy: 0.8259 - val_loss: 0.3484 - val_accuracy: 0.8261 - lr: 0.0020 - 1s/epoch - 201ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 11/200\n",
      "7/7 - 0s - loss: 0.4182 - accuracy: 0.8304 - val_loss: 0.3433 - val_accuracy: 0.8696 - lr: 0.0022 - 469ms/epoch - 67ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 12/200\n",
      "7/7 - 0s - loss: 0.4318 - accuracy: 0.7857 - val_loss: 0.4388 - val_accuracy: 0.8261 - lr: 0.0024 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 13/200\n",
      "7/7 - 0s - loss: 0.4012 - accuracy: 0.8036 - val_loss: 0.3998 - val_accuracy: 0.8261 - lr: 0.0026 - 362ms/epoch - 52ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 14/200\n",
      "7/7 - 1s - loss: 0.3590 - accuracy: 0.8259 - val_loss: 0.3846 - val_accuracy: 0.8261 - lr: 0.0028 - 1s/epoch - 210ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 15/200\n",
      "7/7 - 0s - loss: 0.3564 - accuracy: 0.8170 - val_loss: 0.3416 - val_accuracy: 0.8696 - lr: 0.0030 - 482ms/epoch - 69ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 16/200\n",
      "7/7 - 0s - loss: 0.3966 - accuracy: 0.8080 - val_loss: 0.3647 - val_accuracy: 0.8696 - lr: 0.0032 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 17/200\n",
      "7/7 - 2s - loss: 0.3742 - accuracy: 0.8348 - val_loss: 0.3530 - val_accuracy: 0.8261 - lr: 0.0034 - 2s/epoch - 231ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 18/200\n",
      "7/7 - 0s - loss: 0.3283 - accuracy: 0.8750 - val_loss: 0.3883 - val_accuracy: 0.8261 - lr: 0.0036 - 493ms/epoch - 70ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 19/200\n",
      "7/7 - 1s - loss: 0.3506 - accuracy: 0.8393 - val_loss: 0.3406 - val_accuracy: 0.8261 - lr: 0.0038 - 706ms/epoch - 101ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 20/200\n",
      "7/7 - 0s - loss: 0.3786 - accuracy: 0.8304 - val_loss: 0.4501 - val_accuracy: 0.8261 - lr: 0.0040 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 21/200\n",
      "7/7 - 0s - loss: 0.3550 - accuracy: 0.8348 - val_loss: 0.3657 - val_accuracy: 0.8261 - lr: 0.0042 - 356ms/epoch - 51ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 22/200\n",
      "7/7 - 1s - loss: 0.2943 - accuracy: 0.8884 - val_loss: 0.4800 - val_accuracy: 0.6957 - lr: 0.0044 - 878ms/epoch - 125ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 23/200\n",
      "7/7 - 0s - loss: 0.3818 - accuracy: 0.8571 - val_loss: 0.3938 - val_accuracy: 0.7826 - lr: 0.0046 - 355ms/epoch - 51ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 24/200\n",
      "7/7 - 0s - loss: 0.3205 - accuracy: 0.8616 - val_loss: 0.3042 - val_accuracy: 0.8696 - lr: 0.0048 - 360ms/epoch - 51ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 25/200\n",
      "7/7 - 1s - loss: 0.3466 - accuracy: 0.8438 - val_loss: 0.4163 - val_accuracy: 0.7391 - lr: 0.0050 - 501ms/epoch - 72ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 26/200\n",
      "7/7 - 0s - loss: 0.3410 - accuracy: 0.8482 - val_loss: 0.3196 - val_accuracy: 0.8696 - lr: 0.0052 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 27/200\n",
      "7/7 - 0s - loss: 0.3986 - accuracy: 0.8304 - val_loss: 0.4037 - val_accuracy: 0.7826 - lr: 0.0054 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 28/200\n",
      "7/7 - 0s - loss: 0.3073 - accuracy: 0.8348 - val_loss: 0.3128 - val_accuracy: 0.8696 - lr: 0.0056 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 29/200\n",
      "7/7 - 1s - loss: 0.2781 - accuracy: 0.9062 - val_loss: 0.5686 - val_accuracy: 0.6957 - lr: 0.0058 - 1s/epoch - 179ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 30/200\n",
      "7/7 - 0s - loss: 0.4458 - accuracy: 0.7812 - val_loss: 0.3588 - val_accuracy: 0.8261 - lr: 0.0060 - 449ms/epoch - 64ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 31/200\n",
      "7/7 - 0s - loss: 0.3573 - accuracy: 0.8482 - val_loss: 0.3103 - val_accuracy: 0.8261 - lr: 0.0062 - 341ms/epoch - 49ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 32/200\n",
      "7/7 - 1s - loss: 0.3650 - accuracy: 0.8393 - val_loss: 0.4186 - val_accuracy: 0.8261 - lr: 0.0064 - 512ms/epoch - 73ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 33/200\n",
      "7/7 - 1s - loss: 0.2980 - accuracy: 0.8571 - val_loss: 0.3777 - val_accuracy: 0.8261 - lr: 0.0066 - 1s/epoch - 175ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 34/200\n",
      "7/7 - 1s - loss: 0.3087 - accuracy: 0.8705 - val_loss: 0.4403 - val_accuracy: 0.8261 - lr: 0.0068 - 513ms/epoch - 73ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 35/200\n",
      "7/7 - 1s - loss: 0.2858 - accuracy: 0.8571 - val_loss: 0.3034 - val_accuracy: 0.8696 - lr: 0.0070 - 503ms/epoch - 72ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 36/200\n",
      "7/7 - 0s - loss: 0.2725 - accuracy: 0.8973 - val_loss: 0.4257 - val_accuracy: 0.6957 - lr: 0.0072 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 37/200\n",
      "7/7 - 0s - loss: 0.3202 - accuracy: 0.8259 - val_loss: 0.3267 - val_accuracy: 0.8696 - lr: 0.0074 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 38/200\n",
      "7/7 - 0s - loss: 0.3041 - accuracy: 0.8839 - val_loss: 0.4593 - val_accuracy: 0.8261 - lr: 0.0076 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 39/200\n",
      "7/7 - 1s - loss: 0.2842 - accuracy: 0.8705 - val_loss: 0.3787 - val_accuracy: 0.8261 - lr: 0.0078 - 526ms/epoch - 75ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 40/200\n",
      "7/7 - 1s - loss: 0.4088 - accuracy: 0.7946 - val_loss: 0.4624 - val_accuracy: 0.7826 - lr: 0.0080 - 808ms/epoch - 115ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 41/200\n",
      "7/7 - 0s - loss: 0.3456 - accuracy: 0.8214 - val_loss: 0.3266 - val_accuracy: 0.8696 - lr: 0.0082 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 42/200\n",
      "7/7 - 0s - loss: 0.2756 - accuracy: 0.8795 - val_loss: 0.6093 - val_accuracy: 0.7826 - lr: 0.0084 - 315ms/epoch - 45ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 43/200\n",
      "7/7 - 0s - loss: 0.3220 - accuracy: 0.8393 - val_loss: 0.3145 - val_accuracy: 0.8696 - lr: 0.0086 - 425ms/epoch - 61ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 44/200\n",
      "7/7 - 1s - loss: 0.2705 - accuracy: 0.8884 - val_loss: 0.3302 - val_accuracy: 0.8696 - lr: 0.0088 - 956ms/epoch - 137ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 45/200\n",
      "7/7 - 1s - loss: 0.3716 - accuracy: 0.8348 - val_loss: 0.3512 - val_accuracy: 0.8261 - lr: 0.0090 - 732ms/epoch - 105ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 46/200\n",
      "7/7 - 0s - loss: 0.3195 - accuracy: 0.8571 - val_loss: 0.3581 - val_accuracy: 0.8261 - lr: 0.0092 - 474ms/epoch - 68ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 47/200\n",
      "7/7 - 0s - loss: 0.3461 - accuracy: 0.8661 - val_loss: 0.3286 - val_accuracy: 0.8696 - lr: 0.0094 - 423ms/epoch - 60ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 48/200\n",
      "7/7 - 0s - loss: 0.2702 - accuracy: 0.8705 - val_loss: 0.4161 - val_accuracy: 0.7391 - lr: 0.0096 - 395ms/epoch - 56ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 49/200\n",
      "7/7 - 0s - loss: 0.3298 - accuracy: 0.8616 - val_loss: 0.3262 - val_accuracy: 0.8696 - lr: 0.0098 - 411ms/epoch - 59ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 50/200\n",
      "7/7 - 0s - loss: 0.3012 - accuracy: 0.8571 - val_loss: 0.5365 - val_accuracy: 0.8261 - lr: 0.0100 - 395ms/epoch - 56ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 51/200\n",
      "7/7 - 1s - loss: 0.2629 - accuracy: 0.8929 - val_loss: 0.4195 - val_accuracy: 0.8261 - lr: 0.0098 - 590ms/epoch - 84ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 52/200\n",
      "7/7 - 0s - loss: 0.2805 - accuracy: 0.8839 - val_loss: 0.3134 - val_accuracy: 0.8696 - lr: 0.0096 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 53/200\n",
      "7/7 - 0s - loss: 0.3682 - accuracy: 0.8214 - val_loss: 0.3452 - val_accuracy: 0.8696 - lr: 0.0094 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 54/200\n",
      "7/7 - 1s - loss: 1.0996 - accuracy: 0.7054 - val_loss: 0.4490 - val_accuracy: 0.8261 - lr: 0.0092 - 569ms/epoch - 81ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 55/200\n",
      "7/7 - 1s - loss: 18.5110 - accuracy: 0.6429 - val_loss: 26.9614 - val_accuracy: 0.3478 - lr: 0.0090 - 1s/epoch - 209ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 56/200\n",
      "7/7 - 1s - loss: 312.5735 - accuracy: 0.5179 - val_loss: 40.5802 - val_accuracy: 0.5217 - lr: 0.0088 - 507ms/epoch - 72ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 57/200\n",
      "7/7 - 0s - loss: 161.8774 - accuracy: 0.4866 - val_loss: 148.2305 - val_accuracy: 0.5217 - lr: 0.0086 - 375ms/epoch - 54ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 58/200\n",
      "7/7 - 0s - loss: 21.7311 - accuracy: 0.5357 - val_loss: 1.5689 - val_accuracy: 0.4783 - lr: 0.0084 - 461ms/epoch - 66ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 59/200\n",
      "7/7 - 0s - loss: 0.7839 - accuracy: 0.7321 - val_loss: 0.2866 - val_accuracy: 0.8696 - lr: 0.0082 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 60/200\n",
      "7/7 - 0s - loss: 0.5672 - accuracy: 0.7946 - val_loss: 0.2984 - val_accuracy: 0.8261 - lr: 0.0080 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 61/200\n",
      "7/7 - 1s - loss: 0.4275 - accuracy: 0.8036 - val_loss: 0.2655 - val_accuracy: 0.8696 - lr: 0.0078 - 799ms/epoch - 114ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 62/200\n",
      "7/7 - 0s - loss: 0.4191 - accuracy: 0.8080 - val_loss: 0.2703 - val_accuracy: 0.8696 - lr: 0.0076 - 360ms/epoch - 51ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 63/200\n",
      "======================EPOCH 000063:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "62   63.0  0.474509  0.785714  0.276208  0.913043  0.962121  0.923077  0.931279\n",
      "RFA : 0.9312785113608086\n",
      "7/7 - 0s - loss: 0.4745 - accuracy: 0.7857 - val_loss: 0.2762 - val_accuracy: 0.9130 - lr: 0.0074 - 361ms/epoch - 52ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 64/200\n",
      "7/7 - 0s - loss: 0.4179 - accuracy: 0.8348 - val_loss: 0.2994 - val_accuracy: 0.9130 - lr: 0.0072 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 65/200\n",
      "7/7 - 0s - loss: 0.3620 - accuracy: 0.8304 - val_loss: 0.2996 - val_accuracy: 0.9130 - lr: 0.0070 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 66/200\n",
      "7/7 - 1s - loss: 0.3726 - accuracy: 0.8438 - val_loss: 0.2928 - val_accuracy: 0.8696 - lr: 0.0068 - 694ms/epoch - 99ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 67/200\n",
      "7/7 - 0s - loss: 0.4608 - accuracy: 0.7723 - val_loss: 0.2968 - val_accuracy: 0.8696 - lr: 0.0066 - 343ms/epoch - 49ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 68/200\n",
      "7/7 - 0s - loss: 0.3993 - accuracy: 0.8125 - val_loss: 0.3367 - val_accuracy: 0.8696 - lr: 0.0064 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 69/200\n",
      "7/7 - 1s - loss: 0.3438 - accuracy: 0.8304 - val_loss: 0.4677 - val_accuracy: 0.8261 - lr: 0.0062 - 517ms/epoch - 74ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 70/200\n",
      "7/7 - 1s - loss: 0.3226 - accuracy: 0.8527 - val_loss: 0.2988 - val_accuracy: 0.8696 - lr: 0.0060 - 767ms/epoch - 110ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 71/200\n",
      "7/7 - 1s - loss: 0.3799 - accuracy: 0.8438 - val_loss: 0.3494 - val_accuracy: 0.8261 - lr: 0.0058 - 1s/epoch - 144ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 72/200\n",
      "7/7 - 0s - loss: 0.3055 - accuracy: 0.8527 - val_loss: 0.3116 - val_accuracy: 0.8696 - lr: 0.0056 - 487ms/epoch - 70ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 73/200\n",
      "7/7 - 0s - loss: 0.3115 - accuracy: 0.8571 - val_loss: 0.4505 - val_accuracy: 0.8261 - lr: 0.0054 - 344ms/epoch - 49ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 74/200\n",
      "7/7 - 0s - loss: 0.4150 - accuracy: 0.8125 - val_loss: 0.3987 - val_accuracy: 0.8261 - lr: 0.0052 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 75/200\n",
      "7/7 - 1s - loss: 0.3449 - accuracy: 0.8393 - val_loss: 0.3444 - val_accuracy: 0.8261 - lr: 0.0050 - 688ms/epoch - 98ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 76/200\n",
      "7/7 - 0s - loss: 0.3282 - accuracy: 0.8482 - val_loss: 0.3608 - val_accuracy: 0.8261 - lr: 0.0048 - 488ms/epoch - 70ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 77/200\n",
      "7/7 - 0s - loss: 0.3452 - accuracy: 0.8527 - val_loss: 0.3229 - val_accuracy: 0.8696 - lr: 0.0046 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 78/200\n",
      "7/7 - 1s - loss: 0.3098 - accuracy: 0.8482 - val_loss: 0.4733 - val_accuracy: 0.8261 - lr: 0.0044 - 1s/epoch - 154ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 79/200\n",
      "7/7 - 0s - loss: 0.2940 - accuracy: 0.8839 - val_loss: 0.3400 - val_accuracy: 0.8261 - lr: 0.0042 - 474ms/epoch - 68ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 80/200\n",
      "7/7 - 0s - loss: 0.2899 - accuracy: 0.9018 - val_loss: 0.3522 - val_accuracy: 0.7826 - lr: 0.0040 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 81/200\n",
      "7/7 - 0s - loss: 0.3218 - accuracy: 0.8661 - val_loss: 0.3764 - val_accuracy: 0.8261 - lr: 0.0038 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 82/200\n",
      "7/7 - 0s - loss: 0.2865 - accuracy: 0.8705 - val_loss: 0.3298 - val_accuracy: 0.8696 - lr: 0.0036 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 83/200\n",
      "7/7 - 1s - loss: 0.2860 - accuracy: 0.8571 - val_loss: 0.3313 - val_accuracy: 0.8261 - lr: 0.0034 - 1s/epoch - 146ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 84/200\n",
      "7/7 - 0s - loss: 0.2682 - accuracy: 0.8884 - val_loss: 0.3430 - val_accuracy: 0.8696 - lr: 0.0032 - 321ms/epoch - 46ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 85/200\n",
      "7/7 - 1s - loss: 0.2611 - accuracy: 0.8929 - val_loss: 0.3674 - val_accuracy: 0.7826 - lr: 0.0030 - 906ms/epoch - 129ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 86/200\n",
      "7/7 - 1s - loss: 0.2565 - accuracy: 0.8839 - val_loss: 0.3514 - val_accuracy: 0.8696 - lr: 0.0028 - 501ms/epoch - 72ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 87/200\n",
      "7/7 - 1s - loss: 0.3064 - accuracy: 0.8884 - val_loss: 0.3606 - val_accuracy: 0.8261 - lr: 0.0026 - 1s/epoch - 203ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 88/200\n",
      "7/7 - 1s - loss: 0.2523 - accuracy: 0.8839 - val_loss: 0.3548 - val_accuracy: 0.8261 - lr: 0.0024 - 505ms/epoch - 72ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 89/200\n",
      "7/7 - 1s - loss: 0.3089 - accuracy: 0.8705 - val_loss: 0.3752 - val_accuracy: 0.8261 - lr: 0.0022 - 896ms/epoch - 128ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 90/200\n",
      "7/7 - 0s - loss: 0.2714 - accuracy: 0.9018 - val_loss: 0.3494 - val_accuracy: 0.8696 - lr: 0.0020 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 91/200\n",
      "7/7 - 0s - loss: 0.2630 - accuracy: 0.8884 - val_loss: 0.3511 - val_accuracy: 0.8696 - lr: 0.0018 - 416ms/epoch - 59ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 92/200\n",
      "7/7 - 2s - loss: 0.2980 - accuracy: 0.8839 - val_loss: 0.3504 - val_accuracy: 0.8696 - lr: 0.0016 - 2s/epoch - 254ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 93/200\n",
      "7/7 - 0s - loss: 0.2385 - accuracy: 0.9018 - val_loss: 0.3555 - val_accuracy: 0.8696 - lr: 0.0014 - 491ms/epoch - 70ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 94/200\n",
      "7/7 - 0s - loss: 0.2448 - accuracy: 0.9062 - val_loss: 0.3487 - val_accuracy: 0.8696 - lr: 0.0012 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 95/200\n",
      "7/7 - 0s - loss: 0.2433 - accuracy: 0.9018 - val_loss: 0.3591 - val_accuracy: 0.8696 - lr: 0.0010 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 96/200\n",
      "7/7 - 1s - loss: 0.2580 - accuracy: 0.8884 - val_loss: 0.3469 - val_accuracy: 0.8696 - lr: 8.0009e-04 - 1s/epoch - 177ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 97/200\n",
      "7/7 - 0s - loss: 0.2482 - accuracy: 0.8929 - val_loss: 0.3570 - val_accuracy: 0.8696 - lr: 6.0009e-04 - 492ms/epoch - 70ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 98/200\n",
      "7/7 - 0s - loss: 0.2621 - accuracy: 0.8973 - val_loss: 0.3567 - val_accuracy: 0.8696 - lr: 4.0010e-04 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 99/200\n",
      "7/7 - 0s - loss: 0.2885 - accuracy: 0.9018 - val_loss: 0.3559 - val_accuracy: 0.8696 - lr: 2.0010e-04 - 316ms/epoch - 45ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 100/200\n",
      "7/7 - 1s - loss: 0.2583 - accuracy: 0.8973 - val_loss: 0.3587 - val_accuracy: 0.8696 - lr: 1.0000e-07 - 1s/epoch - 177ms/step\n",
      "Learning rate: 0.00000010\n",
      "Epoch 101/200\n",
      "7/7 - 0s - loss: 0.3146 - accuracy: 0.8884 - val_loss: 0.3577 - val_accuracy: 0.8696 - lr: 2.0010e-04 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 102/200\n",
      "7/7 - 0s - loss: 0.2710 - accuracy: 0.8884 - val_loss: 0.3572 - val_accuracy: 0.8696 - lr: 4.0010e-04 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 103/200\n",
      "7/7 - 0s - loss: 0.2322 - accuracy: 0.9062 - val_loss: 0.3577 - val_accuracy: 0.8696 - lr: 6.0009e-04 - 388ms/epoch - 55ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 104/200\n",
      "7/7 - 1s - loss: 0.2458 - accuracy: 0.9018 - val_loss: 0.3644 - val_accuracy: 0.8696 - lr: 8.0009e-04 - 1s/epoch - 171ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 105/200\n",
      "7/7 - 0s - loss: 0.2525 - accuracy: 0.8973 - val_loss: 0.3657 - val_accuracy: 0.8696 - lr: 0.0010 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 106/200\n",
      "7/7 - 0s - loss: 0.2785 - accuracy: 0.8705 - val_loss: 0.3632 - val_accuracy: 0.8696 - lr: 0.0012 - 427ms/epoch - 61ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 107/200\n",
      "7/7 - 1s - loss: 0.2470 - accuracy: 0.9196 - val_loss: 0.3760 - val_accuracy: 0.8261 - lr: 0.0014 - 1s/epoch - 205ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 108/200\n",
      "7/7 - 0s - loss: 0.2937 - accuracy: 0.8839 - val_loss: 0.3617 - val_accuracy: 0.8696 - lr: 0.0016 - 490ms/epoch - 70ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 109/200\n",
      "7/7 - 0s - loss: 0.2820 - accuracy: 0.8929 - val_loss: 0.3606 - val_accuracy: 0.8696 - lr: 0.0018 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 110/200\n",
      "7/7 - 1s - loss: 0.2289 - accuracy: 0.9018 - val_loss: 0.3775 - val_accuracy: 0.8261 - lr: 0.0020 - 842ms/epoch - 120ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 111/200\n",
      "7/7 - 0s - loss: 0.2866 - accuracy: 0.8661 - val_loss: 0.3775 - val_accuracy: 0.8261 - lr: 0.0022 - 346ms/epoch - 49ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 112/200\n",
      "7/7 - 0s - loss: 0.2964 - accuracy: 0.8884 - val_loss: 0.3665 - val_accuracy: 0.8261 - lr: 0.0024 - 351ms/epoch - 50ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 113/200\n",
      "7/7 - 1s - loss: 0.2416 - accuracy: 0.8929 - val_loss: 0.3829 - val_accuracy: 0.8261 - lr: 0.0026 - 1s/epoch - 186ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 114/200\n",
      "7/7 - 1s - loss: 0.2774 - accuracy: 0.8929 - val_loss: 0.4283 - val_accuracy: 0.8261 - lr: 0.0028 - 679ms/epoch - 97ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 115/200\n",
      "7/7 - 0s - loss: 0.3018 - accuracy: 0.8795 - val_loss: 0.4127 - val_accuracy: 0.8261 - lr: 0.0030 - 475ms/epoch - 68ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 116/200\n",
      "7/7 - 0s - loss: 0.2968 - accuracy: 0.8795 - val_loss: 0.3641 - val_accuracy: 0.8261 - lr: 0.0032 - 343ms/epoch - 49ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 117/200\n",
      "7/7 - 1s - loss: 0.2799 - accuracy: 0.8839 - val_loss: 0.4114 - val_accuracy: 0.7826 - lr: 0.0034 - 1s/epoch - 163ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 118/200\n",
      "7/7 - 0s - loss: 0.2683 - accuracy: 0.9018 - val_loss: 0.4796 - val_accuracy: 0.8261 - lr: 0.0036 - 486ms/epoch - 69ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 119/200\n",
      "7/7 - 0s - loss: 0.2831 - accuracy: 0.8795 - val_loss: 0.4244 - val_accuracy: 0.8261 - lr: 0.0038 - 320ms/epoch - 46ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 120/200\n",
      "7/7 - 0s - loss: 0.2390 - accuracy: 0.9062 - val_loss: 0.3655 - val_accuracy: 0.8261 - lr: 0.0040 - 331ms/epoch - 47ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 121/200\n",
      "7/7 - 0s - loss: 0.2534 - accuracy: 0.8973 - val_loss: 0.4028 - val_accuracy: 0.8261 - lr: 0.0042 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 122/200\n",
      "7/7 - 1s - loss: 0.2993 - accuracy: 0.8705 - val_loss: 0.3741 - val_accuracy: 0.8261 - lr: 0.0044 - 526ms/epoch - 75ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 123/200\n",
      "7/7 - 0s - loss: 0.2269 - accuracy: 0.9107 - val_loss: 0.3896 - val_accuracy: 0.8261 - lr: 0.0046 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 124/200\n",
      "7/7 - 0s - loss: 0.2533 - accuracy: 0.9152 - val_loss: 0.4170 - val_accuracy: 0.8261 - lr: 0.0048 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 125/200\n",
      "7/7 - 0s - loss: 0.2516 - accuracy: 0.9062 - val_loss: 0.3804 - val_accuracy: 0.8261 - lr: 0.0050 - 294ms/epoch - 42ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 126/200\n",
      "7/7 - 0s - loss: 0.2733 - accuracy: 0.9062 - val_loss: 0.3775 - val_accuracy: 0.8261 - lr: 0.0052 - 278ms/epoch - 40ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 127/200\n",
      "7/7 - 1s - loss: 0.2505 - accuracy: 0.8884 - val_loss: 0.4005 - val_accuracy: 0.8696 - lr: 0.0054 - 593ms/epoch - 85ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 128/200\n",
      "7/7 - 0s - loss: 0.3329 - accuracy: 0.8795 - val_loss: 0.3540 - val_accuracy: 0.8261 - lr: 0.0056 - 351ms/epoch - 50ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 129/200\n",
      "7/7 - 0s - loss: 0.3367 - accuracy: 0.8616 - val_loss: 0.4169 - val_accuracy: 0.6957 - lr: 0.0058 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 130/200\n",
      "7/7 - 0s - loss: 0.2750 - accuracy: 0.8795 - val_loss: 0.3430 - val_accuracy: 0.8696 - lr: 0.0060 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 131/200\n",
      "7/7 - 0s - loss: 0.2574 - accuracy: 0.8705 - val_loss: 0.4399 - val_accuracy: 0.8261 - lr: 0.0062 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 132/200\n",
      "7/7 - 1s - loss: 0.2481 - accuracy: 0.8929 - val_loss: 0.3572 - val_accuracy: 0.8261 - lr: 0.0064 - 518ms/epoch - 74ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 133/200\n",
      "7/7 - 1s - loss: 0.2886 - accuracy: 0.8839 - val_loss: 0.3835 - val_accuracy: 0.8261 - lr: 0.0066 - 798ms/epoch - 114ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 134/200\n",
      "7/7 - 1s - loss: 0.2854 - accuracy: 0.9062 - val_loss: 0.3490 - val_accuracy: 0.8261 - lr: 0.0068 - 614ms/epoch - 88ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 135/200\n",
      "7/7 - 1s - loss: 0.2481 - accuracy: 0.8795 - val_loss: 0.4492 - val_accuracy: 0.6957 - lr: 0.0070 - 520ms/epoch - 74ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 136/200\n",
      "7/7 - 1s - loss: 0.2621 - accuracy: 0.8929 - val_loss: 0.3832 - val_accuracy: 0.8696 - lr: 0.0072 - 870ms/epoch - 124ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 137/200\n",
      "7/7 - 0s - loss: 0.2743 - accuracy: 0.8929 - val_loss: 0.3686 - val_accuracy: 0.8696 - lr: 0.0074 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 138/200\n",
      "7/7 - 0s - loss: 0.2692 - accuracy: 0.8884 - val_loss: 0.3870 - val_accuracy: 0.8261 - lr: 0.0076 - 439ms/epoch - 63ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 139/200\n",
      "7/7 - 0s - loss: 0.2362 - accuracy: 0.8884 - val_loss: 0.3678 - val_accuracy: 0.8696 - lr: 0.0078 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 140/200\n",
      "7/7 - 0s - loss: 0.2442 - accuracy: 0.9152 - val_loss: 0.3923 - val_accuracy: 0.8261 - lr: 0.0080 - 341ms/epoch - 49ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 141/200\n",
      "7/7 - 0s - loss: 0.2292 - accuracy: 0.9241 - val_loss: 0.3959 - val_accuracy: 0.8261 - lr: 0.0082 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 142/200\n",
      "7/7 - 0s - loss: 0.2387 - accuracy: 0.9018 - val_loss: 0.4139 - val_accuracy: 0.8261 - lr: 0.0084 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 143/200\n",
      "7/7 - 0s - loss: 0.2703 - accuracy: 0.9018 - val_loss: 0.3934 - val_accuracy: 0.8261 - lr: 0.0086 - 482ms/epoch - 69ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 144/200\n",
      "7/7 - 2s - loss: 0.2507 - accuracy: 0.9018 - val_loss: 0.5699 - val_accuracy: 0.8261 - lr: 0.0088 - 2s/epoch - 234ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 145/200\n",
      "7/7 - 1s - loss: 0.3502 - accuracy: 0.8482 - val_loss: 0.3718 - val_accuracy: 0.8261 - lr: 0.0090 - 554ms/epoch - 79ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 146/200\n",
      "7/7 - 0s - loss: 0.2888 - accuracy: 0.8795 - val_loss: 0.3759 - val_accuracy: 0.8261 - lr: 0.0092 - 353ms/epoch - 50ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 147/200\n",
      "7/7 - 1s - loss: 0.2498 - accuracy: 0.9018 - val_loss: 0.4137 - val_accuracy: 0.8261 - lr: 0.0094 - 746ms/epoch - 107ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 148/200\n",
      "7/7 - 1s - loss: 0.2533 - accuracy: 0.9107 - val_loss: 0.4195 - val_accuracy: 0.8261 - lr: 0.0096 - 565ms/epoch - 81ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 149/200\n",
      "7/7 - 1s - loss: 0.3444 - accuracy: 0.8661 - val_loss: 0.3857 - val_accuracy: 0.7826 - lr: 0.0098 - 1s/epoch - 143ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 150/200\n",
      "7/7 - 0s - loss: 0.3404 - accuracy: 0.8348 - val_loss: 0.4340 - val_accuracy: 0.6522 - lr: 0.0100 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 151/200\n",
      "7/7 - 1s - loss: 0.2365 - accuracy: 0.9018 - val_loss: 0.4036 - val_accuracy: 0.7826 - lr: 0.0098 - 567ms/epoch - 81ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 152/200\n",
      "7/7 - 1s - loss: 0.2820 - accuracy: 0.8661 - val_loss: 0.3340 - val_accuracy: 0.8261 - lr: 0.0096 - 870ms/epoch - 124ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 153/200\n",
      "7/7 - 0s - loss: 0.2665 - accuracy: 0.8929 - val_loss: 0.4162 - val_accuracy: 0.7826 - lr: 0.0094 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 154/200\n",
      "7/7 - 0s - loss: 0.3128 - accuracy: 0.8750 - val_loss: 0.3749 - val_accuracy: 0.8261 - lr: 0.0092 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 155/200\n",
      "7/7 - 1s - loss: 0.2662 - accuracy: 0.8750 - val_loss: 0.3668 - val_accuracy: 0.8261 - lr: 0.0090 - 584ms/epoch - 83ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 156/200\n",
      "7/7 - 0s - loss: 0.2995 - accuracy: 0.8705 - val_loss: 0.3877 - val_accuracy: 0.8261 - lr: 0.0088 - 346ms/epoch - 49ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 157/200\n",
      "7/7 - 0s - loss: 0.2712 - accuracy: 0.9018 - val_loss: 0.3634 - val_accuracy: 0.8261 - lr: 0.0086 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 158/200\n",
      "7/7 - 0s - loss: 0.2443 - accuracy: 0.8973 - val_loss: 0.3929 - val_accuracy: 0.8261 - lr: 0.0084 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 159/200\n",
      "7/7 - 0s - loss: 0.2104 - accuracy: 0.9062 - val_loss: 0.6105 - val_accuracy: 0.8261 - lr: 0.0082 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 160/200\n",
      "7/7 - 1s - loss: 0.3027 - accuracy: 0.8616 - val_loss: 0.5015 - val_accuracy: 0.8261 - lr: 0.0080 - 508ms/epoch - 73ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 161/200\n",
      "7/7 - 0s - loss: 0.2589 - accuracy: 0.8839 - val_loss: 0.4512 - val_accuracy: 0.8261 - lr: 0.0078 - 400ms/epoch - 57ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 162/200\n",
      "7/7 - 0s - loss: 0.2512 - accuracy: 0.8884 - val_loss: 0.3537 - val_accuracy: 0.8696 - lr: 0.0076 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 163/200\n",
      "7/7 - 0s - loss: 0.1966 - accuracy: 0.9241 - val_loss: 0.3950 - val_accuracy: 0.8261 - lr: 0.0074 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 164/200\n",
      "7/7 - 1s - loss: 0.2826 - accuracy: 0.9107 - val_loss: 0.3786 - val_accuracy: 0.8261 - lr: 0.0072 - 521ms/epoch - 74ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 165/200\n",
      "7/7 - 1s - loss: 0.2052 - accuracy: 0.9152 - val_loss: 0.4054 - val_accuracy: 0.8261 - lr: 0.0070 - 770ms/epoch - 110ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 166/200\n",
      "7/7 - 1s - loss: 0.2358 - accuracy: 0.9152 - val_loss: 0.3876 - val_accuracy: 0.8696 - lr: 0.0068 - 847ms/epoch - 121ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 167/200\n",
      "7/7 - 0s - loss: 0.1920 - accuracy: 0.9196 - val_loss: 0.3950 - val_accuracy: 0.8261 - lr: 0.0066 - 342ms/epoch - 49ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 168/200\n",
      "7/7 - 0s - loss: 0.3048 - accuracy: 0.9018 - val_loss: 0.4160 - val_accuracy: 0.8261 - lr: 0.0064 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 169/200\n",
      "7/7 - 0s - loss: 0.2861 - accuracy: 0.9062 - val_loss: 0.4039 - val_accuracy: 0.8696 - lr: 0.0062 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 170/200\n",
      "7/7 - 0s - loss: 0.2484 - accuracy: 0.9107 - val_loss: 0.4366 - val_accuracy: 0.8261 - lr: 0.0060 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 171/200\n",
      "7/7 - 0s - loss: 0.3248 - accuracy: 0.8661 - val_loss: 0.3816 - val_accuracy: 0.8696 - lr: 0.0058 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 172/200\n",
      "7/7 - 0s - loss: 0.2940 - accuracy: 0.8795 - val_loss: 0.3887 - val_accuracy: 0.8696 - lr: 0.0056 - 472ms/epoch - 67ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 173/200\n",
      "7/7 - 1s - loss: 0.2103 - accuracy: 0.9107 - val_loss: 0.4034 - val_accuracy: 0.8261 - lr: 0.0054 - 988ms/epoch - 141ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 174/200\n",
      "7/7 - 0s - loss: 0.2136 - accuracy: 0.9018 - val_loss: 0.3911 - val_accuracy: 0.8696 - lr: 0.0052 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 175/200\n",
      "7/7 - 1s - loss: 0.2461 - accuracy: 0.9018 - val_loss: 0.4539 - val_accuracy: 0.8261 - lr: 0.0050 - 576ms/epoch - 82ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 176/200\n",
      "7/7 - 1s - loss: 0.2299 - accuracy: 0.8973 - val_loss: 0.3920 - val_accuracy: 0.8261 - lr: 0.0048 - 1s/epoch - 179ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 177/200\n",
      "7/7 - 0s - loss: 0.1840 - accuracy: 0.9330 - val_loss: 0.4401 - val_accuracy: 0.8261 - lr: 0.0046 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 178/200\n",
      "7/7 - 1s - loss: 0.2081 - accuracy: 0.9330 - val_loss: 0.4642 - val_accuracy: 0.8261 - lr: 0.0044 - 504ms/epoch - 72ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 179/200\n",
      "7/7 - 0s - loss: 0.2211 - accuracy: 0.9152 - val_loss: 0.4243 - val_accuracy: 0.8696 - lr: 0.0042 - 392ms/epoch - 56ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 180/200\n",
      "7/7 - 0s - loss: 0.2155 - accuracy: 0.9062 - val_loss: 0.4013 - val_accuracy: 0.8261 - lr: 0.0040 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 181/200\n",
      "7/7 - 0s - loss: 0.2214 - accuracy: 0.9196 - val_loss: 0.3986 - val_accuracy: 0.8261 - lr: 0.0038 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 182/200\n",
      "7/7 - 0s - loss: 0.2473 - accuracy: 0.9241 - val_loss: 0.4366 - val_accuracy: 0.8696 - lr: 0.0036 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 183/200\n",
      "7/7 - 1s - loss: 0.2775 - accuracy: 0.9062 - val_loss: 0.4166 - val_accuracy: 0.8696 - lr: 0.0034 - 519ms/epoch - 74ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 184/200\n",
      "7/7 - 0s - loss: 0.1644 - accuracy: 0.9464 - val_loss: 0.3965 - val_accuracy: 0.8261 - lr: 0.0032 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 185/200\n",
      "7/7 - 0s - loss: 0.2047 - accuracy: 0.9062 - val_loss: 0.4256 - val_accuracy: 0.8696 - lr: 0.0030 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 186/200\n",
      "7/7 - 0s - loss: 0.1839 - accuracy: 0.9241 - val_loss: 0.4346 - val_accuracy: 0.8696 - lr: 0.0028 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 187/200\n",
      "7/7 - 1s - loss: 0.1887 - accuracy: 0.9330 - val_loss: 0.4367 - val_accuracy: 0.8696 - lr: 0.0026 - 977ms/epoch - 140ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 188/200\n",
      "7/7 - 0s - loss: 0.2455 - accuracy: 0.9107 - val_loss: 0.4344 - val_accuracy: 0.8696 - lr: 0.0024 - 488ms/epoch - 70ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 189/200\n",
      "7/7 - 0s - loss: 0.1979 - accuracy: 0.9241 - val_loss: 0.4292 - val_accuracy: 0.8261 - lr: 0.0022 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 190/200\n",
      "7/7 - 0s - loss: 0.1874 - accuracy: 0.9152 - val_loss: 0.4489 - val_accuracy: 0.8696 - lr: 0.0020 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 191/200\n",
      "7/7 - 0s - loss: 0.1960 - accuracy: 0.9330 - val_loss: 0.4587 - val_accuracy: 0.8696 - lr: 0.0018 - 380ms/epoch - 54ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 192/200\n",
      "7/7 - 1s - loss: 0.2156 - accuracy: 0.9152 - val_loss: 0.4466 - val_accuracy: 0.8261 - lr: 0.0016 - 1s/epoch - 188ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 193/200\n",
      "7/7 - 0s - loss: 0.2033 - accuracy: 0.9286 - val_loss: 0.4511 - val_accuracy: 0.8696 - lr: 0.0014 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 194/200\n",
      "7/7 - 1s - loss: 0.2094 - accuracy: 0.9241 - val_loss: 0.4467 - val_accuracy: 0.8261 - lr: 0.0012 - 512ms/epoch - 73ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 195/200\n",
      "7/7 - 1s - loss: 0.2069 - accuracy: 0.9107 - val_loss: 0.4434 - val_accuracy: 0.8261 - lr: 0.0010 - 564ms/epoch - 81ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 196/200\n",
      "7/7 - 1s - loss: 0.1954 - accuracy: 0.9018 - val_loss: 0.4447 - val_accuracy: 0.8696 - lr: 8.0009e-04 - 852ms/epoch - 122ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 197/200\n",
      "7/7 - 1s - loss: 0.2244 - accuracy: 0.8929 - val_loss: 0.4318 - val_accuracy: 0.8261 - lr: 6.0009e-04 - 510ms/epoch - 73ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 198/200\n",
      "7/7 - 0s - loss: 0.1685 - accuracy: 0.9286 - val_loss: 0.4346 - val_accuracy: 0.8696 - lr: 4.0010e-04 - 331ms/epoch - 47ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 199/200\n",
      "7/7 - 0s - loss: 0.2152 - accuracy: 0.9062 - val_loss: 0.4341 - val_accuracy: 0.8696 - lr: 2.0010e-04 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 200/200\n",
      "7/7 - 0s - loss: 0.2093 - accuracy: 0.9018 - val_loss: 0.4340 - val_accuracy: 0.8696 - lr: 1.0000e-07 - 337ms/epoch - 48ms/step\n",
      "\n",
      "======================BEST:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "62   63.0  0.474509  0.785714  0.276208  0.913043  0.962121  0.923077  0.931279\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       0.85      0.92      0.88        12\n",
      "\n",
      "    accuracy                           0.87        23\n",
      "   macro avg       0.87      0.87      0.87        23\n",
      "weighted avg       0.87      0.87      0.87        23\n",
      "\n",
      "[[ 9  2]\n",
      " [ 1 11]]\n",
      "ACCURACY: 0.8695652173913043\n",
      "PRECISION: 0.8461538461538461\n",
      "RECALL: 0.9166666666666666\n",
      "F1: 0.8799999999999999\n",
      "ROC_AUC(Pr.): 0.8863636363636364\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.82      0.90        11\n",
      "           1       0.86      1.00      0.92        12\n",
      "\n",
      "    accuracy                           0.91        23\n",
      "   macro avg       0.93      0.91      0.91        23\n",
      "weighted avg       0.93      0.91      0.91        23\n",
      "\n",
      "[[ 9  2]\n",
      " [ 0 12]]\n",
      "ACCURACY: 0.9130434782608695\n",
      "PRECISION: 0.8571428571428571\n",
      "RECALL: 1.0\n",
      "F1: 0.923076923076923\n",
      "ROC_AUC(Pr.): 0.962121212121212\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.82      0.90        11\n",
      "           1       0.86      1.00      0.92        12\n",
      "\n",
      "    accuracy                           0.91        23\n",
      "   macro avg       0.93      0.91      0.91        23\n",
      "weighted avg       0.93      0.91      0.91        23\n",
      "\n",
      "[[ 9  2]\n",
      " [ 0 12]]\n",
      "ACCURACY: 0.9130434782608695\n",
      "PRECISION: 0.8571428571428571\n",
      "RECALL: 1.0\n",
      "F1: 0.923076923076923\n",
      "ROC_AUC(Pr.): 0.962121212121212\n",
      "\n",
      "=====================CV[6]========================\n",
      "\n",
      "207 train samples\n",
      "23 test samples\n",
      "\n",
      "Over sampling: 207 -> 208\n",
      "Add train samples: 16\n",
      "224 new train samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\tf28\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.00000010\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_28484\\4287042063.py:159: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================EPOCH 000001:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp   f1       rfa\n",
      "0    1.0  0.692491  0.517857  0.692495  0.521739  0.651515  0.0  0.378063\n",
      "RFA : 0.37806323929266494\n",
      "7/7 - 6s - loss: 0.6925 - accuracy: 0.5179 - val_loss: 0.6925 - val_accuracy: 0.5217 - lr: 2.0010e-04 - 6s/epoch - 886ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 2/200\n",
      "======================EPOCH 000002:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "1    2.0  0.694207  0.464286  0.692429  0.478261  0.765152  0.647059  0.623407\n",
      "RFA : 0.6234073489426291\n",
      "7/7 - 0s - loss: 0.6942 - accuracy: 0.4643 - val_loss: 0.6924 - val_accuracy: 0.4783 - lr: 4.0010e-04 - 356ms/epoch - 51ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 3/200\n",
      "======================EPOCH 000003:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1      rfa\n",
      "2    3.0  0.691807  0.566964   0.69041  0.826087  0.840909  0.833333  0.83307\n",
      "RFA : 0.8330698223728121\n",
      "7/7 - 1s - loss: 0.6918 - accuracy: 0.5670 - val_loss: 0.6904 - val_accuracy: 0.8261 - lr: 6.0009e-04 - 691ms/epoch - 99ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 4/200\n",
      "7/7 - 0s - loss: 0.6873 - accuracy: 0.5938 - val_loss: 0.6827 - val_accuracy: 0.5217 - lr: 8.0009e-04 - 486ms/epoch - 69ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 5/200\n",
      "======================EPOCH 000005:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "4    5.0  0.669573  0.700893  0.647653  0.826087  0.893939  0.846154  0.853466\n",
      "RFA : 0.8534660927690825\n",
      "7/7 - 0s - loss: 0.6696 - accuracy: 0.7009 - val_loss: 0.6477 - val_accuracy: 0.8261 - lr: 0.0010 - 392ms/epoch - 56ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 6/200\n",
      "7/7 - 0s - loss: 0.5972 - accuracy: 0.7366 - val_loss: 0.5369 - val_accuracy: 0.7391 - lr: 0.0012 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 7/200\n",
      "7/7 - 0s - loss: 0.4847 - accuracy: 0.7768 - val_loss: 0.5317 - val_accuracy: 0.7391 - lr: 0.0014 - 370ms/epoch - 53ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 8/200\n",
      "7/7 - 0s - loss: 0.4494 - accuracy: 0.7991 - val_loss: 0.4121 - val_accuracy: 0.7391 - lr: 0.0016 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 9/200\n",
      "7/7 - 1s - loss: 0.4119 - accuracy: 0.8036 - val_loss: 0.3991 - val_accuracy: 0.7391 - lr: 0.0018 - 700ms/epoch - 100ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 10/200\n",
      "7/7 - 0s - loss: 0.4224 - accuracy: 0.7946 - val_loss: 0.3514 - val_accuracy: 0.7826 - lr: 0.0020 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 11/200\n",
      "======================EPOCH 000011:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp    f1       rfa\n",
      "10   11.0  0.353802  0.839286  0.339369  0.869565  0.931818  0.88  0.891893\n",
      "RFA : 0.8918932706551119\n",
      "7/7 - 1s - loss: 0.3538 - accuracy: 0.8393 - val_loss: 0.3394 - val_accuracy: 0.8696 - lr: 0.0022 - 660ms/epoch - 94ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 12/200\n",
      "======================EPOCH 000012:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "11   12.0  0.380623  0.834821  0.295761  0.956522  0.931818  0.956522  0.949111\n",
      "RFA : 0.9491106755648677\n",
      "7/7 - 1s - loss: 0.3806 - accuracy: 0.8348 - val_loss: 0.2958 - val_accuracy: 0.9565 - lr: 0.0024 - 592ms/epoch - 85ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 13/200\n",
      "7/7 - 1s - loss: 0.3740 - accuracy: 0.7991 - val_loss: 0.4254 - val_accuracy: 0.6957 - lr: 0.0026 - 547ms/epoch - 78ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 14/200\n",
      "7/7 - 1s - loss: 0.3948 - accuracy: 0.7991 - val_loss: 0.4300 - val_accuracy: 0.6957 - lr: 0.0028 - 1s/epoch - 177ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 15/200\n",
      "7/7 - 0s - loss: 0.3281 - accuracy: 0.8527 - val_loss: 0.2968 - val_accuracy: 0.8696 - lr: 0.0030 - 496ms/epoch - 71ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 16/200\n",
      "7/7 - 1s - loss: 0.3517 - accuracy: 0.8616 - val_loss: 0.3048 - val_accuracy: 0.8696 - lr: 0.0032 - 699ms/epoch - 100ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 17/200\n",
      "7/7 - 0s - loss: 0.3576 - accuracy: 0.8527 - val_loss: 0.3412 - val_accuracy: 0.7826 - lr: 0.0034 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 18/200\n",
      "======================EPOCH 000018:======================\n",
      "    epoch     loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "17   18.0  0.37281  0.808036  0.289061  0.956522  0.939394  0.956522  0.951383\n",
      "RFA : 0.9513834028375949\n",
      "7/7 - 1s - loss: 0.3728 - accuracy: 0.8080 - val_loss: 0.2891 - val_accuracy: 0.9565 - lr: 0.0036 - 1s/epoch - 177ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 19/200\n",
      "7/7 - 0s - loss: 0.3586 - accuracy: 0.8214 - val_loss: 0.2760 - val_accuracy: 0.9565 - lr: 0.0038 - 343ms/epoch - 49ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 20/200\n",
      "7/7 - 0s - loss: 0.3280 - accuracy: 0.8616 - val_loss: 0.2673 - val_accuracy: 0.8261 - lr: 0.0040 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 21/200\n",
      "7/7 - 1s - loss: 0.3390 - accuracy: 0.8527 - val_loss: 0.2369 - val_accuracy: 0.9565 - lr: 0.0042 - 1s/epoch - 211ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 22/200\n",
      "7/7 - 0s - loss: 0.3231 - accuracy: 0.8884 - val_loss: 0.2667 - val_accuracy: 0.8696 - lr: 0.0044 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 23/200\n",
      "7/7 - 0s - loss: 0.3088 - accuracy: 0.8616 - val_loss: 0.2987 - val_accuracy: 0.8696 - lr: 0.0046 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 24/200\n",
      "7/7 - 1s - loss: 0.3304 - accuracy: 0.8661 - val_loss: 0.3345 - val_accuracy: 0.8696 - lr: 0.0048 - 516ms/epoch - 74ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 25/200\n",
      "7/7 - 0s - loss: 0.3325 - accuracy: 0.8616 - val_loss: 0.2406 - val_accuracy: 0.9565 - lr: 0.0050 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 26/200\n",
      "7/7 - 1s - loss: 0.2901 - accuracy: 0.8705 - val_loss: 0.2826 - val_accuracy: 0.8696 - lr: 0.0052 - 1s/epoch - 191ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 27/200\n",
      "7/7 - 0s - loss: 0.4068 - accuracy: 0.8348 - val_loss: 0.4552 - val_accuracy: 0.7391 - lr: 0.0054 - 486ms/epoch - 69ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 28/200\n",
      "7/7 - 0s - loss: 0.3923 - accuracy: 0.8214 - val_loss: 0.2976 - val_accuracy: 0.9130 - lr: 0.0056 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 29/200\n",
      "7/7 - 1s - loss: 0.3443 - accuracy: 0.8571 - val_loss: 0.2678 - val_accuracy: 0.8696 - lr: 0.0058 - 614ms/epoch - 88ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 30/200\n",
      "7/7 - 0s - loss: 0.3701 - accuracy: 0.8527 - val_loss: 0.2753 - val_accuracy: 0.9130 - lr: 0.0060 - 446ms/epoch - 64ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 31/200\n",
      "7/7 - 0s - loss: 0.3827 - accuracy: 0.8259 - val_loss: 0.2635 - val_accuracy: 0.9565 - lr: 0.0062 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 32/200\n",
      "7/7 - 0s - loss: 0.3482 - accuracy: 0.8348 - val_loss: 0.2327 - val_accuracy: 0.9565 - lr: 0.0064 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 33/200\n",
      "7/7 - 0s - loss: 0.3512 - accuracy: 0.8661 - val_loss: 0.3610 - val_accuracy: 0.7826 - lr: 0.0066 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 34/200\n",
      "7/7 - 0s - loss: 0.3108 - accuracy: 0.8348 - val_loss: 0.2590 - val_accuracy: 0.8696 - lr: 0.0068 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 35/200\n",
      "7/7 - 1s - loss: 0.2985 - accuracy: 0.8795 - val_loss: 0.3831 - val_accuracy: 0.7391 - lr: 0.0070 - 526ms/epoch - 75ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 36/200\n",
      "7/7 - 1s - loss: 0.3001 - accuracy: 0.8795 - val_loss: 0.3334 - val_accuracy: 0.8261 - lr: 0.0072 - 923ms/epoch - 132ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 37/200\n",
      "7/7 - 0s - loss: 0.2880 - accuracy: 0.8884 - val_loss: 0.2799 - val_accuracy: 0.8696 - lr: 0.0074 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 38/200\n",
      "7/7 - 0s - loss: 0.2878 - accuracy: 0.8438 - val_loss: 0.2232 - val_accuracy: 0.9130 - lr: 0.0076 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 39/200\n",
      "7/7 - 0s - loss: 0.2497 - accuracy: 0.8973 - val_loss: 0.3300 - val_accuracy: 0.8696 - lr: 0.0078 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 40/200\n",
      "======================EPOCH 000040:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "39   40.0  0.328337  0.825893  0.237694  0.956522  0.954545  0.956522  0.955929\n",
      "RFA : 0.9559288573830494\n",
      "7/7 - 1s - loss: 0.3283 - accuracy: 0.8259 - val_loss: 0.2377 - val_accuracy: 0.9565 - lr: 0.0080 - 577ms/epoch - 82ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 41/200\n",
      "7/7 - 0s - loss: 0.2874 - accuracy: 0.8616 - val_loss: 0.2312 - val_accuracy: 0.9565 - lr: 0.0082 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 42/200\n",
      "7/7 - 1s - loss: 0.2817 - accuracy: 0.8839 - val_loss: 0.2318 - val_accuracy: 0.9565 - lr: 0.0084 - 1s/epoch - 153ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 43/200\n",
      "7/7 - 0s - loss: 0.4079 - accuracy: 0.7902 - val_loss: 0.7320 - val_accuracy: 0.6087 - lr: 0.0086 - 472ms/epoch - 67ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 44/200\n",
      "7/7 - 0s - loss: 0.4556 - accuracy: 0.7857 - val_loss: 0.2631 - val_accuracy: 0.9565 - lr: 0.0088 - 323ms/epoch - 46ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 45/200\n",
      "7/7 - 0s - loss: 0.4436 - accuracy: 0.8125 - val_loss: 0.3537 - val_accuracy: 0.8696 - lr: 0.0090 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 46/200\n",
      "7/7 - 0s - loss: 0.4604 - accuracy: 0.8214 - val_loss: 0.7760 - val_accuracy: 0.7391 - lr: 0.0092 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 47/200\n",
      "7/7 - 1s - loss: 2.3306 - accuracy: 0.7277 - val_loss: 4.0808 - val_accuracy: 0.6522 - lr: 0.0094 - 525ms/epoch - 75ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 48/200\n",
      "7/7 - 1s - loss: 65.0598 - accuracy: 0.4643 - val_loss: 2525.8821 - val_accuracy: 0.4783 - lr: 0.0096 - 660ms/epoch - 94ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 49/200\n",
      "7/7 - 0s - loss: 1017.8788 - accuracy: 0.4911 - val_loss: 29.3570 - val_accuracy: 0.4783 - lr: 0.0098 - 358ms/epoch - 51ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 50/200\n",
      "7/7 - 0s - loss: 387.6923 - accuracy: 0.5179 - val_loss: 54.5370 - val_accuracy: 0.4783 - lr: 0.0100 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 51/200\n",
      "7/7 - 1s - loss: 22.2667 - accuracy: 0.5134 - val_loss: 22.2622 - val_accuracy: 0.5217 - lr: 0.0098 - 524ms/epoch - 75ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 52/200\n",
      "7/7 - 0s - loss: 4.7119 - accuracy: 0.4420 - val_loss: 0.7171 - val_accuracy: 0.4783 - lr: 0.0096 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 53/200\n",
      "7/7 - 0s - loss: 1.0926 - accuracy: 0.5446 - val_loss: 0.6462 - val_accuracy: 0.5652 - lr: 0.0094 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 54/200\n",
      "7/7 - 0s - loss: 0.9362 - accuracy: 0.5223 - val_loss: 0.6928 - val_accuracy: 0.4783 - lr: 0.0092 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 55/200\n",
      "7/7 - 0s - loss: 0.9147 - accuracy: 0.5223 - val_loss: 0.6024 - val_accuracy: 0.6087 - lr: 0.0090 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 56/200\n",
      "7/7 - 1s - loss: 0.7811 - accuracy: 0.5446 - val_loss: 0.6977 - val_accuracy: 0.5652 - lr: 0.0088 - 526ms/epoch - 75ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 57/200\n",
      "7/7 - 0s - loss: 0.6835 - accuracy: 0.6473 - val_loss: 0.5258 - val_accuracy: 0.6957 - lr: 0.0086 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 58/200\n",
      "7/7 - 0s - loss: 0.5858 - accuracy: 0.7054 - val_loss: 0.4226 - val_accuracy: 0.7391 - lr: 0.0084 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 59/200\n",
      "7/7 - 0s - loss: 0.4960 - accuracy: 0.7411 - val_loss: 0.3596 - val_accuracy: 0.8261 - lr: 0.0082 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 60/200\n",
      "7/7 - 0s - loss: 0.4666 - accuracy: 0.7946 - val_loss: 0.2986 - val_accuracy: 0.9130 - lr: 0.0080 - 318ms/epoch - 45ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 61/200\n",
      "7/7 - 1s - loss: 0.4753 - accuracy: 0.7545 - val_loss: 0.3359 - val_accuracy: 0.8696 - lr: 0.0078 - 524ms/epoch - 75ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 62/200\n",
      "7/7 - 0s - loss: 0.4461 - accuracy: 0.7723 - val_loss: 0.3417 - val_accuracy: 0.9130 - lr: 0.0076 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 63/200\n",
      "7/7 - 1s - loss: 0.4492 - accuracy: 0.8036 - val_loss: 0.4437 - val_accuracy: 0.6957 - lr: 0.0074 - 1s/epoch - 199ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 64/200\n",
      "7/7 - 0s - loss: 0.3690 - accuracy: 0.8214 - val_loss: 0.6471 - val_accuracy: 0.6087 - lr: 0.0072 - 487ms/epoch - 70ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 65/200\n",
      "7/7 - 0s - loss: 0.4446 - accuracy: 0.8036 - val_loss: 0.3848 - val_accuracy: 0.7826 - lr: 0.0070 - 323ms/epoch - 46ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 66/200\n",
      "7/7 - 0s - loss: 0.3896 - accuracy: 0.8259 - val_loss: 0.3844 - val_accuracy: 0.7826 - lr: 0.0068 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 67/200\n",
      "7/7 - 1s - loss: 0.4082 - accuracy: 0.8214 - val_loss: 0.7113 - val_accuracy: 0.6522 - lr: 0.0066 - 953ms/epoch - 136ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 68/200\n",
      "7/7 - 0s - loss: 0.4989 - accuracy: 0.7812 - val_loss: 0.4332 - val_accuracy: 0.6522 - lr: 0.0064 - 472ms/epoch - 67ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 69/200\n",
      "7/7 - 1s - loss: 0.3573 - accuracy: 0.8259 - val_loss: 0.3017 - val_accuracy: 0.8696 - lr: 0.0062 - 946ms/epoch - 135ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 70/200\n",
      "7/7 - 0s - loss: 0.4213 - accuracy: 0.8125 - val_loss: 0.2977 - val_accuracy: 0.8696 - lr: 0.0060 - 488ms/epoch - 70ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 71/200\n",
      "7/7 - 0s - loss: 0.3811 - accuracy: 0.8080 - val_loss: 0.3116 - val_accuracy: 0.8261 - lr: 0.0058 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 72/200\n",
      "7/7 - 0s - loss: 0.3593 - accuracy: 0.8482 - val_loss: 0.2900 - val_accuracy: 0.8696 - lr: 0.0056 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 73/200\n",
      "7/7 - 0s - loss: 0.3956 - accuracy: 0.8080 - val_loss: 0.2887 - val_accuracy: 0.9130 - lr: 0.0054 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 74/200\n",
      "7/7 - 1s - loss: 0.3890 - accuracy: 0.8214 - val_loss: 0.2835 - val_accuracy: 0.9565 - lr: 0.0052 - 1s/epoch - 200ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 75/200\n",
      "7/7 - 0s - loss: 0.3909 - accuracy: 0.8170 - val_loss: 0.3066 - val_accuracy: 0.9130 - lr: 0.0050 - 486ms/epoch - 69ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 76/200\n",
      "7/7 - 0s - loss: 0.3252 - accuracy: 0.8571 - val_loss: 0.4400 - val_accuracy: 0.7391 - lr: 0.0048 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 77/200\n",
      "7/7 - 1s - loss: 0.3707 - accuracy: 0.8438 - val_loss: 0.3095 - val_accuracy: 0.8696 - lr: 0.0046 - 647ms/epoch - 92ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 78/200\n",
      "7/7 - 0s - loss: 0.3380 - accuracy: 0.8616 - val_loss: 0.3188 - val_accuracy: 0.8696 - lr: 0.0044 - 344ms/epoch - 49ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 79/200\n",
      "7/7 - 0s - loss: 0.3870 - accuracy: 0.8036 - val_loss: 0.2895 - val_accuracy: 0.9130 - lr: 0.0042 - 351ms/epoch - 50ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 80/200\n",
      "7/7 - 1s - loss: 0.3626 - accuracy: 0.8080 - val_loss: 0.2936 - val_accuracy: 0.9130 - lr: 0.0040 - 747ms/epoch - 107ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 81/200\n",
      "7/7 - 0s - loss: 0.3497 - accuracy: 0.8482 - val_loss: 0.3396 - val_accuracy: 0.8696 - lr: 0.0038 - 493ms/epoch - 70ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 82/200\n",
      "7/7 - 0s - loss: 0.3743 - accuracy: 0.8214 - val_loss: 0.4699 - val_accuracy: 0.7391 - lr: 0.0036 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 83/200\n",
      "7/7 - 0s - loss: 0.3459 - accuracy: 0.8348 - val_loss: 0.3094 - val_accuracy: 0.8696 - lr: 0.0034 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 84/200\n",
      "7/7 - 0s - loss: 0.3296 - accuracy: 0.8438 - val_loss: 0.3027 - val_accuracy: 0.8696 - lr: 0.0032 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 85/200\n",
      "7/7 - 1s - loss: 0.3079 - accuracy: 0.8705 - val_loss: 0.3250 - val_accuracy: 0.8696 - lr: 0.0030 - 517ms/epoch - 74ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 86/200\n",
      "7/7 - 1s - loss: 0.3291 - accuracy: 0.8125 - val_loss: 0.2773 - val_accuracy: 0.9130 - lr: 0.0028 - 787ms/epoch - 112ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 87/200\n",
      "7/7 - 0s - loss: 0.4207 - accuracy: 0.8259 - val_loss: 0.3273 - val_accuracy: 0.8696 - lr: 0.0026 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 88/200\n",
      "7/7 - 0s - loss: 0.3397 - accuracy: 0.8214 - val_loss: 0.3848 - val_accuracy: 0.7826 - lr: 0.0024 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 89/200\n",
      "7/7 - 0s - loss: 0.4328 - accuracy: 0.8170 - val_loss: 0.2769 - val_accuracy: 0.9565 - lr: 0.0022 - 449ms/epoch - 64ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 90/200\n",
      "7/7 - 1s - loss: 0.3589 - accuracy: 0.8482 - val_loss: 0.3110 - val_accuracy: 0.8696 - lr: 0.0020 - 1s/epoch - 165ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 91/200\n",
      "7/7 - 1s - loss: 0.3094 - accuracy: 0.8616 - val_loss: 0.3077 - val_accuracy: 0.8696 - lr: 0.0018 - 1s/epoch - 155ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 92/200\n",
      "7/7 - 0s - loss: 0.3747 - accuracy: 0.8393 - val_loss: 0.2853 - val_accuracy: 0.8696 - lr: 0.0016 - 496ms/epoch - 71ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 93/200\n",
      "7/7 - 0s - loss: 0.3159 - accuracy: 0.8482 - val_loss: 0.2887 - val_accuracy: 0.8696 - lr: 0.0014 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 94/200\n",
      "7/7 - 0s - loss: 0.3540 - accuracy: 0.8348 - val_loss: 0.2995 - val_accuracy: 0.8696 - lr: 0.0012 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 95/200\n",
      "7/7 - 0s - loss: 0.3432 - accuracy: 0.8304 - val_loss: 0.3280 - val_accuracy: 0.8696 - lr: 0.0010 - 438ms/epoch - 63ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 96/200\n",
      "7/7 - 1s - loss: 0.3416 - accuracy: 0.8661 - val_loss: 0.3005 - val_accuracy: 0.8696 - lr: 8.0009e-04 - 1s/epoch - 207ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 97/200\n",
      "7/7 - 1s - loss: 0.3273 - accuracy: 0.8705 - val_loss: 0.2783 - val_accuracy: 0.9130 - lr: 6.0009e-04 - 501ms/epoch - 72ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 98/200\n",
      "7/7 - 1s - loss: 0.3350 - accuracy: 0.8571 - val_loss: 0.2896 - val_accuracy: 0.8696 - lr: 4.0010e-04 - 624ms/epoch - 89ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 99/200\n",
      "7/7 - 1s - loss: 0.3771 - accuracy: 0.8482 - val_loss: 0.2832 - val_accuracy: 0.8696 - lr: 2.0010e-04 - 629ms/epoch - 90ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 100/200\n",
      "7/7 - 0s - loss: 0.3068 - accuracy: 0.8795 - val_loss: 0.2887 - val_accuracy: 0.8696 - lr: 1.0000e-07 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00000010\n",
      "Epoch 101/200\n",
      "7/7 - 2s - loss: 0.3192 - accuracy: 0.8795 - val_loss: 0.2902 - val_accuracy: 0.8696 - lr: 2.0010e-04 - 2s/epoch - 238ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 102/200\n",
      "7/7 - 1s - loss: 0.3407 - accuracy: 0.8482 - val_loss: 0.3029 - val_accuracy: 0.8696 - lr: 4.0010e-04 - 1s/epoch - 179ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 103/200\n",
      "7/7 - 0s - loss: 0.3808 - accuracy: 0.8527 - val_loss: 0.2831 - val_accuracy: 0.8696 - lr: 6.0009e-04 - 486ms/epoch - 69ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 104/200\n",
      "7/7 - 0s - loss: 0.3250 - accuracy: 0.8527 - val_loss: 0.2960 - val_accuracy: 0.8696 - lr: 8.0009e-04 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 105/200\n",
      "7/7 - 1s - loss: 0.3355 - accuracy: 0.8527 - val_loss: 0.2879 - val_accuracy: 0.8696 - lr: 0.0010 - 1s/epoch - 175ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 106/200\n",
      "7/7 - 0s - loss: 0.3192 - accuracy: 0.8527 - val_loss: 0.3121 - val_accuracy: 0.8696 - lr: 0.0012 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 107/200\n",
      "7/7 - 0s - loss: 0.2753 - accuracy: 0.9018 - val_loss: 0.2773 - val_accuracy: 0.8696 - lr: 0.0014 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 108/200\n",
      "7/7 - 0s - loss: 0.3389 - accuracy: 0.8571 - val_loss: 0.2637 - val_accuracy: 0.9130 - lr: 0.0016 - 423ms/epoch - 60ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 109/200\n",
      "7/7 - 1s - loss: 0.2985 - accuracy: 0.8705 - val_loss: 0.2860 - val_accuracy: 0.8696 - lr: 0.0018 - 614ms/epoch - 88ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 110/200\n",
      "7/7 - 0s - loss: 0.3210 - accuracy: 0.8661 - val_loss: 0.2641 - val_accuracy: 0.9130 - lr: 0.0020 - 341ms/epoch - 49ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 111/200\n",
      "7/7 - 0s - loss: 0.3722 - accuracy: 0.8214 - val_loss: 0.2901 - val_accuracy: 0.8696 - lr: 0.0022 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 112/200\n",
      "7/7 - 0s - loss: 0.3316 - accuracy: 0.8795 - val_loss: 0.2955 - val_accuracy: 0.8696 - lr: 0.0024 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 113/200\n",
      "7/7 - 0s - loss: 0.3213 - accuracy: 0.8616 - val_loss: 0.3044 - val_accuracy: 0.8696 - lr: 0.0026 - 487ms/epoch - 70ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 114/200\n",
      "7/7 - 0s - loss: 0.3080 - accuracy: 0.8482 - val_loss: 0.3135 - val_accuracy: 0.8696 - lr: 0.0028 - 323ms/epoch - 46ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 115/200\n",
      "7/7 - 1s - loss: 0.2633 - accuracy: 0.8884 - val_loss: 0.2611 - val_accuracy: 0.9565 - lr: 0.0030 - 1s/epoch - 192ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 116/200\n",
      "7/7 - 0s - loss: 0.3184 - accuracy: 0.8571 - val_loss: 0.3299 - val_accuracy: 0.8696 - lr: 0.0032 - 479ms/epoch - 68ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 117/200\n",
      "7/7 - 1s - loss: 0.3572 - accuracy: 0.8348 - val_loss: 0.4471 - val_accuracy: 0.7826 - lr: 0.0034 - 1s/epoch - 175ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 118/200\n",
      "7/7 - 0s - loss: 0.2845 - accuracy: 0.8839 - val_loss: 0.2661 - val_accuracy: 0.9130 - lr: 0.0036 - 488ms/epoch - 70ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 119/200\n",
      "7/7 - 0s - loss: 0.3140 - accuracy: 0.8571 - val_loss: 0.2922 - val_accuracy: 0.8696 - lr: 0.0038 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 120/200\n",
      "7/7 - 0s - loss: 0.2978 - accuracy: 0.8616 - val_loss: 0.4503 - val_accuracy: 0.7391 - lr: 0.0040 - 342ms/epoch - 49ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 121/200\n",
      "7/7 - 0s - loss: 0.3244 - accuracy: 0.8705 - val_loss: 0.4917 - val_accuracy: 0.7391 - lr: 0.0042 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 122/200\n",
      "7/7 - 0s - loss: 0.3669 - accuracy: 0.8259 - val_loss: 0.4086 - val_accuracy: 0.7826 - lr: 0.0044 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 123/200\n",
      "7/7 - 1s - loss: 0.4423 - accuracy: 0.8259 - val_loss: 0.2638 - val_accuracy: 0.9565 - lr: 0.0046 - 862ms/epoch - 123ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 124/200\n",
      "7/7 - 1s - loss: 0.3302 - accuracy: 0.8661 - val_loss: 0.3757 - val_accuracy: 0.8261 - lr: 0.0048 - 837ms/epoch - 120ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 125/200\n",
      "7/7 - 0s - loss: 0.3152 - accuracy: 0.8482 - val_loss: 0.2897 - val_accuracy: 0.8696 - lr: 0.0050 - 490ms/epoch - 70ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 126/200\n",
      "7/7 - 0s - loss: 0.3257 - accuracy: 0.8661 - val_loss: 0.3208 - val_accuracy: 0.8696 - lr: 0.0052 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 127/200\n",
      "7/7 - 0s - loss: 0.2766 - accuracy: 0.8839 - val_loss: 0.3816 - val_accuracy: 0.8261 - lr: 0.0054 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 128/200\n",
      "7/7 - 0s - loss: 0.3660 - accuracy: 0.8482 - val_loss: 0.6028 - val_accuracy: 0.6522 - lr: 0.0056 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 129/200\n",
      "7/7 - 0s - loss: 0.4320 - accuracy: 0.8036 - val_loss: 0.3194 - val_accuracy: 0.8696 - lr: 0.0058 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 130/200\n",
      "7/7 - 0s - loss: 0.3141 - accuracy: 0.8795 - val_loss: 0.2615 - val_accuracy: 0.9130 - lr: 0.0060 - 498ms/epoch - 71ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 131/200\n",
      "7/7 - 2s - loss: 0.3106 - accuracy: 0.8750 - val_loss: 0.2699 - val_accuracy: 0.9565 - lr: 0.0062 - 2s/epoch - 234ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 132/200\n",
      "7/7 - 0s - loss: 0.3815 - accuracy: 0.7991 - val_loss: 0.2702 - val_accuracy: 0.9130 - lr: 0.0064 - 346ms/epoch - 49ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 133/200\n",
      "7/7 - 0s - loss: 0.3111 - accuracy: 0.8661 - val_loss: 0.2784 - val_accuracy: 0.9565 - lr: 0.0066 - 351ms/epoch - 50ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 134/200\n",
      "7/7 - 2s - loss: 0.3774 - accuracy: 0.8348 - val_loss: 0.2589 - val_accuracy: 0.9565 - lr: 0.0068 - 2s/epoch - 224ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 135/200\n",
      "7/7 - 0s - loss: 0.2970 - accuracy: 0.8929 - val_loss: 0.2600 - val_accuracy: 0.9130 - lr: 0.0070 - 483ms/epoch - 69ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 136/200\n",
      "7/7 - 0s - loss: 0.2971 - accuracy: 0.8750 - val_loss: 0.2659 - val_accuracy: 0.9130 - lr: 0.0072 - 313ms/epoch - 45ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 137/200\n",
      "7/7 - 0s - loss: 0.3908 - accuracy: 0.8036 - val_loss: 0.3023 - val_accuracy: 0.8696 - lr: 0.0074 - 343ms/epoch - 49ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 138/200\n",
      "7/7 - 0s - loss: 0.3147 - accuracy: 0.8571 - val_loss: 0.3599 - val_accuracy: 0.8261 - lr: 0.0076 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 139/200\n",
      "7/7 - 1s - loss: 0.3622 - accuracy: 0.8571 - val_loss: 0.3030 - val_accuracy: 0.8696 - lr: 0.0078 - 715ms/epoch - 102ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 140/200\n",
      "7/7 - 0s - loss: 0.3059 - accuracy: 0.8527 - val_loss: 0.4414 - val_accuracy: 0.6957 - lr: 0.0080 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 141/200\n",
      "7/7 - 1s - loss: 0.2870 - accuracy: 0.8750 - val_loss: 0.2568 - val_accuracy: 0.9565 - lr: 0.0082 - 581ms/epoch - 83ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 142/200\n",
      "7/7 - 0s - loss: 0.3292 - accuracy: 0.8616 - val_loss: 0.4035 - val_accuracy: 0.7391 - lr: 0.0084 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 143/200\n",
      "7/7 - 1s - loss: 0.3612 - accuracy: 0.8348 - val_loss: 0.3402 - val_accuracy: 0.8696 - lr: 0.0086 - 1s/epoch - 156ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 144/200\n",
      "7/7 - 2s - loss: 0.2699 - accuracy: 0.8973 - val_loss: 0.3059 - val_accuracy: 0.8696 - lr: 0.0088 - 2s/epoch - 235ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 145/200\n",
      "7/7 - 0s - loss: 0.2893 - accuracy: 0.8616 - val_loss: 0.3156 - val_accuracy: 0.8696 - lr: 0.0090 - 490ms/epoch - 70ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 146/200\n",
      "7/7 - 0s - loss: 0.3198 - accuracy: 0.8482 - val_loss: 0.3016 - val_accuracy: 0.9130 - lr: 0.0092 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 147/200\n",
      "7/7 - 1s - loss: 0.3588 - accuracy: 0.8393 - val_loss: 0.2698 - val_accuracy: 0.9565 - lr: 0.0094 - 1s/epoch - 204ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 148/200\n",
      "7/7 - 0s - loss: 0.3899 - accuracy: 0.8036 - val_loss: 0.3206 - val_accuracy: 0.8696 - lr: 0.0096 - 354ms/epoch - 51ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 149/200\n",
      "7/7 - 0s - loss: 0.4012 - accuracy: 0.8080 - val_loss: 0.3087 - val_accuracy: 0.8261 - lr: 0.0098 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 150/200\n",
      "7/7 - 0s - loss: 0.3857 - accuracy: 0.8527 - val_loss: 0.3343 - val_accuracy: 0.8261 - lr: 0.0100 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 151/200\n",
      "7/7 - 1s - loss: 0.3306 - accuracy: 0.8170 - val_loss: 0.2937 - val_accuracy: 0.8261 - lr: 0.0098 - 714ms/epoch - 102ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 152/200\n",
      "7/7 - 1s - loss: 0.3045 - accuracy: 0.8438 - val_loss: 0.4630 - val_accuracy: 0.7391 - lr: 0.0096 - 567ms/epoch - 81ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 153/200\n",
      "7/7 - 0s - loss: 0.4629 - accuracy: 0.7812 - val_loss: 0.4606 - val_accuracy: 0.6957 - lr: 0.0094 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 154/200\n",
      "7/7 - 0s - loss: 0.3716 - accuracy: 0.8571 - val_loss: 0.2770 - val_accuracy: 0.8696 - lr: 0.0092 - 496ms/epoch - 71ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 155/200\n",
      "7/7 - 2s - loss: 0.2969 - accuracy: 0.8839 - val_loss: 0.2989 - val_accuracy: 0.8696 - lr: 0.0090 - 2s/epoch - 257ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 156/200\n",
      "7/7 - 1s - loss: 0.3297 - accuracy: 0.8482 - val_loss: 0.2554 - val_accuracy: 0.9565 - lr: 0.0088 - 502ms/epoch - 72ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 157/200\n",
      "7/7 - 0s - loss: 0.3001 - accuracy: 0.8929 - val_loss: 0.2605 - val_accuracy: 0.8696 - lr: 0.0086 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 158/200\n",
      "7/7 - 0s - loss: 0.2922 - accuracy: 0.8929 - val_loss: 0.4481 - val_accuracy: 0.7391 - lr: 0.0084 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 159/200\n",
      "7/7 - 0s - loss: 0.3141 - accuracy: 0.8571 - val_loss: 0.2941 - val_accuracy: 0.8696 - lr: 0.0082 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 160/200\n",
      "7/7 - 0s - loss: 0.3284 - accuracy: 0.8438 - val_loss: 0.2507 - val_accuracy: 0.9565 - lr: 0.0080 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 161/200\n",
      "7/7 - 1s - loss: 0.2603 - accuracy: 0.8929 - val_loss: 0.2508 - val_accuracy: 0.9565 - lr: 0.0078 - 1s/epoch - 180ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 162/200\n",
      "7/7 - 0s - loss: 0.2703 - accuracy: 0.8661 - val_loss: 0.2860 - val_accuracy: 0.8696 - lr: 0.0076 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 163/200\n",
      "7/7 - 2s - loss: 0.2613 - accuracy: 0.9018 - val_loss: 0.3090 - val_accuracy: 0.8696 - lr: 0.0074 - 2s/epoch - 246ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 164/200\n",
      "7/7 - 1s - loss: 0.2861 - accuracy: 0.9018 - val_loss: 0.2845 - val_accuracy: 0.8696 - lr: 0.0072 - 1s/epoch - 190ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 165/200\n",
      "7/7 - 0s - loss: 0.3021 - accuracy: 0.8839 - val_loss: 0.3202 - val_accuracy: 0.8696 - lr: 0.0070 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 166/200\n",
      "7/7 - 0s - loss: 0.2432 - accuracy: 0.9018 - val_loss: 0.2563 - val_accuracy: 0.9565 - lr: 0.0068 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 167/200\n",
      "7/7 - 0s - loss: 0.3179 - accuracy: 0.8571 - val_loss: 0.2373 - val_accuracy: 0.9565 - lr: 0.0066 - 434ms/epoch - 62ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 168/200\n",
      "7/7 - 1s - loss: 0.3615 - accuracy: 0.8438 - val_loss: 0.4565 - val_accuracy: 0.7391 - lr: 0.0064 - 1s/epoch - 149ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 169/200\n",
      "7/7 - 1s - loss: 0.3232 - accuracy: 0.8616 - val_loss: 0.2625 - val_accuracy: 0.9130 - lr: 0.0062 - 1s/epoch - 191ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 170/200\n",
      "7/7 - 0s - loss: 0.3282 - accuracy: 0.8571 - val_loss: 0.2598 - val_accuracy: 0.9565 - lr: 0.0060 - 463ms/epoch - 66ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 171/200\n",
      "7/7 - 0s - loss: 0.3214 - accuracy: 0.8750 - val_loss: 0.2559 - val_accuracy: 0.9565 - lr: 0.0058 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 172/200\n",
      "7/7 - 0s - loss: 0.2993 - accuracy: 0.8839 - val_loss: 0.2448 - val_accuracy: 0.9130 - lr: 0.0056 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 173/200\n",
      "7/7 - 0s - loss: 0.2916 - accuracy: 0.8750 - val_loss: 0.2394 - val_accuracy: 0.9130 - lr: 0.0054 - 343ms/epoch - 49ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 174/200\n",
      "7/7 - 1s - loss: 0.2817 - accuracy: 0.9018 - val_loss: 0.2869 - val_accuracy: 0.8696 - lr: 0.0052 - 893ms/epoch - 128ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 175/200\n",
      "7/7 - 0s - loss: 0.2837 - accuracy: 0.9018 - val_loss: 0.2729 - val_accuracy: 0.8696 - lr: 0.0050 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 176/200\n",
      "7/7 - 0s - loss: 0.3039 - accuracy: 0.9018 - val_loss: 0.2583 - val_accuracy: 0.8696 - lr: 0.0048 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 177/200\n",
      "7/7 - 0s - loss: 0.2376 - accuracy: 0.9152 - val_loss: 0.2914 - val_accuracy: 0.8696 - lr: 0.0046 - 496ms/epoch - 71ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 178/200\n",
      "7/7 - 1s - loss: 0.2850 - accuracy: 0.8795 - val_loss: 0.2655 - val_accuracy: 0.8696 - lr: 0.0044 - 566ms/epoch - 81ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 179/200\n",
      "7/7 - 0s - loss: 0.2768 - accuracy: 0.8839 - val_loss: 0.3469 - val_accuracy: 0.8696 - lr: 0.0042 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 180/200\n",
      "7/7 - 0s - loss: 0.3202 - accuracy: 0.8839 - val_loss: 0.4056 - val_accuracy: 0.8261 - lr: 0.0040 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 181/200\n",
      "7/7 - 1s - loss: 0.3016 - accuracy: 0.8705 - val_loss: 0.2298 - val_accuracy: 0.9565 - lr: 0.0038 - 851ms/epoch - 122ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 182/200\n",
      "7/7 - 0s - loss: 0.3328 - accuracy: 0.8839 - val_loss: 0.2441 - val_accuracy: 0.9130 - lr: 0.0036 - 355ms/epoch - 51ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 183/200\n",
      "7/7 - 0s - loss: 0.2655 - accuracy: 0.9018 - val_loss: 0.2537 - val_accuracy: 0.9130 - lr: 0.0034 - 331ms/epoch - 47ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 184/200\n",
      "7/7 - 0s - loss: 0.3178 - accuracy: 0.8616 - val_loss: 0.3596 - val_accuracy: 0.8696 - lr: 0.0032 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 185/200\n",
      "7/7 - 0s - loss: 0.2788 - accuracy: 0.8839 - val_loss: 0.2323 - val_accuracy: 0.9565 - lr: 0.0030 - 448ms/epoch - 64ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 186/200\n",
      "7/7 - 1s - loss: 0.2519 - accuracy: 0.9241 - val_loss: 0.2689 - val_accuracy: 0.8696 - lr: 0.0028 - 636ms/epoch - 91ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 187/200\n",
      "7/7 - 0s - loss: 0.2809 - accuracy: 0.8884 - val_loss: 0.2837 - val_accuracy: 0.8696 - lr: 0.0026 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 188/200\n",
      "7/7 - 0s - loss: 0.2709 - accuracy: 0.8929 - val_loss: 0.2417 - val_accuracy: 0.9130 - lr: 0.0024 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 189/200\n",
      "7/7 - 0s - loss: 0.2803 - accuracy: 0.9062 - val_loss: 0.2787 - val_accuracy: 0.8696 - lr: 0.0022 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 190/200\n",
      "7/7 - 1s - loss: 0.2948 - accuracy: 0.8884 - val_loss: 0.2594 - val_accuracy: 0.8696 - lr: 0.0020 - 1s/epoch - 145ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 191/200\n",
      "7/7 - 0s - loss: 0.2415 - accuracy: 0.9018 - val_loss: 0.2722 - val_accuracy: 0.8696 - lr: 0.0018 - 417ms/epoch - 60ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 192/200\n",
      "7/7 - 0s - loss: 0.2743 - accuracy: 0.8839 - val_loss: 0.2605 - val_accuracy: 0.8696 - lr: 0.0016 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 193/200\n",
      "7/7 - 0s - loss: 0.2809 - accuracy: 0.9062 - val_loss: 0.2580 - val_accuracy: 0.8696 - lr: 0.0014 - 403ms/epoch - 58ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 194/200\n",
      "7/7 - 2s - loss: 0.2795 - accuracy: 0.8973 - val_loss: 0.2709 - val_accuracy: 0.8696 - lr: 0.0012 - 2s/epoch - 231ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 195/200\n",
      "7/7 - 0s - loss: 0.2565 - accuracy: 0.8839 - val_loss: 0.2617 - val_accuracy: 0.8696 - lr: 0.0010 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 196/200\n",
      "7/7 - 0s - loss: 0.2945 - accuracy: 0.8973 - val_loss: 0.2786 - val_accuracy: 0.8696 - lr: 8.0009e-04 - 343ms/epoch - 49ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 197/200\n",
      "7/7 - 0s - loss: 0.2366 - accuracy: 0.9152 - val_loss: 0.2678 - val_accuracy: 0.8696 - lr: 6.0009e-04 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 198/200\n",
      "7/7 - 0s - loss: 0.2764 - accuracy: 0.9107 - val_loss: 0.2467 - val_accuracy: 0.9130 - lr: 4.0010e-04 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 199/200\n",
      "7/7 - 0s - loss: 0.2635 - accuracy: 0.9107 - val_loss: 0.2454 - val_accuracy: 0.9130 - lr: 2.0010e-04 - 341ms/epoch - 49ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 200/200\n",
      "7/7 - 0s - loss: 0.2654 - accuracy: 0.9152 - val_loss: 0.2513 - val_accuracy: 0.9130 - lr: 1.0000e-07 - 469ms/epoch - 67ms/step\n",
      "\n",
      "======================BEST:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "39   40.0  0.328337  0.825893  0.237694  0.956522  0.954545  0.956522  0.955929\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92        12\n",
      "           1       0.91      0.91      0.91        11\n",
      "\n",
      "    accuracy                           0.91        23\n",
      "   macro avg       0.91      0.91      0.91        23\n",
      "weighted avg       0.91      0.91      0.91        23\n",
      "\n",
      "[[11  1]\n",
      " [ 1 10]]\n",
      "ACCURACY: 0.9130434782608695\n",
      "PRECISION: 0.9090909090909091\n",
      "RECALL: 0.9090909090909091\n",
      "F1: 0.9090909090909091\n",
      "ROC_AUC(Pr.): 0.9393939393939393\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.96        23\n",
      "   macro avg       0.96      0.96      0.96        23\n",
      "weighted avg       0.96      0.96      0.96        23\n",
      "\n",
      "[[11  1]\n",
      " [ 0 11]]\n",
      "ACCURACY: 0.9565217391304348\n",
      "PRECISION: 0.9166666666666666\n",
      "RECALL: 1.0\n",
      "F1: 0.9565217391304348\n",
      "ROC_AUC(Pr.): 0.9545454545454545\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.96        23\n",
      "   macro avg       0.96      0.96      0.96        23\n",
      "weighted avg       0.96      0.96      0.96        23\n",
      "\n",
      "[[11  1]\n",
      " [ 0 11]]\n",
      "ACCURACY: 0.9565217391304348\n",
      "PRECISION: 0.9166666666666666\n",
      "RECALL: 1.0\n",
      "F1: 0.9565217391304348\n",
      "ROC_AUC(Pr.): 0.9545454545454545\n",
      "\n",
      "=====================CV[7]========================\n",
      "\n",
      "207 train samples\n",
      "23 test samples\n",
      "\n",
      "Over sampling: 207 -> 208\n",
      "Add train samples: 16\n",
      "224 new train samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\tf28\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.00000010\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_28484\\4287042063.py:159: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================EPOCH 000001:======================\n",
      "   epoch      loss     acc  val_loss   val_acc      rocp        f1       rfa\n",
      "0    1.0  0.691154  0.5625  0.692384  0.652174  0.878788  0.733333  0.748564\n",
      "RFA : 0.7485639080314925\n",
      "7/7 - 6s - loss: 0.6912 - accuracy: 0.5625 - val_loss: 0.6924 - val_accuracy: 0.6522 - lr: 2.0010e-04 - 6s/epoch - 902ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 2/200\n",
      "7/7 - 1s - loss: 0.6933 - accuracy: 0.4866 - val_loss: 0.6921 - val_accuracy: 0.4783 - lr: 4.0010e-04 - 1s/epoch - 177ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 3/200\n",
      "7/7 - 0s - loss: 0.6912 - accuracy: 0.5223 - val_loss: 0.6886 - val_accuracy: 0.5217 - lr: 6.0009e-04 - 355ms/epoch - 51ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 4/200\n",
      "======================EPOCH 000004:======================\n",
      "   epoch      loss       acc  val_loss  val_acc      rocp        f1       rfa\n",
      "3    4.0  0.689748  0.504464  0.679687  0.73913  0.878788  0.666667  0.755665\n",
      "RFA : 0.7556653500506372\n",
      "7/7 - 0s - loss: 0.6897 - accuracy: 0.5045 - val_loss: 0.6797 - val_accuracy: 0.7391 - lr: 8.0009e-04 - 383ms/epoch - 55ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 5/200\n",
      "======================EPOCH 000005:======================\n",
      "   epoch      loss     acc  val_loss   val_acc      rocp        f1       rfa\n",
      "4    5.0  0.671177  0.6875  0.634905  0.782609  0.886364  0.761905  0.806489\n",
      "RFA : 0.8064887983329369\n",
      "7/7 - 0s - loss: 0.6712 - accuracy: 0.6875 - val_loss: 0.6349 - val_accuracy: 0.7826 - lr: 0.0010 - 382ms/epoch - 55ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 6/200\n",
      "7/7 - 1s - loss: 0.6129 - accuracy: 0.7054 - val_loss: 0.5038 - val_accuracy: 0.7826 - lr: 0.0012 - 1s/epoch - 161ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 7/200\n",
      "======================EPOCH 000007:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "6    7.0  0.504439  0.741071  0.409013  0.782609  0.886364  0.782609  0.813735\n",
      "RFA : 0.813735175144531\n",
      "7/7 - 0s - loss: 0.5044 - accuracy: 0.7411 - val_loss: 0.4090 - val_accuracy: 0.7826 - lr: 0.0014 - 381ms/epoch - 54ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 8/200\n",
      "======================EPOCH 000008:======================\n",
      "   epoch      loss      acc  val_loss   val_acc      rocp        f1       rfa\n",
      "7    8.0  0.504575  0.71875  0.400724  0.826087  0.886364  0.833333  0.846706\n",
      "RFA : 0.8467061860091758\n",
      "7/7 - 0s - loss: 0.5046 - accuracy: 0.7188 - val_loss: 0.4007 - val_accuracy: 0.8261 - lr: 0.0016 - 380ms/epoch - 54ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 9/200\n",
      "7/7 - 0s - loss: 0.4323 - accuracy: 0.7902 - val_loss: 0.3797 - val_accuracy: 0.8261 - lr: 0.0018 - 490ms/epoch - 70ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 10/200\n",
      "7/7 - 1s - loss: 0.4253 - accuracy: 0.7902 - val_loss: 0.3996 - val_accuracy: 0.7826 - lr: 0.0020 - 688ms/epoch - 98ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 11/200\n",
      "7/7 - 1s - loss: 0.3942 - accuracy: 0.8125 - val_loss: 0.4203 - val_accuracy: 0.7826 - lr: 0.0022 - 668ms/epoch - 95ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 12/200\n",
      "======================EPOCH 000012:======================\n",
      "    epoch      loss     acc  val_loss   val_acc      rocp        f1       rfa\n",
      "11   12.0  0.433379  0.8125  0.358404  0.869565  0.916667  0.869565  0.883696\n",
      "RFA : 0.8836956421966138\n",
      "7/7 - 1s - loss: 0.4334 - accuracy: 0.8125 - val_loss: 0.3584 - val_accuracy: 0.8696 - lr: 0.0024 - 552ms/epoch - 79ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 13/200\n",
      "7/7 - 0s - loss: 0.3619 - accuracy: 0.8080 - val_loss: 0.3856 - val_accuracy: 0.7826 - lr: 0.0026 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 14/200\n",
      "7/7 - 0s - loss: 0.3550 - accuracy: 0.8214 - val_loss: 0.4220 - val_accuracy: 0.7826 - lr: 0.0028 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 15/200\n",
      "7/7 - 0s - loss: 0.4044 - accuracy: 0.8080 - val_loss: 0.3770 - val_accuracy: 0.8261 - lr: 0.0030 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 16/200\n",
      "7/7 - 1s - loss: 0.4274 - accuracy: 0.8125 - val_loss: 0.4074 - val_accuracy: 0.7826 - lr: 0.0032 - 777ms/epoch - 111ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 17/200\n",
      "7/7 - 0s - loss: 0.3560 - accuracy: 0.8482 - val_loss: 0.3383 - val_accuracy: 0.8696 - lr: 0.0034 - 488ms/epoch - 70ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 18/200\n",
      "7/7 - 2s - loss: 0.3473 - accuracy: 0.8750 - val_loss: 0.3433 - val_accuracy: 0.8261 - lr: 0.0036 - 2s/epoch - 241ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 19/200\n",
      "7/7 - 0s - loss: 0.3779 - accuracy: 0.8482 - val_loss: 0.3484 - val_accuracy: 0.8261 - lr: 0.0038 - 499ms/epoch - 71ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 20/200\n",
      "7/7 - 0s - loss: 0.3188 - accuracy: 0.8438 - val_loss: 0.3912 - val_accuracy: 0.7826 - lr: 0.0040 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 21/200\n",
      "7/7 - 1s - loss: 0.3363 - accuracy: 0.8482 - val_loss: 0.3658 - val_accuracy: 0.7826 - lr: 0.0042 - 1s/epoch - 201ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 22/200\n",
      "7/7 - 0s - loss: 0.3679 - accuracy: 0.8214 - val_loss: 0.4936 - val_accuracy: 0.8261 - lr: 0.0044 - 475ms/epoch - 68ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 23/200\n",
      "7/7 - 0s - loss: 0.4255 - accuracy: 0.7991 - val_loss: 0.3772 - val_accuracy: 0.8261 - lr: 0.0046 - 392ms/epoch - 56ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 24/200\n",
      "======================EPOCH 000024:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "23   24.0  0.340785  0.839286  0.312907  0.869565  0.931818  0.869565  0.888241\n",
      "RFA : 0.8882410967420684\n",
      "7/7 - 0s - loss: 0.3408 - accuracy: 0.8393 - val_loss: 0.3129 - val_accuracy: 0.8696 - lr: 0.0048 - 370ms/epoch - 53ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 25/200\n",
      "7/7 - 1s - loss: 0.3411 - accuracy: 0.8616 - val_loss: 0.4101 - val_accuracy: 0.7826 - lr: 0.0050 - 1s/epoch - 207ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 26/200\n",
      "7/7 - 0s - loss: 0.3752 - accuracy: 0.8080 - val_loss: 0.3463 - val_accuracy: 0.8261 - lr: 0.0052 - 494ms/epoch - 71ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 27/200\n",
      "======================EPOCH 000027:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "26   27.0  0.358066  0.830357  0.327628  0.913043  0.931818  0.916667  0.919944\n",
      "RFA : 0.9199440125263099\n",
      "7/7 - 0s - loss: 0.3581 - accuracy: 0.8304 - val_loss: 0.3276 - val_accuracy: 0.9130 - lr: 0.0054 - 377ms/epoch - 54ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 28/200\n",
      "7/7 - 1s - loss: 0.2880 - accuracy: 0.8750 - val_loss: 0.3704 - val_accuracy: 0.8261 - lr: 0.0056 - 627ms/epoch - 90ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 29/200\n",
      "7/7 - 1s - loss: 0.3844 - accuracy: 0.8438 - val_loss: 0.3085 - val_accuracy: 0.8261 - lr: 0.0058 - 1s/epoch - 167ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 30/200\n",
      "======================EPOCH 000030:======================\n",
      "    epoch      loss       acc  val_loss   val_acc     rocp        f1       rfa\n",
      "29   30.0  0.322539  0.839286  0.287739  0.913043  0.94697  0.909091  0.921838\n",
      "RFA : 0.9218379519202493\n",
      "7/7 - 0s - loss: 0.3225 - accuracy: 0.8393 - val_loss: 0.2877 - val_accuracy: 0.9130 - lr: 0.0060 - 468ms/epoch - 67ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 31/200\n",
      "7/7 - 0s - loss: 0.4227 - accuracy: 0.7946 - val_loss: 0.6689 - val_accuracy: 0.6522 - lr: 0.0062 - 352ms/epoch - 50ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 32/200\n",
      "7/7 - 0s - loss: 0.4051 - accuracy: 0.7768 - val_loss: 0.3882 - val_accuracy: 0.7826 - lr: 0.0064 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 33/200\n",
      "7/7 - 0s - loss: 0.3457 - accuracy: 0.8304 - val_loss: 0.3523 - val_accuracy: 0.8261 - lr: 0.0066 - 457ms/epoch - 65ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 34/200\n",
      "7/7 - 1s - loss: 0.3263 - accuracy: 0.8705 - val_loss: 0.5085 - val_accuracy: 0.7826 - lr: 0.0068 - 1s/epoch - 161ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 35/200\n",
      "7/7 - 1s - loss: 0.3306 - accuracy: 0.8616 - val_loss: 0.4628 - val_accuracy: 0.7826 - lr: 0.0070 - 1s/epoch - 168ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 36/200\n",
      "7/7 - 0s - loss: 0.3188 - accuracy: 0.8393 - val_loss: 0.3800 - val_accuracy: 0.8696 - lr: 0.0072 - 481ms/epoch - 69ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 37/200\n",
      "7/7 - 0s - loss: 0.3245 - accuracy: 0.8661 - val_loss: 0.4727 - val_accuracy: 0.7826 - lr: 0.0074 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 38/200\n",
      "======================EPOCH 000038:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "37   38.0  0.330567  0.866071  0.310113  0.956522  0.939394  0.956522  0.951383\n",
      "RFA : 0.9513834028375949\n",
      "7/7 - 0s - loss: 0.3306 - accuracy: 0.8661 - val_loss: 0.3101 - val_accuracy: 0.9565 - lr: 0.0076 - 364ms/epoch - 52ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 39/200\n",
      "7/7 - 0s - loss: 0.2986 - accuracy: 0.8750 - val_loss: 0.6173 - val_accuracy: 0.6522 - lr: 0.0078 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 40/200\n",
      "7/7 - 0s - loss: 0.3999 - accuracy: 0.8036 - val_loss: 0.3612 - val_accuracy: 0.8261 - lr: 0.0080 - 403ms/epoch - 58ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 41/200\n",
      "7/7 - 0s - loss: 0.3557 - accuracy: 0.8214 - val_loss: 0.3641 - val_accuracy: 0.8261 - lr: 0.0082 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 42/200\n",
      "7/7 - 0s - loss: 0.3329 - accuracy: 0.8661 - val_loss: 0.3566 - val_accuracy: 0.8696 - lr: 0.0084 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 43/200\n",
      "7/7 - 0s - loss: 0.3175 - accuracy: 0.8750 - val_loss: 0.3435 - val_accuracy: 0.8261 - lr: 0.0086 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 44/200\n",
      "7/7 - 0s - loss: 0.2970 - accuracy: 0.8616 - val_loss: 0.3977 - val_accuracy: 0.7826 - lr: 0.0088 - 409ms/epoch - 58ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 45/200\n",
      "7/7 - 1s - loss: 0.2677 - accuracy: 0.8750 - val_loss: 0.4098 - val_accuracy: 0.8261 - lr: 0.0090 - 1s/epoch - 190ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 46/200\n",
      "7/7 - 1s - loss: 0.3541 - accuracy: 0.8393 - val_loss: 0.3613 - val_accuracy: 0.8696 - lr: 0.0092 - 1s/epoch - 176ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 47/200\n",
      "7/7 - 0s - loss: 0.3082 - accuracy: 0.8571 - val_loss: 0.3301 - val_accuracy: 0.8696 - lr: 0.0094 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 48/200\n",
      "7/7 - 0s - loss: 0.2736 - accuracy: 0.8750 - val_loss: 0.2680 - val_accuracy: 0.9130 - lr: 0.0096 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 49/200\n",
      "7/7 - 0s - loss: 0.3055 - accuracy: 0.8750 - val_loss: 0.3325 - val_accuracy: 0.8696 - lr: 0.0098 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 50/200\n",
      "7/7 - 0s - loss: 0.3162 - accuracy: 0.8482 - val_loss: 0.3345 - val_accuracy: 0.8696 - lr: 0.0100 - 487ms/epoch - 70ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 51/200\n",
      "7/7 - 0s - loss: 0.3539 - accuracy: 0.8393 - val_loss: 0.2817 - val_accuracy: 0.9130 - lr: 0.0098 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 52/200\n",
      "7/7 - 0s - loss: 0.3078 - accuracy: 0.8616 - val_loss: 0.3798 - val_accuracy: 0.8261 - lr: 0.0096 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 53/200\n",
      "7/7 - 0s - loss: 0.2937 - accuracy: 0.8705 - val_loss: 0.4996 - val_accuracy: 0.7826 - lr: 0.0094 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 54/200\n",
      "7/7 - 0s - loss: 0.3717 - accuracy: 0.8348 - val_loss: 0.4989 - val_accuracy: 0.7826 - lr: 0.0092 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 55/200\n",
      "7/7 - 0s - loss: 0.3667 - accuracy: 0.8036 - val_loss: 0.3273 - val_accuracy: 0.8261 - lr: 0.0090 - 495ms/epoch - 71ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 56/200\n",
      "7/7 - 0s - loss: 0.2815 - accuracy: 0.8929 - val_loss: 0.3064 - val_accuracy: 0.8696 - lr: 0.0088 - 321ms/epoch - 46ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 57/200\n",
      "7/7 - 1s - loss: 0.3162 - accuracy: 0.8482 - val_loss: 0.4710 - val_accuracy: 0.7826 - lr: 0.0086 - 733ms/epoch - 105ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 58/200\n",
      "7/7 - 0s - loss: 0.3217 - accuracy: 0.8571 - val_loss: 0.2987 - val_accuracy: 0.9130 - lr: 0.0084 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 59/200\n",
      "7/7 - 1s - loss: 0.2760 - accuracy: 0.8973 - val_loss: 0.3438 - val_accuracy: 0.8261 - lr: 0.0082 - 641ms/epoch - 92ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 60/200\n",
      "7/7 - 2s - loss: 0.3104 - accuracy: 0.8839 - val_loss: 0.3045 - val_accuracy: 0.9130 - lr: 0.0080 - 2s/epoch - 245ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 61/200\n",
      "7/7 - 0s - loss: 0.2959 - accuracy: 0.8705 - val_loss: 0.3327 - val_accuracy: 0.8261 - lr: 0.0078 - 487ms/epoch - 70ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 62/200\n",
      "7/7 - 0s - loss: 0.2620 - accuracy: 0.8973 - val_loss: 0.3072 - val_accuracy: 0.8261 - lr: 0.0076 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 63/200\n",
      "7/7 - 0s - loss: 0.3037 - accuracy: 0.8973 - val_loss: 0.3192 - val_accuracy: 0.8261 - lr: 0.0074 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 64/200\n",
      "7/7 - 0s - loss: 0.3352 - accuracy: 0.8214 - val_loss: 0.4033 - val_accuracy: 0.7391 - lr: 0.0072 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 65/200\n",
      "7/7 - 1s - loss: 0.2777 - accuracy: 0.8839 - val_loss: 0.2807 - val_accuracy: 0.8696 - lr: 0.0070 - 1s/epoch - 167ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 66/200\n",
      "7/7 - 0s - loss: 0.2737 - accuracy: 0.8661 - val_loss: 0.3399 - val_accuracy: 0.8696 - lr: 0.0068 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 67/200\n",
      "7/7 - 0s - loss: 0.2177 - accuracy: 0.9107 - val_loss: 0.4029 - val_accuracy: 0.8261 - lr: 0.0066 - 331ms/epoch - 47ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 68/200\n",
      "7/7 - 1s - loss: 0.2731 - accuracy: 0.9018 - val_loss: 0.3172 - val_accuracy: 0.9130 - lr: 0.0064 - 559ms/epoch - 80ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 69/200\n",
      "7/7 - 0s - loss: 0.2531 - accuracy: 0.8973 - val_loss: 0.3472 - val_accuracy: 0.8696 - lr: 0.0062 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 70/200\n",
      "7/7 - 0s - loss: 0.2257 - accuracy: 0.9152 - val_loss: 0.3723 - val_accuracy: 0.8696 - lr: 0.0060 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 71/200\n",
      "7/7 - 0s - loss: 0.2661 - accuracy: 0.9152 - val_loss: 0.4630 - val_accuracy: 0.7826 - lr: 0.0058 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 72/200\n",
      "7/7 - 0s - loss: 0.2901 - accuracy: 0.8750 - val_loss: 0.3357 - val_accuracy: 0.8696 - lr: 0.0056 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 73/200\n",
      "7/7 - 0s - loss: 0.2360 - accuracy: 0.8795 - val_loss: 0.4131 - val_accuracy: 0.8696 - lr: 0.0054 - 492ms/epoch - 70ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 74/200\n",
      "7/7 - 0s - loss: 0.2625 - accuracy: 0.8839 - val_loss: 0.3205 - val_accuracy: 0.9130 - lr: 0.0052 - 331ms/epoch - 47ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 75/200\n",
      "7/7 - 1s - loss: 0.2417 - accuracy: 0.9018 - val_loss: 0.3987 - val_accuracy: 0.8261 - lr: 0.0050 - 635ms/epoch - 91ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 76/200\n",
      "7/7 - 0s - loss: 0.2496 - accuracy: 0.8839 - val_loss: 0.3529 - val_accuracy: 0.8696 - lr: 0.0048 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 77/200\n",
      "7/7 - 1s - loss: 0.3070 - accuracy: 0.8929 - val_loss: 0.3381 - val_accuracy: 0.8261 - lr: 0.0046 - 878ms/epoch - 125ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 78/200\n",
      "7/7 - 0s - loss: 0.2425 - accuracy: 0.9107 - val_loss: 0.3644 - val_accuracy: 0.8261 - lr: 0.0044 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 79/200\n",
      "7/7 - 0s - loss: 0.2446 - accuracy: 0.9062 - val_loss: 0.3118 - val_accuracy: 0.8696 - lr: 0.0042 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 80/200\n",
      "7/7 - 0s - loss: 0.3019 - accuracy: 0.8750 - val_loss: 0.2772 - val_accuracy: 0.8696 - lr: 0.0040 - 276ms/epoch - 39ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 81/200\n",
      "7/7 - 0s - loss: 0.2836 - accuracy: 0.8973 - val_loss: 0.3486 - val_accuracy: 0.8261 - lr: 0.0038 - 382ms/epoch - 55ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 82/200\n",
      "7/7 - 0s - loss: 0.2916 - accuracy: 0.8661 - val_loss: 0.3207 - val_accuracy: 0.8696 - lr: 0.0036 - 351ms/epoch - 50ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 83/200\n",
      "7/7 - 0s - loss: 0.2477 - accuracy: 0.8973 - val_loss: 0.2820 - val_accuracy: 0.8696 - lr: 0.0034 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 84/200\n",
      "7/7 - 0s - loss: 0.2033 - accuracy: 0.9107 - val_loss: 0.3621 - val_accuracy: 0.8261 - lr: 0.0032 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 85/200\n",
      "7/7 - 0s - loss: 0.2150 - accuracy: 0.8929 - val_loss: 0.3184 - val_accuracy: 0.8696 - lr: 0.0030 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 86/200\n",
      "7/7 - 1s - loss: 0.2335 - accuracy: 0.9062 - val_loss: 0.2652 - val_accuracy: 0.9130 - lr: 0.0028 - 716ms/epoch - 102ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 87/200\n",
      "7/7 - 0s - loss: 0.2754 - accuracy: 0.8973 - val_loss: 0.3157 - val_accuracy: 0.9130 - lr: 0.0026 - 341ms/epoch - 49ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 88/200\n",
      "7/7 - 0s - loss: 0.2372 - accuracy: 0.8973 - val_loss: 0.2891 - val_accuracy: 0.8696 - lr: 0.0024 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 89/200\n",
      "7/7 - 0s - loss: 0.2503 - accuracy: 0.9018 - val_loss: 0.2577 - val_accuracy: 0.9130 - lr: 0.0022 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 90/200\n",
      "7/7 - 1s - loss: 0.2378 - accuracy: 0.9018 - val_loss: 0.3055 - val_accuracy: 0.8696 - lr: 0.0020 - 635ms/epoch - 91ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 91/200\n",
      "7/7 - 0s - loss: 0.1947 - accuracy: 0.9196 - val_loss: 0.2741 - val_accuracy: 0.8696 - lr: 0.0018 - 485ms/epoch - 69ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 92/200\n",
      "7/7 - 0s - loss: 0.2157 - accuracy: 0.9196 - val_loss: 0.2890 - val_accuracy: 0.9130 - lr: 0.0016 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 93/200\n",
      "7/7 - 0s - loss: 0.2198 - accuracy: 0.9062 - val_loss: 0.3434 - val_accuracy: 0.8696 - lr: 0.0014 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 94/200\n",
      "7/7 - 0s - loss: 0.2127 - accuracy: 0.9196 - val_loss: 0.2873 - val_accuracy: 0.8261 - lr: 0.0012 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 95/200\n",
      "7/7 - 0s - loss: 0.2423 - accuracy: 0.9018 - val_loss: 0.2589 - val_accuracy: 0.9130 - lr: 0.0010 - 395ms/epoch - 56ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 96/200\n",
      "7/7 - 1s - loss: 0.2311 - accuracy: 0.9286 - val_loss: 0.2533 - val_accuracy: 0.9130 - lr: 8.0009e-04 - 603ms/epoch - 86ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 97/200\n",
      "7/7 - 0s - loss: 0.2412 - accuracy: 0.9107 - val_loss: 0.2521 - val_accuracy: 0.9130 - lr: 6.0009e-04 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 98/200\n",
      "7/7 - 0s - loss: 0.2559 - accuracy: 0.8973 - val_loss: 0.2681 - val_accuracy: 0.9130 - lr: 4.0010e-04 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 99/200\n",
      "7/7 - 1s - loss: 0.2004 - accuracy: 0.9196 - val_loss: 0.2625 - val_accuracy: 0.9130 - lr: 2.0010e-04 - 1s/epoch - 144ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 100/200\n",
      "7/7 - 0s - loss: 0.2152 - accuracy: 0.9107 - val_loss: 0.2591 - val_accuracy: 0.9130 - lr: 1.0000e-07 - 478ms/epoch - 68ms/step\n",
      "Learning rate: 0.00000010\n",
      "Epoch 101/200\n",
      "7/7 - 0s - loss: 0.2542 - accuracy: 0.8884 - val_loss: 0.2565 - val_accuracy: 0.9130 - lr: 2.0010e-04 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 102/200\n",
      "7/7 - 0s - loss: 0.2465 - accuracy: 0.9107 - val_loss: 0.2596 - val_accuracy: 0.9130 - lr: 4.0010e-04 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 103/200\n",
      "7/7 - 0s - loss: 0.2348 - accuracy: 0.8929 - val_loss: 0.2638 - val_accuracy: 0.9130 - lr: 6.0009e-04 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 104/200\n",
      "7/7 - 2s - loss: 0.1970 - accuracy: 0.9196 - val_loss: 0.2811 - val_accuracy: 0.9130 - lr: 8.0009e-04 - 2s/epoch - 229ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 105/200\n",
      "7/7 - 0s - loss: 0.1982 - accuracy: 0.9241 - val_loss: 0.2723 - val_accuracy: 0.9130 - lr: 0.0010 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 106/200\n",
      "7/7 - 0s - loss: 0.2097 - accuracy: 0.9107 - val_loss: 0.3036 - val_accuracy: 0.9130 - lr: 0.0012 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 107/200\n",
      "7/7 - 0s - loss: 0.2135 - accuracy: 0.9152 - val_loss: 0.2809 - val_accuracy: 0.9130 - lr: 0.0014 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 108/200\n",
      "7/7 - 0s - loss: 0.3078 - accuracy: 0.8884 - val_loss: 0.2558 - val_accuracy: 0.9130 - lr: 0.0016 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 109/200\n",
      "7/7 - 0s - loss: 0.2223 - accuracy: 0.9196 - val_loss: 0.2871 - val_accuracy: 0.9130 - lr: 0.0018 - 341ms/epoch - 49ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 110/200\n",
      "7/7 - 1s - loss: 0.2507 - accuracy: 0.8884 - val_loss: 0.2484 - val_accuracy: 0.9130 - lr: 0.0020 - 520ms/epoch - 74ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 111/200\n",
      "7/7 - 1s - loss: 0.2607 - accuracy: 0.8973 - val_loss: 0.2591 - val_accuracy: 0.9130 - lr: 0.0022 - 782ms/epoch - 112ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 112/200\n",
      "7/7 - 0s - loss: 0.1969 - accuracy: 0.9509 - val_loss: 0.2539 - val_accuracy: 0.9130 - lr: 0.0024 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 113/200\n",
      "7/7 - 0s - loss: 0.2547 - accuracy: 0.8973 - val_loss: 0.2350 - val_accuracy: 0.9130 - lr: 0.0026 - 351ms/epoch - 50ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 114/200\n",
      "7/7 - 0s - loss: 0.2140 - accuracy: 0.9107 - val_loss: 0.2478 - val_accuracy: 0.9130 - lr: 0.0028 - 439ms/epoch - 63ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 115/200\n",
      "7/7 - 1s - loss: 0.2168 - accuracy: 0.9196 - val_loss: 0.2724 - val_accuracy: 0.8696 - lr: 0.0030 - 1s/epoch - 183ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 116/200\n",
      "7/7 - 1s - loss: 0.2447 - accuracy: 0.9196 - val_loss: 0.2629 - val_accuracy: 0.9130 - lr: 0.0032 - 1s/epoch - 145ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 117/200\n",
      "7/7 - 0s - loss: 0.2162 - accuracy: 0.9062 - val_loss: 0.2816 - val_accuracy: 0.9130 - lr: 0.0034 - 494ms/epoch - 71ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 118/200\n",
      "7/7 - 1s - loss: 0.2252 - accuracy: 0.9152 - val_loss: 0.2867 - val_accuracy: 0.9130 - lr: 0.0036 - 507ms/epoch - 72ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 119/200\n",
      "7/7 - 2s - loss: 0.2263 - accuracy: 0.8884 - val_loss: 0.3422 - val_accuracy: 0.8696 - lr: 0.0038 - 2s/epoch - 320ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 120/200\n",
      "7/7 - 1s - loss: 0.2737 - accuracy: 0.8750 - val_loss: 0.2387 - val_accuracy: 0.9130 - lr: 0.0040 - 707ms/epoch - 101ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 121/200\n",
      "7/7 - 0s - loss: 0.2559 - accuracy: 0.9152 - val_loss: 0.2550 - val_accuracy: 0.8696 - lr: 0.0042 - 354ms/epoch - 51ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 122/200\n",
      "7/7 - 0s - loss: 0.2021 - accuracy: 0.9196 - val_loss: 0.2379 - val_accuracy: 0.9130 - lr: 0.0044 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 123/200\n",
      "7/7 - 0s - loss: 0.1938 - accuracy: 0.9241 - val_loss: 0.2453 - val_accuracy: 0.9130 - lr: 0.0046 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 124/200\n",
      "7/7 - 0s - loss: 0.2094 - accuracy: 0.9196 - val_loss: 0.2703 - val_accuracy: 0.8696 - lr: 0.0048 - 437ms/epoch - 62ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 125/200\n",
      "7/7 - 1s - loss: 0.2534 - accuracy: 0.9107 - val_loss: 0.1986 - val_accuracy: 0.9130 - lr: 0.0050 - 1s/epoch - 147ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 126/200\n",
      "7/7 - 0s - loss: 0.2814 - accuracy: 0.9107 - val_loss: 0.2511 - val_accuracy: 0.9130 - lr: 0.0052 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 127/200\n",
      "7/7 - 1s - loss: 0.2761 - accuracy: 0.8795 - val_loss: 0.2031 - val_accuracy: 0.9130 - lr: 0.0054 - 532ms/epoch - 76ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 128/200\n",
      "7/7 - 1s - loss: 0.3074 - accuracy: 0.8661 - val_loss: 0.2679 - val_accuracy: 0.8696 - lr: 0.0056 - 1s/epoch - 186ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 129/200\n",
      "7/7 - 0s - loss: 0.2647 - accuracy: 0.9018 - val_loss: 0.1891 - val_accuracy: 0.9130 - lr: 0.0058 - 359ms/epoch - 51ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 130/200\n",
      "7/7 - 1s - loss: 0.2322 - accuracy: 0.9107 - val_loss: 0.2030 - val_accuracy: 0.9130 - lr: 0.0060 - 501ms/epoch - 72ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 131/200\n",
      "7/7 - 0s - loss: 0.2300 - accuracy: 0.9241 - val_loss: 0.2476 - val_accuracy: 0.8696 - lr: 0.0062 - 400ms/epoch - 57ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 132/200\n",
      "7/7 - 1s - loss: 0.2525 - accuracy: 0.9018 - val_loss: 0.2455 - val_accuracy: 0.8696 - lr: 0.0064 - 713ms/epoch - 102ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 133/200\n",
      "7/7 - 1s - loss: 0.2666 - accuracy: 0.8616 - val_loss: 0.2405 - val_accuracy: 0.9130 - lr: 0.0066 - 551ms/epoch - 79ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 134/200\n",
      "7/7 - 1s - loss: 0.2221 - accuracy: 0.9062 - val_loss: 0.1758 - val_accuracy: 0.8696 - lr: 0.0068 - 1s/epoch - 151ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 135/200\n",
      "7/7 - 1s - loss: 0.2252 - accuracy: 0.9062 - val_loss: 0.2263 - val_accuracy: 0.9130 - lr: 0.0070 - 1s/epoch - 189ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 136/200\n",
      "======================EPOCH 000136:======================\n",
      "     epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "135  136.0  0.321673  0.892857  0.225302  0.956522  0.969697  0.956522  0.960474\n",
      "RFA : 0.960474311928504\n",
      "7/7 - 0s - loss: 0.3217 - accuracy: 0.8929 - val_loss: 0.2253 - val_accuracy: 0.9565 - lr: 0.0072 - 490ms/epoch - 70ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 137/200\n",
      "7/7 - 1s - loss: 0.2556 - accuracy: 0.8929 - val_loss: 0.2570 - val_accuracy: 0.8696 - lr: 0.0074 - 1s/epoch - 198ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 138/200\n",
      "7/7 - 1s - loss: 0.3036 - accuracy: 0.8839 - val_loss: 0.2609 - val_accuracy: 0.9130 - lr: 0.0076 - 504ms/epoch - 72ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 139/200\n",
      "7/7 - 0s - loss: 0.2784 - accuracy: 0.8795 - val_loss: 0.2875 - val_accuracy: 0.8696 - lr: 0.0078 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 140/200\n",
      "======================EPOCH 000140:======================\n",
      "     epoch      loss       acc  val_loss   val_acc      rocp        f1      rfa\n",
      "139  140.0  0.252046  0.883929  0.197247  0.956522  0.984848  0.952381  0.96357\n",
      "RFA : 0.9635704911116397\n",
      "7/7 - 0s - loss: 0.2520 - accuracy: 0.8839 - val_loss: 0.1972 - val_accuracy: 0.9565 - lr: 0.0080 - 360ms/epoch - 51ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 141/200\n",
      "7/7 - 0s - loss: 0.2281 - accuracy: 0.9107 - val_loss: 0.4077 - val_accuracy: 0.7826 - lr: 0.0082 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 142/200\n",
      "7/7 - 2s - loss: 29.0377 - accuracy: 0.7768 - val_loss: 15.6142 - val_accuracy: 0.4783 - lr: 0.0084 - 2s/epoch - 246ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 143/200\n",
      "7/7 - 1s - loss: 3313.0884 - accuracy: 0.5134 - val_loss: 50919.4062 - val_accuracy: 0.5217 - lr: 0.0086 - 534ms/epoch - 76ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 144/200\n",
      "7/7 - 0s - loss: 8640.6045 - accuracy: 0.4598 - val_loss: 242.1285 - val_accuracy: 0.5217 - lr: 0.0088 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 145/200\n",
      "7/7 - 1s - loss: 107.2984 - accuracy: 0.5312 - val_loss: 103.2174 - val_accuracy: 0.4783 - lr: 0.0090 - 1s/epoch - 201ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 146/200\n",
      "7/7 - 1s - loss: 47.6314 - accuracy: 0.5491 - val_loss: 67.0891 - val_accuracy: 0.4783 - lr: 0.0092 - 1s/epoch - 174ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 147/200\n",
      "7/7 - 0s - loss: 19.0649 - accuracy: 0.5491 - val_loss: 6.9715 - val_accuracy: 0.5217 - lr: 0.0094 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 148/200\n",
      "7/7 - 0s - loss: 6.0940 - accuracy: 0.5357 - val_loss: 3.0196 - val_accuracy: 0.6522 - lr: 0.0096 - 481ms/epoch - 69ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 149/200\n",
      "7/7 - 0s - loss: 2.8381 - accuracy: 0.6473 - val_loss: 0.9222 - val_accuracy: 0.6957 - lr: 0.0098 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 150/200\n",
      "7/7 - 0s - loss: 2.2494 - accuracy: 0.6429 - val_loss: 2.1286 - val_accuracy: 0.6522 - lr: 0.0100 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 151/200\n",
      "7/7 - 0s - loss: 2.6460 - accuracy: 0.6116 - val_loss: 0.2221 - val_accuracy: 0.9130 - lr: 0.0098 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 152/200\n",
      "7/7 - 0s - loss: 1.4769 - accuracy: 0.7232 - val_loss: 1.6141 - val_accuracy: 0.6957 - lr: 0.0096 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 153/200\n",
      "7/7 - 0s - loss: 1.3793 - accuracy: 0.7679 - val_loss: 1.5641 - val_accuracy: 0.6522 - lr: 0.0094 - 494ms/epoch - 71ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 154/200\n",
      "7/7 - 0s - loss: 1.3996 - accuracy: 0.6875 - val_loss: 0.3549 - val_accuracy: 0.8696 - lr: 0.0092 - 322ms/epoch - 46ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 155/200\n",
      "7/7 - 0s - loss: 0.9526 - accuracy: 0.7812 - val_loss: 0.5122 - val_accuracy: 0.7826 - lr: 0.0090 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 156/200\n",
      "7/7 - 0s - loss: 0.9843 - accuracy: 0.7366 - val_loss: 0.1803 - val_accuracy: 0.9130 - lr: 0.0088 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 157/200\n",
      "7/7 - 0s - loss: 0.9282 - accuracy: 0.7411 - val_loss: 0.2914 - val_accuracy: 0.8696 - lr: 0.0086 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 158/200\n",
      "7/7 - 1s - loss: 0.8732 - accuracy: 0.7902 - val_loss: 0.1538 - val_accuracy: 0.9565 - lr: 0.0084 - 507ms/epoch - 72ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 159/200\n",
      "7/7 - 0s - loss: 0.8831 - accuracy: 0.7768 - val_loss: 0.6096 - val_accuracy: 0.7826 - lr: 0.0082 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 160/200\n",
      "7/7 - 2s - loss: 1.0986 - accuracy: 0.7545 - val_loss: 1.3081 - val_accuracy: 0.6957 - lr: 0.0080 - 2s/epoch - 262ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 161/200\n",
      "7/7 - 0s - loss: 1.0457 - accuracy: 0.7946 - val_loss: 0.1931 - val_accuracy: 0.9130 - lr: 0.0078 - 485ms/epoch - 69ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 162/200\n",
      "7/7 - 0s - loss: 0.8737 - accuracy: 0.7902 - val_loss: 0.1596 - val_accuracy: 0.9130 - lr: 0.0076 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 163/200\n",
      "7/7 - 2s - loss: 0.8446 - accuracy: 0.7946 - val_loss: 0.1806 - val_accuracy: 0.8696 - lr: 0.0074 - 2s/epoch - 239ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 164/200\n",
      "7/7 - 0s - loss: 0.9018 - accuracy: 0.7500 - val_loss: 0.1956 - val_accuracy: 0.8696 - lr: 0.0072 - 492ms/epoch - 70ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 165/200\n",
      "7/7 - 0s - loss: 0.7293 - accuracy: 0.7545 - val_loss: 0.2190 - val_accuracy: 0.8696 - lr: 0.0070 - 399ms/epoch - 57ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 166/200\n",
      "7/7 - 0s - loss: 0.7679 - accuracy: 0.7589 - val_loss: 0.2170 - val_accuracy: 0.8696 - lr: 0.0068 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 167/200\n",
      "7/7 - 0s - loss: 0.6506 - accuracy: 0.7812 - val_loss: 0.2670 - val_accuracy: 0.8696 - lr: 0.0066 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 168/200\n",
      "7/7 - 0s - loss: 0.8789 - accuracy: 0.7589 - val_loss: 0.1910 - val_accuracy: 0.9130 - lr: 0.0064 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 169/200\n",
      "7/7 - 0s - loss: 0.7093 - accuracy: 0.7812 - val_loss: 0.2617 - val_accuracy: 0.8261 - lr: 0.0062 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 170/200\n",
      "7/7 - 2s - loss: 0.6604 - accuracy: 0.7946 - val_loss: 0.3011 - val_accuracy: 0.8696 - lr: 0.0060 - 2s/epoch - 228ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 171/200\n",
      "7/7 - 0s - loss: 0.8269 - accuracy: 0.7545 - val_loss: 0.1945 - val_accuracy: 0.9130 - lr: 0.0058 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 172/200\n",
      "7/7 - 1s - loss: 0.6291 - accuracy: 0.8080 - val_loss: 0.2137 - val_accuracy: 0.8696 - lr: 0.0056 - 539ms/epoch - 77ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 173/200\n",
      "7/7 - 1s - loss: 0.5102 - accuracy: 0.7991 - val_loss: 0.2219 - val_accuracy: 0.9130 - lr: 0.0054 - 1s/epoch - 192ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 174/200\n",
      "7/7 - 0s - loss: 0.6660 - accuracy: 0.7768 - val_loss: 0.1782 - val_accuracy: 0.9565 - lr: 0.0052 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 175/200\n",
      "7/7 - 2s - loss: 0.4948 - accuracy: 0.8705 - val_loss: 0.3306 - val_accuracy: 0.8261 - lr: 0.0050 - 2s/epoch - 223ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 176/200\n",
      "7/7 - 1s - loss: 0.5573 - accuracy: 0.7946 - val_loss: 0.3036 - val_accuracy: 0.8696 - lr: 0.0048 - 1s/epoch - 173ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 177/200\n",
      "7/7 - 0s - loss: 0.6018 - accuracy: 0.7991 - val_loss: 0.2051 - val_accuracy: 0.9565 - lr: 0.0046 - 493ms/epoch - 70ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 178/200\n",
      "7/7 - 0s - loss: 0.6324 - accuracy: 0.7946 - val_loss: 0.2444 - val_accuracy: 0.8696 - lr: 0.0044 - 331ms/epoch - 47ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 179/200\n",
      "7/7 - 2s - loss: 0.5718 - accuracy: 0.7991 - val_loss: 0.2631 - val_accuracy: 0.8261 - lr: 0.0042 - 2s/epoch - 215ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 180/200\n",
      "7/7 - 0s - loss: 0.5565 - accuracy: 0.7812 - val_loss: 0.2184 - val_accuracy: 0.9565 - lr: 0.0040 - 353ms/epoch - 50ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 181/200\n",
      "7/7 - 0s - loss: 0.5047 - accuracy: 0.7991 - val_loss: 0.2004 - val_accuracy: 0.9565 - lr: 0.0038 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 182/200\n",
      "7/7 - 1s - loss: 0.6267 - accuracy: 0.7679 - val_loss: 0.2304 - val_accuracy: 0.9130 - lr: 0.0036 - 1s/epoch - 202ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 183/200\n",
      "7/7 - 1s - loss: 0.7133 - accuracy: 0.7768 - val_loss: 0.2087 - val_accuracy: 0.9565 - lr: 0.0034 - 1s/epoch - 144ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 184/200\n",
      "7/7 - 0s - loss: 0.7152 - accuracy: 0.7545 - val_loss: 0.2068 - val_accuracy: 0.9565 - lr: 0.0032 - 481ms/epoch - 69ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 185/200\n",
      "7/7 - 0s - loss: 0.4974 - accuracy: 0.8214 - val_loss: 0.2597 - val_accuracy: 0.8261 - lr: 0.0030 - 322ms/epoch - 46ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 186/200\n",
      "7/7 - 1s - loss: 0.6057 - accuracy: 0.7634 - val_loss: 0.2038 - val_accuracy: 0.9565 - lr: 0.0028 - 1s/epoch - 144ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 187/200\n",
      "7/7 - 0s - loss: 0.5197 - accuracy: 0.8170 - val_loss: 0.2362 - val_accuracy: 0.8261 - lr: 0.0026 - 481ms/epoch - 69ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 188/200\n",
      "7/7 - 1s - loss: 0.5064 - accuracy: 0.8125 - val_loss: 0.2074 - val_accuracy: 0.9565 - lr: 0.0024 - 573ms/epoch - 82ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 189/200\n",
      "7/7 - 1s - loss: 0.4717 - accuracy: 0.8438 - val_loss: 0.2050 - val_accuracy: 0.8696 - lr: 0.0022 - 995ms/epoch - 142ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 190/200\n",
      "7/7 - 0s - loss: 0.4792 - accuracy: 0.8214 - val_loss: 0.2242 - val_accuracy: 0.9565 - lr: 0.0020 - 490ms/epoch - 70ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 191/200\n",
      "7/7 - 0s - loss: 0.5383 - accuracy: 0.8036 - val_loss: 0.2128 - val_accuracy: 0.8696 - lr: 0.0018 - 322ms/epoch - 46ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 192/200\n",
      "7/7 - 0s - loss: 0.4956 - accuracy: 0.8438 - val_loss: 0.1983 - val_accuracy: 0.9565 - lr: 0.0016 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 193/200\n",
      "7/7 - 1s - loss: 0.5205 - accuracy: 0.8036 - val_loss: 0.2043 - val_accuracy: 0.9130 - lr: 0.0014 - 706ms/epoch - 101ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 194/200\n",
      "7/7 - 0s - loss: 0.5877 - accuracy: 0.7812 - val_loss: 0.2151 - val_accuracy: 0.9565 - lr: 0.0012 - 351ms/epoch - 50ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 195/200\n",
      "7/7 - 0s - loss: 0.5033 - accuracy: 0.8304 - val_loss: 0.1999 - val_accuracy: 0.9565 - lr: 0.0010 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 196/200\n",
      "7/7 - 0s - loss: 0.6220 - accuracy: 0.8214 - val_loss: 0.2007 - val_accuracy: 0.9565 - lr: 8.0009e-04 - 342ms/epoch - 49ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 197/200\n",
      "7/7 - 0s - loss: 0.4684 - accuracy: 0.8036 - val_loss: 0.2044 - val_accuracy: 0.9130 - lr: 6.0009e-04 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 198/200\n",
      "7/7 - 1s - loss: 0.5147 - accuracy: 0.8080 - val_loss: 0.2036 - val_accuracy: 0.9565 - lr: 4.0010e-04 - 501ms/epoch - 72ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 199/200\n",
      "7/7 - 1s - loss: 0.4637 - accuracy: 0.8393 - val_loss: 0.2010 - val_accuracy: 0.9565 - lr: 2.0010e-04 - 538ms/epoch - 77ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 200/200\n",
      "7/7 - 0s - loss: 0.5048 - accuracy: 0.7812 - val_loss: 0.2009 - val_accuracy: 0.9565 - lr: 1.0000e-07 - 329ms/epoch - 47ms/step\n",
      "\n",
      "======================BEST:======================\n",
      "     epoch      loss       acc  val_loss   val_acc      rocp        f1      rfa\n",
      "139  140.0  0.252046  0.883929  0.197247  0.956522  0.984848  0.952381  0.96357\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.96        23\n",
      "   macro avg       0.96      0.96      0.96        23\n",
      "weighted avg       0.96      0.96      0.96        23\n",
      "\n",
      "[[11  1]\n",
      " [ 0 11]]\n",
      "ACCURACY: 0.9565217391304348\n",
      "PRECISION: 0.9166666666666666\n",
      "RECALL: 1.0\n",
      "F1: 0.9565217391304348\n",
      "ROC_AUC(Pr.): 0.9772727272727273\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96        12\n",
      "           1       1.00      0.91      0.95        11\n",
      "\n",
      "    accuracy                           0.96        23\n",
      "   macro avg       0.96      0.95      0.96        23\n",
      "weighted avg       0.96      0.96      0.96        23\n",
      "\n",
      "[[12  0]\n",
      " [ 1 10]]\n",
      "ACCURACY: 0.9565217391304348\n",
      "PRECISION: 1.0\n",
      "RECALL: 0.9090909090909091\n",
      "F1: 0.9523809523809523\n",
      "ROC_AUC(Pr.): 0.9848484848484849\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96        12\n",
      "           1       1.00      0.91      0.95        11\n",
      "\n",
      "    accuracy                           0.96        23\n",
      "   macro avg       0.96      0.95      0.96        23\n",
      "weighted avg       0.96      0.96      0.96        23\n",
      "\n",
      "[[12  0]\n",
      " [ 1 10]]\n",
      "ACCURACY: 0.9565217391304348\n",
      "PRECISION: 1.0\n",
      "RECALL: 0.9090909090909091\n",
      "F1: 0.9523809523809523\n",
      "ROC_AUC(Pr.): 0.9848484848484849\n",
      "\n",
      "=====================CV[8]========================\n",
      "\n",
      "207 train samples\n",
      "23 test samples\n",
      "\n",
      "Over sampling: 207 -> 208\n",
      "Add train samples: 16\n",
      "224 new train samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\tf28\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.00000010\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_28484\\4287042063.py:159: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================EPOCH 000001:======================\n",
      "   epoch      loss      acc  val_loss   val_acc  rocp   f1       rfa\n",
      "0    1.0  0.692771  0.53125  0.692374  0.521739  0.75  0.0  0.407609\n",
      "RFA : 0.4076086938381195\n",
      "7/7 - 6s - loss: 0.6928 - accuracy: 0.5312 - val_loss: 0.6924 - val_accuracy: 0.5217 - lr: 2.0010e-04 - 6s/epoch - 904ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 2/200\n",
      "======================EPOCH 000002:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp   f1     rfa\n",
      "1    2.0  0.691152  0.580357  0.691985  0.521739  0.780303  0.0  0.4167\n",
      "RFA : 0.4166996029290286\n",
      "7/7 - 1s - loss: 0.6912 - accuracy: 0.5804 - val_loss: 0.6920 - val_accuracy: 0.5217 - lr: 4.0010e-04 - 821ms/epoch - 117ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 3/200\n",
      "======================EPOCH 000003:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp      f1       rfa\n",
      "2    3.0  0.691963  0.558036  0.691213  0.565217  0.772727  0.6875  0.670269\n",
      "RFA : 0.6702692633325403\n",
      "7/7 - 1s - loss: 0.6920 - accuracy: 0.5580 - val_loss: 0.6912 - val_accuracy: 0.5652 - lr: 6.0009e-04 - 1s/epoch - 195ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 4/200\n",
      "======================EPOCH 000004:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp    f1       rfa\n",
      "3    4.0  0.686944  0.584821  0.685737  0.695652  0.787879  0.72  0.731842\n",
      "RFA : 0.7318419017683375\n",
      "7/7 - 0s - loss: 0.6869 - accuracy: 0.5848 - val_loss: 0.6857 - val_accuracy: 0.6957 - lr: 8.0009e-04 - 415ms/epoch - 59ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 5/200\n",
      "======================EPOCH 000005:======================\n",
      "   epoch     loss       acc  val_loss  val_acc     rocp        f1      rfa\n",
      "4    5.0  0.67416  0.669643  0.659673  0.73913  0.80303  0.727273  0.75415\n",
      "RFA : 0.7541501985354856\n",
      "7/7 - 0s - loss: 0.6742 - accuracy: 0.6696 - val_loss: 0.6597 - val_accuracy: 0.7391 - lr: 0.0010 - 383ms/epoch - 55ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 6/200\n",
      "7/7 - 1s - loss: 0.6033 - accuracy: 0.7545 - val_loss: 0.5614 - val_accuracy: 0.6522 - lr: 0.0012 - 514ms/epoch - 73ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 7/200\n",
      "7/7 - 1s - loss: 0.4786 - accuracy: 0.7411 - val_loss: 0.6411 - val_accuracy: 0.6522 - lr: 0.0014 - 694ms/epoch - 99ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 8/200\n",
      "======================EPOCH 000008:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "7    8.0  0.422881  0.821429  0.503507  0.782609  0.856061  0.736842  0.788626\n",
      "RFA : 0.7886259594174663\n",
      "7/7 - 1s - loss: 0.4229 - accuracy: 0.8214 - val_loss: 0.5035 - val_accuracy: 0.7826 - lr: 0.0016 - 720ms/epoch - 103ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 9/200\n",
      "======================EPOCH 000009:======================\n",
      "   epoch    loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "8    9.0  0.4185  0.776786  0.431934  0.869565  0.878788  0.857143  0.867984\n",
      "RFA : 0.8679841797460208\n",
      "7/7 - 0s - loss: 0.4185 - accuracy: 0.7768 - val_loss: 0.4319 - val_accuracy: 0.8696 - lr: 0.0018 - 498ms/epoch - 71ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 10/200\n",
      "======================EPOCH 000010:======================\n",
      "   epoch     loss       acc  val_loss   val_acc      rocp        f1      rfa\n",
      "9   10.0  0.38295  0.808036  0.415467  0.869565  0.893939  0.857143  0.87253\n",
      "RFA : 0.8725296342914755\n",
      "7/7 - 0s - loss: 0.3829 - accuracy: 0.8080 - val_loss: 0.4155 - val_accuracy: 0.8696 - lr: 0.0020 - 370ms/epoch - 53ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 11/200\n",
      "======================EPOCH 000011:======================\n",
      "    epoch     loss     acc  val_loss   val_acc      rocp        f1       rfa\n",
      "10   11.0  0.38364  0.8125  0.401165  0.869565  0.909091  0.869565  0.881423\n",
      "RFA : 0.8814229149238864\n",
      "7/7 - 0s - loss: 0.3836 - accuracy: 0.8125 - val_loss: 0.4012 - val_accuracy: 0.8696 - lr: 0.0022 - 387ms/epoch - 55ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 12/200\n",
      "======================EPOCH 000012:======================\n",
      "    epoch     loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "11   12.0  0.38234  0.834821  0.384656  0.913043  0.909091  0.909091  0.910474\n",
      "RFA : 0.9104743155566128\n",
      "7/7 - 0s - loss: 0.3823 - accuracy: 0.8348 - val_loss: 0.3847 - val_accuracy: 0.9130 - lr: 0.0024 - 370ms/epoch - 53ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 13/200\n",
      "7/7 - 0s - loss: 0.3444 - accuracy: 0.8214 - val_loss: 0.5663 - val_accuracy: 0.7826 - lr: 0.0026 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 14/200\n",
      "7/7 - 0s - loss: 0.4600 - accuracy: 0.7723 - val_loss: 0.5943 - val_accuracy: 0.7826 - lr: 0.0028 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 15/200\n",
      "7/7 - 1s - loss: 0.3979 - accuracy: 0.8214 - val_loss: 0.3801 - val_accuracy: 0.8696 - lr: 0.0030 - 1s/epoch - 144ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 16/200\n",
      "7/7 - 0s - loss: 0.3362 - accuracy: 0.8571 - val_loss: 0.4419 - val_accuracy: 0.7826 - lr: 0.0032 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 17/200\n",
      "======================EPOCH 000017:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "16   17.0  0.320969  0.825893   0.36632  0.913043  0.916667  0.909091  0.912747\n",
      "RFA : 0.91274704282934\n",
      "7/7 - 1s - loss: 0.3210 - accuracy: 0.8259 - val_loss: 0.3663 - val_accuracy: 0.9130 - lr: 0.0034 - 554ms/epoch - 79ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 18/200\n",
      "7/7 - 1s - loss: 0.3286 - accuracy: 0.8482 - val_loss: 0.3321 - val_accuracy: 0.8696 - lr: 0.0036 - 1s/epoch - 174ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 19/200\n",
      "7/7 - 1s - loss: 0.3268 - accuracy: 0.8527 - val_loss: 0.3352 - val_accuracy: 0.8696 - lr: 0.0038 - 709ms/epoch - 101ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 20/200\n",
      "7/7 - 0s - loss: 0.2765 - accuracy: 0.8884 - val_loss: 0.4163 - val_accuracy: 0.7391 - lr: 0.0040 - 490ms/epoch - 70ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 21/200\n",
      "7/7 - 0s - loss: 0.4193 - accuracy: 0.8304 - val_loss: 0.3681 - val_accuracy: 0.8261 - lr: 0.0042 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 22/200\n",
      "7/7 - 0s - loss: 0.3405 - accuracy: 0.8571 - val_loss: 0.3513 - val_accuracy: 0.8696 - lr: 0.0044 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 23/200\n",
      "7/7 - 0s - loss: 0.3290 - accuracy: 0.8348 - val_loss: 0.4707 - val_accuracy: 0.7826 - lr: 0.0046 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 24/200\n",
      "7/7 - 0s - loss: 0.3725 - accuracy: 0.8259 - val_loss: 0.3448 - val_accuracy: 0.8696 - lr: 0.0048 - 388ms/epoch - 55ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 25/200\n",
      "7/7 - 1s - loss: 0.3521 - accuracy: 0.8304 - val_loss: 0.4038 - val_accuracy: 0.7826 - lr: 0.0050 - 581ms/epoch - 83ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 26/200\n",
      "7/7 - 0s - loss: 0.3354 - accuracy: 0.8527 - val_loss: 0.4089 - val_accuracy: 0.7826 - lr: 0.0052 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 27/200\n",
      "7/7 - 0s - loss: 0.3251 - accuracy: 0.8616 - val_loss: 0.4476 - val_accuracy: 0.7826 - lr: 0.0054 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 28/200\n",
      "7/7 - 0s - loss: 0.2828 - accuracy: 0.8661 - val_loss: 0.5907 - val_accuracy: 0.7826 - lr: 0.0056 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 29/200\n",
      "7/7 - 0s - loss: 0.3586 - accuracy: 0.8661 - val_loss: 0.6065 - val_accuracy: 0.7826 - lr: 0.0058 - 492ms/epoch - 70ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 30/200\n",
      "======================EPOCH 000030:======================\n",
      "    epoch      loss      acc  val_loss   val_acc      rocp        f1       rfa\n",
      "29   30.0  0.315222  0.84375  0.351696  0.913043  0.924242  0.916667  0.917671\n",
      "RFA : 0.9176712852535824\n",
      "7/7 - 1s - loss: 0.3152 - accuracy: 0.8438 - val_loss: 0.3517 - val_accuracy: 0.9130 - lr: 0.0060 - 569ms/epoch - 81ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 31/200\n",
      "======================EPOCH 000031:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "30   31.0  0.282638  0.870536  0.380544  0.956522  0.916667  0.956522  0.944565\n",
      "RFA : 0.9445652210194131\n",
      "7/7 - 0s - loss: 0.2826 - accuracy: 0.8705 - val_loss: 0.3805 - val_accuracy: 0.9565 - lr: 0.0062 - 357ms/epoch - 51ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 32/200\n",
      "7/7 - 0s - loss: 0.3255 - accuracy: 0.8348 - val_loss: 0.3534 - val_accuracy: 0.8261 - lr: 0.0064 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 33/200\n",
      "7/7 - 0s - loss: 0.3332 - accuracy: 0.8259 - val_loss: 0.5449 - val_accuracy: 0.7826 - lr: 0.0066 - 389ms/epoch - 56ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 34/200\n",
      "7/7 - 0s - loss: 0.3135 - accuracy: 0.8616 - val_loss: 0.5484 - val_accuracy: 0.7826 - lr: 0.0068 - 460ms/epoch - 66ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 35/200\n",
      "7/7 - 1s - loss: 0.2899 - accuracy: 0.8795 - val_loss: 0.4588 - val_accuracy: 0.7826 - lr: 0.0070 - 1s/epoch - 151ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 36/200\n",
      "7/7 - 0s - loss: 0.3096 - accuracy: 0.8527 - val_loss: 0.4208 - val_accuracy: 0.8261 - lr: 0.0072 - 377ms/epoch - 54ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 37/200\n",
      "7/7 - 0s - loss: 0.3037 - accuracy: 0.8705 - val_loss: 0.3796 - val_accuracy: 0.8696 - lr: 0.0074 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 38/200\n",
      "7/7 - 0s - loss: 0.3013 - accuracy: 0.8616 - val_loss: 0.4095 - val_accuracy: 0.9130 - lr: 0.0076 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 39/200\n",
      "7/7 - 0s - loss: 0.2792 - accuracy: 0.8795 - val_loss: 0.4468 - val_accuracy: 0.8696 - lr: 0.0078 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 40/200\n",
      "7/7 - 0s - loss: 0.2700 - accuracy: 0.8839 - val_loss: 0.4335 - val_accuracy: 0.9130 - lr: 0.0080 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 41/200\n",
      "7/7 - 0s - loss: 0.2824 - accuracy: 0.8929 - val_loss: 0.4395 - val_accuracy: 0.8261 - lr: 0.0082 - 470ms/epoch - 67ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 42/200\n",
      "7/7 - 2s - loss: 0.3317 - accuracy: 0.8393 - val_loss: 0.4056 - val_accuracy: 0.8696 - lr: 0.0084 - 2s/epoch - 215ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 43/200\n",
      "7/7 - 0s - loss: 0.2920 - accuracy: 0.8661 - val_loss: 0.4267 - val_accuracy: 0.9130 - lr: 0.0086 - 437ms/epoch - 62ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 44/200\n",
      "7/7 - 0s - loss: 0.2995 - accuracy: 0.8750 - val_loss: 0.5259 - val_accuracy: 0.6957 - lr: 0.0088 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 45/200\n",
      "7/7 - 0s - loss: 0.3817 - accuracy: 0.8214 - val_loss: 0.3788 - val_accuracy: 0.9130 - lr: 0.0090 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 46/200\n",
      "7/7 - 1s - loss: 0.3340 - accuracy: 0.8705 - val_loss: 0.4459 - val_accuracy: 0.7826 - lr: 0.0092 - 1s/epoch - 212ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 47/200\n",
      "7/7 - 0s - loss: 0.4948 - accuracy: 0.7768 - val_loss: 0.4339 - val_accuracy: 0.8696 - lr: 0.0094 - 488ms/epoch - 70ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 48/200\n",
      "7/7 - 0s - loss: 0.5870 - accuracy: 0.8214 - val_loss: 0.9016 - val_accuracy: 0.6957 - lr: 0.0096 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 49/200\n",
      "7/7 - 1s - loss: 33.8317 - accuracy: 0.5179 - val_loss: 19.4722 - val_accuracy: 0.4783 - lr: 0.0098 - 876ms/epoch - 125ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 50/200\n",
      "7/7 - 1s - loss: 647.8339 - accuracy: 0.5045 - val_loss: 703.3383 - val_accuracy: 0.4783 - lr: 0.0100 - 503ms/epoch - 72ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 51/200\n",
      "7/7 - 0s - loss: 2850.6074 - accuracy: 0.4598 - val_loss: 890.0217 - val_accuracy: 0.5217 - lr: 0.0098 - 343ms/epoch - 49ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 52/200\n",
      "7/7 - 0s - loss: 219.8451 - accuracy: 0.4732 - val_loss: 58.3388 - val_accuracy: 0.4783 - lr: 0.0096 - 344ms/epoch - 49ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 53/200\n",
      "7/7 - 1s - loss: 19.2386 - accuracy: 0.5312 - val_loss: 7.0480 - val_accuracy: 0.4783 - lr: 0.0094 - 1s/epoch - 199ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 54/200\n",
      "7/7 - 1s - loss: 2.2700 - accuracy: 0.5625 - val_loss: 2.0990 - val_accuracy: 0.4783 - lr: 0.0092 - 502ms/epoch - 72ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 55/200\n",
      "7/7 - 0s - loss: 1.1056 - accuracy: 0.5357 - val_loss: 0.7634 - val_accuracy: 0.5217 - lr: 0.0090 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 56/200\n",
      "7/7 - 0s - loss: 0.8180 - accuracy: 0.6295 - val_loss: 0.5941 - val_accuracy: 0.6957 - lr: 0.0088 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 57/200\n",
      "7/7 - 0s - loss: 0.6665 - accuracy: 0.6562 - val_loss: 0.5946 - val_accuracy: 0.6957 - lr: 0.0086 - 437ms/epoch - 62ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 58/200\n",
      "7/7 - 0s - loss: 0.5266 - accuracy: 0.7500 - val_loss: 0.4670 - val_accuracy: 0.7826 - lr: 0.0084 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 59/200\n",
      "7/7 - 0s - loss: 0.5363 - accuracy: 0.7455 - val_loss: 0.4533 - val_accuracy: 0.8696 - lr: 0.0082 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 60/200\n",
      "7/7 - 0s - loss: 0.4734 - accuracy: 0.7902 - val_loss: 0.4583 - val_accuracy: 0.7826 - lr: 0.0080 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 61/200\n",
      "7/7 - 0s - loss: 0.4641 - accuracy: 0.7768 - val_loss: 0.4445 - val_accuracy: 0.8696 - lr: 0.0078 - 391ms/epoch - 56ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 62/200\n",
      "7/7 - 1s - loss: 0.4520 - accuracy: 0.8036 - val_loss: 0.4300 - val_accuracy: 0.8696 - lr: 0.0076 - 1s/epoch - 211ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 63/200\n",
      "7/7 - 0s - loss: 0.4872 - accuracy: 0.7768 - val_loss: 0.5116 - val_accuracy: 0.7391 - lr: 0.0074 - 458ms/epoch - 65ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 64/200\n",
      "7/7 - 0s - loss: 0.4769 - accuracy: 0.7902 - val_loss: 0.4642 - val_accuracy: 0.7391 - lr: 0.0072 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 65/200\n",
      "7/7 - 0s - loss: 0.4492 - accuracy: 0.7902 - val_loss: 0.5148 - val_accuracy: 0.7391 - lr: 0.0070 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 66/200\n",
      "7/7 - 0s - loss: 0.3990 - accuracy: 0.8304 - val_loss: 0.4472 - val_accuracy: 0.9130 - lr: 0.0068 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 67/200\n",
      "7/7 - 1s - loss: 0.5105 - accuracy: 0.7679 - val_loss: 0.4239 - val_accuracy: 0.9130 - lr: 0.0066 - 1s/epoch - 193ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 68/200\n",
      "7/7 - 0s - loss: 0.4061 - accuracy: 0.8036 - val_loss: 0.4918 - val_accuracy: 0.7391 - lr: 0.0064 - 492ms/epoch - 70ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 69/200\n",
      "7/7 - 0s - loss: 0.3917 - accuracy: 0.7812 - val_loss: 0.5106 - val_accuracy: 0.7391 - lr: 0.0062 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 70/200\n",
      "7/7 - 0s - loss: 0.4420 - accuracy: 0.8125 - val_loss: 0.4424 - val_accuracy: 0.7826 - lr: 0.0060 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 71/200\n",
      "7/7 - 0s - loss: 0.4113 - accuracy: 0.8080 - val_loss: 0.5146 - val_accuracy: 0.7391 - lr: 0.0058 - 487ms/epoch - 70ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 72/200\n",
      "7/7 - 0s - loss: 0.5591 - accuracy: 0.7589 - val_loss: 0.4304 - val_accuracy: 0.8261 - lr: 0.0056 - 314ms/epoch - 45ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 73/200\n",
      "7/7 - 2s - loss: 0.4138 - accuracy: 0.7857 - val_loss: 0.4540 - val_accuracy: 0.7826 - lr: 0.0054 - 2s/epoch - 240ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 74/200\n",
      "7/7 - 0s - loss: 0.4150 - accuracy: 0.8080 - val_loss: 0.4257 - val_accuracy: 0.8261 - lr: 0.0052 - 497ms/epoch - 71ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 75/200\n",
      "7/7 - 0s - loss: 0.3783 - accuracy: 0.8125 - val_loss: 0.4511 - val_accuracy: 0.7826 - lr: 0.0050 - 400ms/epoch - 57ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 76/200\n",
      "7/7 - 0s - loss: 0.3597 - accuracy: 0.8348 - val_loss: 0.4340 - val_accuracy: 0.8261 - lr: 0.0048 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 77/200\n",
      "7/7 - 0s - loss: 0.3865 - accuracy: 0.8125 - val_loss: 0.4348 - val_accuracy: 0.7826 - lr: 0.0046 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 78/200\n",
      "7/7 - 1s - loss: 0.3479 - accuracy: 0.8438 - val_loss: 0.4392 - val_accuracy: 0.7826 - lr: 0.0044 - 587ms/epoch - 84ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 79/200\n",
      "7/7 - 0s - loss: 0.3418 - accuracy: 0.8393 - val_loss: 0.4533 - val_accuracy: 0.7826 - lr: 0.0042 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 80/200\n",
      "7/7 - 0s - loss: 0.4053 - accuracy: 0.8393 - val_loss: 0.4346 - val_accuracy: 0.8261 - lr: 0.0040 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 81/200\n",
      "7/7 - 0s - loss: 0.3828 - accuracy: 0.8527 - val_loss: 0.4922 - val_accuracy: 0.7826 - lr: 0.0038 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 82/200\n",
      "7/7 - 1s - loss: 0.4008 - accuracy: 0.8036 - val_loss: 0.4454 - val_accuracy: 0.8261 - lr: 0.0036 - 500ms/epoch - 71ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 83/200\n",
      "7/7 - 1s - loss: 0.4151 - accuracy: 0.8170 - val_loss: 0.4359 - val_accuracy: 0.8261 - lr: 0.0034 - 675ms/epoch - 96ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 84/200\n",
      "7/7 - 0s - loss: 0.3849 - accuracy: 0.8616 - val_loss: 0.4770 - val_accuracy: 0.7826 - lr: 0.0032 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 85/200\n",
      "7/7 - 0s - loss: 0.3027 - accuracy: 0.8705 - val_loss: 0.4423 - val_accuracy: 0.8696 - lr: 0.0030 - 322ms/epoch - 46ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 86/200\n",
      "7/7 - 1s - loss: 0.3987 - accuracy: 0.8393 - val_loss: 0.5051 - val_accuracy: 0.7826 - lr: 0.0028 - 1s/epoch - 146ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 87/200\n",
      "7/7 - 0s - loss: 0.4140 - accuracy: 0.8393 - val_loss: 0.4837 - val_accuracy: 0.7826 - lr: 0.0026 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 88/200\n",
      "7/7 - 0s - loss: 0.3912 - accuracy: 0.8170 - val_loss: 0.4420 - val_accuracy: 0.8696 - lr: 0.0024 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 89/200\n",
      "7/7 - 1s - loss: 0.4003 - accuracy: 0.8571 - val_loss: 0.4425 - val_accuracy: 0.7826 - lr: 0.0022 - 503ms/epoch - 72ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 90/200\n",
      "7/7 - 1s - loss: 0.3769 - accuracy: 0.8571 - val_loss: 0.4301 - val_accuracy: 0.8261 - lr: 0.0020 - 1s/epoch - 190ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 91/200\n",
      "7/7 - 0s - loss: 0.3727 - accuracy: 0.8170 - val_loss: 0.4323 - val_accuracy: 0.8261 - lr: 0.0018 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 92/200\n",
      "7/7 - 0s - loss: 0.3593 - accuracy: 0.8661 - val_loss: 0.4608 - val_accuracy: 0.7826 - lr: 0.0016 - 498ms/epoch - 71ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 93/200\n",
      "7/7 - 0s - loss: 0.3332 - accuracy: 0.8571 - val_loss: 0.4400 - val_accuracy: 0.7826 - lr: 0.0014 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 94/200\n",
      "7/7 - 0s - loss: 0.3077 - accuracy: 0.8705 - val_loss: 0.4429 - val_accuracy: 0.7826 - lr: 0.0012 - 331ms/epoch - 47ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 95/200\n",
      "7/7 - 0s - loss: 0.3702 - accuracy: 0.8839 - val_loss: 0.4362 - val_accuracy: 0.8261 - lr: 0.0010 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 96/200\n",
      "7/7 - 0s - loss: 0.3388 - accuracy: 0.8884 - val_loss: 0.4377 - val_accuracy: 0.8261 - lr: 8.0009e-04 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 97/200\n",
      "7/7 - 1s - loss: 0.3979 - accuracy: 0.8348 - val_loss: 0.4331 - val_accuracy: 0.8261 - lr: 6.0009e-04 - 503ms/epoch - 72ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 98/200\n",
      "7/7 - 2s - loss: 0.3955 - accuracy: 0.8304 - val_loss: 0.4521 - val_accuracy: 0.7826 - lr: 4.0010e-04 - 2s/epoch - 222ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 99/200\n",
      "7/7 - 0s - loss: 0.3539 - accuracy: 0.8438 - val_loss: 0.4472 - val_accuracy: 0.7826 - lr: 2.0010e-04 - 484ms/epoch - 69ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 100/200\n",
      "7/7 - 0s - loss: 0.3304 - accuracy: 0.8616 - val_loss: 0.4440 - val_accuracy: 0.7826 - lr: 1.0000e-07 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00000010\n",
      "Epoch 101/200\n",
      "7/7 - 0s - loss: 0.3664 - accuracy: 0.8348 - val_loss: 0.4415 - val_accuracy: 0.7826 - lr: 2.0010e-04 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 102/200\n",
      "7/7 - 1s - loss: 0.3552 - accuracy: 0.8304 - val_loss: 0.4419 - val_accuracy: 0.7826 - lr: 4.0010e-04 - 1s/epoch - 179ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 103/200\n",
      "7/7 - 0s - loss: 0.3091 - accuracy: 0.8750 - val_loss: 0.4468 - val_accuracy: 0.7826 - lr: 6.0009e-04 - 499ms/epoch - 71ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 104/200\n",
      "7/7 - 0s - loss: 0.3599 - accuracy: 0.8393 - val_loss: 0.4444 - val_accuracy: 0.7826 - lr: 8.0009e-04 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 105/200\n",
      "7/7 - 0s - loss: 0.3395 - accuracy: 0.8661 - val_loss: 0.4462 - val_accuracy: 0.7826 - lr: 0.0010 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 106/200\n",
      "7/7 - 0s - loss: 0.3219 - accuracy: 0.8661 - val_loss: 0.4300 - val_accuracy: 0.8696 - lr: 0.0012 - 437ms/epoch - 62ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 107/200\n",
      "7/7 - 0s - loss: 0.4093 - accuracy: 0.7991 - val_loss: 0.4470 - val_accuracy: 0.7826 - lr: 0.0014 - 426ms/epoch - 61ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 108/200\n",
      "7/7 - 0s - loss: 0.4400 - accuracy: 0.8393 - val_loss: 0.4416 - val_accuracy: 0.7826 - lr: 0.0016 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 109/200\n",
      "7/7 - 0s - loss: 0.3215 - accuracy: 0.8705 - val_loss: 0.4412 - val_accuracy: 0.7826 - lr: 0.0018 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 110/200\n",
      "7/7 - 1s - loss: 0.3719 - accuracy: 0.8438 - val_loss: 0.4327 - val_accuracy: 0.8261 - lr: 0.0020 - 580ms/epoch - 83ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 111/200\n",
      "7/7 - 0s - loss: 0.4013 - accuracy: 0.8348 - val_loss: 0.4498 - val_accuracy: 0.7826 - lr: 0.0022 - 489ms/epoch - 70ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 112/200\n",
      "7/7 - 2s - loss: 0.3942 - accuracy: 0.8036 - val_loss: 0.5047 - val_accuracy: 0.7826 - lr: 0.0024 - 2s/epoch - 226ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 113/200\n",
      "7/7 - 0s - loss: 0.4072 - accuracy: 0.8438 - val_loss: 0.4288 - val_accuracy: 0.8261 - lr: 0.0026 - 482ms/epoch - 69ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 114/200\n",
      "7/7 - 0s - loss: 0.4781 - accuracy: 0.8214 - val_loss: 0.4256 - val_accuracy: 0.8261 - lr: 0.0028 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 115/200\n",
      "7/7 - 0s - loss: 0.3362 - accuracy: 0.8661 - val_loss: 0.4598 - val_accuracy: 0.7826 - lr: 0.0030 - 439ms/epoch - 63ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 116/200\n",
      "7/7 - 0s - loss: 0.3800 - accuracy: 0.8438 - val_loss: 0.4252 - val_accuracy: 0.8261 - lr: 0.0032 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 117/200\n",
      "7/7 - 1s - loss: 0.3713 - accuracy: 0.8393 - val_loss: 0.4337 - val_accuracy: 0.7826 - lr: 0.0034 - 532ms/epoch - 76ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 118/200\n",
      "7/7 - 0s - loss: 0.3978 - accuracy: 0.8036 - val_loss: 0.5082 - val_accuracy: 0.7826 - lr: 0.0036 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 119/200\n",
      "7/7 - 1s - loss: 0.4065 - accuracy: 0.8125 - val_loss: 0.4669 - val_accuracy: 0.7826 - lr: 0.0038 - 855ms/epoch - 122ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 120/200\n",
      "7/7 - 0s - loss: 0.4065 - accuracy: 0.8393 - val_loss: 0.4378 - val_accuracy: 0.8261 - lr: 0.0040 - 488ms/epoch - 70ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 121/200\n",
      "7/7 - 0s - loss: 0.3944 - accuracy: 0.8348 - val_loss: 0.4418 - val_accuracy: 0.7826 - lr: 0.0042 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 122/200\n",
      "7/7 - 0s - loss: 0.3740 - accuracy: 0.8259 - val_loss: 0.4437 - val_accuracy: 0.7826 - lr: 0.0044 - 319ms/epoch - 46ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 123/200\n",
      "7/7 - 0s - loss: 0.3307 - accuracy: 0.8571 - val_loss: 0.4320 - val_accuracy: 0.8696 - lr: 0.0046 - 274ms/epoch - 39ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 124/200\n",
      "7/7 - 0s - loss: 0.3001 - accuracy: 0.8616 - val_loss: 0.5774 - val_accuracy: 0.7391 - lr: 0.0048 - 261ms/epoch - 37ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 125/200\n",
      "7/7 - 0s - loss: 0.3624 - accuracy: 0.8259 - val_loss: 0.4558 - val_accuracy: 0.7826 - lr: 0.0050 - 271ms/epoch - 39ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 126/200\n",
      "7/7 - 1s - loss: 0.3589 - accuracy: 0.8616 - val_loss: 0.5004 - val_accuracy: 0.7826 - lr: 0.0052 - 1s/epoch - 190ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 127/200\n",
      "7/7 - 0s - loss: 0.3388 - accuracy: 0.8571 - val_loss: 0.4451 - val_accuracy: 0.7826 - lr: 0.0054 - 343ms/epoch - 49ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 128/200\n",
      "7/7 - 0s - loss: 0.3693 - accuracy: 0.8348 - val_loss: 0.4520 - val_accuracy: 0.7826 - lr: 0.0056 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 129/200\n",
      "7/7 - 2s - loss: 0.3345 - accuracy: 0.8750 - val_loss: 0.4412 - val_accuracy: 0.8261 - lr: 0.0058 - 2s/epoch - 214ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 130/200\n",
      "7/7 - 0s - loss: 0.3978 - accuracy: 0.8438 - val_loss: 0.4732 - val_accuracy: 0.7391 - lr: 0.0060 - 321ms/epoch - 46ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 131/200\n",
      "7/7 - 0s - loss: 0.3361 - accuracy: 0.8438 - val_loss: 0.5645 - val_accuracy: 0.7391 - lr: 0.0062 - 384ms/epoch - 55ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 132/200\n",
      "7/7 - 2s - loss: 0.2880 - accuracy: 0.8661 - val_loss: 0.4927 - val_accuracy: 0.6957 - lr: 0.0064 - 2s/epoch - 229ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 133/200\n",
      "7/7 - 0s - loss: 0.4201 - accuracy: 0.8170 - val_loss: 0.4373 - val_accuracy: 0.8696 - lr: 0.0066 - 359ms/epoch - 51ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 134/200\n",
      "7/7 - 0s - loss: 0.3642 - accuracy: 0.8571 - val_loss: 0.4324 - val_accuracy: 0.8696 - lr: 0.0068 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 135/200\n",
      "7/7 - 0s - loss: 0.3300 - accuracy: 0.8482 - val_loss: 0.5158 - val_accuracy: 0.7826 - lr: 0.0070 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 136/200\n",
      "7/7 - 0s - loss: 0.3801 - accuracy: 0.8393 - val_loss: 0.5045 - val_accuracy: 0.7826 - lr: 0.0072 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 137/200\n",
      "7/7 - 0s - loss: 0.3798 - accuracy: 0.8304 - val_loss: 0.4624 - val_accuracy: 0.8261 - lr: 0.0074 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 138/200\n",
      "7/7 - 0s - loss: 0.3464 - accuracy: 0.8527 - val_loss: 0.5826 - val_accuracy: 0.7826 - lr: 0.0076 - 495ms/epoch - 71ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 139/200\n",
      "7/7 - 1s - loss: 0.3050 - accuracy: 0.8795 - val_loss: 0.4433 - val_accuracy: 0.8261 - lr: 0.0078 - 1s/epoch - 188ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 140/200\n",
      "7/7 - 1s - loss: 0.3234 - accuracy: 0.8393 - val_loss: 0.4534 - val_accuracy: 0.8696 - lr: 0.0080 - 547ms/epoch - 78ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 141/200\n",
      "7/7 - 0s - loss: 0.3502 - accuracy: 0.8661 - val_loss: 0.4944 - val_accuracy: 0.7826 - lr: 0.0082 - 487ms/epoch - 70ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 142/200\n",
      "7/7 - 0s - loss: 0.2691 - accuracy: 0.8705 - val_loss: 0.6272 - val_accuracy: 0.7391 - lr: 0.0084 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 143/200\n",
      "7/7 - 0s - loss: 0.2819 - accuracy: 0.8661 - val_loss: 0.5239 - val_accuracy: 0.7826 - lr: 0.0086 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 144/200\n",
      "7/7 - 0s - loss: 0.3300 - accuracy: 0.8795 - val_loss: 0.5332 - val_accuracy: 0.7826 - lr: 0.0088 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 145/200\n",
      "7/7 - 0s - loss: 0.3561 - accuracy: 0.8527 - val_loss: 0.4364 - val_accuracy: 0.8261 - lr: 0.0090 - 493ms/epoch - 70ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 146/200\n",
      "7/7 - 0s - loss: 0.3927 - accuracy: 0.8214 - val_loss: 0.6602 - val_accuracy: 0.7391 - lr: 0.0092 - 406ms/epoch - 58ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 147/200\n",
      "7/7 - 0s - loss: 0.3481 - accuracy: 0.8438 - val_loss: 0.4345 - val_accuracy: 0.7826 - lr: 0.0094 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 148/200\n",
      "7/7 - 0s - loss: 0.3128 - accuracy: 0.8705 - val_loss: 0.4283 - val_accuracy: 0.8696 - lr: 0.0096 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 149/200\n",
      "7/7 - 0s - loss: 0.3855 - accuracy: 0.7946 - val_loss: 0.6718 - val_accuracy: 0.7391 - lr: 0.0098 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 150/200\n",
      "7/7 - 0s - loss: 0.3525 - accuracy: 0.8393 - val_loss: 0.4403 - val_accuracy: 0.7826 - lr: 0.0100 - 452ms/epoch - 65ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 151/200\n",
      "7/7 - 2s - loss: 0.3763 - accuracy: 0.8393 - val_loss: 0.5188 - val_accuracy: 0.7826 - lr: 0.0098 - 2s/epoch - 224ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 152/200\n",
      "7/7 - 1s - loss: 0.3407 - accuracy: 0.8661 - val_loss: 0.9036 - val_accuracy: 0.7826 - lr: 0.0096 - 1s/epoch - 174ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 153/200\n",
      "7/7 - 0s - loss: 0.3334 - accuracy: 0.8750 - val_loss: 0.4215 - val_accuracy: 0.8696 - lr: 0.0094 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 154/200\n",
      "7/7 - 0s - loss: 0.3384 - accuracy: 0.8705 - val_loss: 0.4316 - val_accuracy: 0.8696 - lr: 0.0092 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 155/200\n",
      "7/7 - 0s - loss: 0.3350 - accuracy: 0.8393 - val_loss: 0.5119 - val_accuracy: 0.7826 - lr: 0.0090 - 384ms/epoch - 55ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 156/200\n",
      "7/7 - 1s - loss: 0.3432 - accuracy: 0.8750 - val_loss: 0.4356 - val_accuracy: 0.7826 - lr: 0.0088 - 997ms/epoch - 142ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 157/200\n",
      "7/7 - 0s - loss: 0.3712 - accuracy: 0.8705 - val_loss: 0.4304 - val_accuracy: 0.8696 - lr: 0.0086 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 158/200\n",
      "7/7 - 0s - loss: 0.3009 - accuracy: 0.8571 - val_loss: 0.6077 - val_accuracy: 0.7391 - lr: 0.0084 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 159/200\n",
      "7/7 - 0s - loss: 0.3295 - accuracy: 0.8482 - val_loss: 0.8185 - val_accuracy: 0.7826 - lr: 0.0082 - 487ms/epoch - 70ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 160/200\n",
      "7/7 - 0s - loss: 0.3261 - accuracy: 0.8616 - val_loss: 0.4847 - val_accuracy: 0.7826 - lr: 0.0080 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 161/200\n",
      "7/7 - 1s - loss: 0.2955 - accuracy: 0.8839 - val_loss: 0.4500 - val_accuracy: 0.8261 - lr: 0.0078 - 759ms/epoch - 108ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 162/200\n",
      "7/7 - 1s - loss: 0.2750 - accuracy: 0.9018 - val_loss: 0.4972 - val_accuracy: 0.7826 - lr: 0.0076 - 551ms/epoch - 79ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 163/200\n",
      "7/7 - 1s - loss: 0.2637 - accuracy: 0.8795 - val_loss: 0.4570 - val_accuracy: 0.8696 - lr: 0.0074 - 1s/epoch - 143ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 164/200\n",
      "7/7 - 1s - loss: 0.2534 - accuracy: 0.8929 - val_loss: 0.4877 - val_accuracy: 0.7826 - lr: 0.0072 - 580ms/epoch - 83ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 165/200\n",
      "7/7 - 0s - loss: 0.3583 - accuracy: 0.8304 - val_loss: 0.4352 - val_accuracy: 0.8696 - lr: 0.0070 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 166/200\n",
      "7/7 - 1s - loss: 0.3036 - accuracy: 0.8839 - val_loss: 0.4275 - val_accuracy: 0.8696 - lr: 0.0068 - 1s/epoch - 168ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 167/200\n",
      "7/7 - 1s - loss: 0.4019 - accuracy: 0.8616 - val_loss: 0.4134 - val_accuracy: 0.8696 - lr: 0.0066 - 889ms/epoch - 127ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 168/200\n",
      "7/7 - 1s - loss: 0.3967 - accuracy: 0.8080 - val_loss: 0.5689 - val_accuracy: 0.7391 - lr: 0.0064 - 522ms/epoch - 75ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 169/200\n",
      "7/7 - 1s - loss: 0.4253 - accuracy: 0.8080 - val_loss: 0.4077 - val_accuracy: 0.7826 - lr: 0.0062 - 618ms/epoch - 88ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 170/200\n",
      "7/7 - 0s - loss: 0.3560 - accuracy: 0.8393 - val_loss: 0.3951 - val_accuracy: 0.8261 - lr: 0.0060 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 171/200\n",
      "7/7 - 1s - loss: 0.3019 - accuracy: 0.8795 - val_loss: 0.4225 - val_accuracy: 0.7826 - lr: 0.0058 - 532ms/epoch - 76ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 172/200\n",
      "7/7 - 0s - loss: 0.2668 - accuracy: 0.9018 - val_loss: 0.3974 - val_accuracy: 0.7826 - lr: 0.0056 - 358ms/epoch - 51ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 173/200\n",
      "7/7 - 0s - loss: 0.2992 - accuracy: 0.8795 - val_loss: 0.4024 - val_accuracy: 0.8696 - lr: 0.0054 - 358ms/epoch - 51ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 174/200\n",
      "7/7 - 0s - loss: 0.2630 - accuracy: 0.8929 - val_loss: 0.4315 - val_accuracy: 0.8261 - lr: 0.0052 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 175/200\n",
      "7/7 - 0s - loss: 0.3444 - accuracy: 0.8527 - val_loss: 0.4197 - val_accuracy: 0.8261 - lr: 0.0050 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 176/200\n",
      "7/7 - 0s - loss: 0.2882 - accuracy: 0.8750 - val_loss: 0.4313 - val_accuracy: 0.8261 - lr: 0.0048 - 489ms/epoch - 70ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 177/200\n",
      "7/7 - 0s - loss: 0.3324 - accuracy: 0.8393 - val_loss: 0.4280 - val_accuracy: 0.7826 - lr: 0.0046 - 353ms/epoch - 50ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 178/200\n",
      "7/7 - 0s - loss: 0.2869 - accuracy: 0.8839 - val_loss: 0.4252 - val_accuracy: 0.8696 - lr: 0.0044 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 179/200\n",
      "7/7 - 0s - loss: 0.3059 - accuracy: 0.8750 - val_loss: 0.5920 - val_accuracy: 0.7391 - lr: 0.0042 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 180/200\n",
      "7/7 - 1s - loss: 0.3013 - accuracy: 0.8750 - val_loss: 0.4334 - val_accuracy: 0.8261 - lr: 0.0040 - 1s/epoch - 188ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 181/200\n",
      "7/7 - 0s - loss: 0.2708 - accuracy: 0.9062 - val_loss: 0.4262 - val_accuracy: 0.8261 - lr: 0.0038 - 353ms/epoch - 50ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 182/200\n",
      "7/7 - 2s - loss: 0.3345 - accuracy: 0.8527 - val_loss: 0.4545 - val_accuracy: 0.8261 - lr: 0.0036 - 2s/epoch - 261ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 183/200\n",
      "7/7 - 0s - loss: 0.2476 - accuracy: 0.9152 - val_loss: 0.4247 - val_accuracy: 0.8261 - lr: 0.0034 - 490ms/epoch - 70ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 184/200\n",
      "7/7 - 0s - loss: 0.3504 - accuracy: 0.8304 - val_loss: 0.4250 - val_accuracy: 0.8696 - lr: 0.0032 - 427ms/epoch - 61ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 185/200\n",
      "7/7 - 0s - loss: 0.2968 - accuracy: 0.8795 - val_loss: 0.4767 - val_accuracy: 0.7826 - lr: 0.0030 - 351ms/epoch - 50ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 186/200\n",
      "7/7 - 2s - loss: 0.2801 - accuracy: 0.8839 - val_loss: 0.4339 - val_accuracy: 0.8261 - lr: 0.0028 - 2s/epoch - 232ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 187/200\n",
      "7/7 - 0s - loss: 0.2504 - accuracy: 0.9018 - val_loss: 0.4572 - val_accuracy: 0.8261 - lr: 0.0026 - 490ms/epoch - 70ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 188/200\n",
      "7/7 - 0s - loss: 0.2869 - accuracy: 0.8929 - val_loss: 0.4548 - val_accuracy: 0.8261 - lr: 0.0024 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 189/200\n",
      "7/7 - 0s - loss: 0.2594 - accuracy: 0.8929 - val_loss: 0.4597 - val_accuracy: 0.8261 - lr: 0.0022 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 190/200\n",
      "7/7 - 1s - loss: 0.2723 - accuracy: 0.8795 - val_loss: 0.4588 - val_accuracy: 0.8261 - lr: 0.0020 - 1s/epoch - 163ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 191/200\n",
      "7/7 - 0s - loss: 0.2440 - accuracy: 0.9107 - val_loss: 0.4352 - val_accuracy: 0.8696 - lr: 0.0018 - 487ms/epoch - 70ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 192/200\n",
      "7/7 - 0s - loss: 0.2982 - accuracy: 0.8884 - val_loss: 0.4649 - val_accuracy: 0.8261 - lr: 0.0016 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 193/200\n",
      "7/7 - 1s - loss: 0.2625 - accuracy: 0.9062 - val_loss: 0.4372 - val_accuracy: 0.8696 - lr: 0.0014 - 1s/epoch - 166ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 194/200\n",
      "7/7 - 0s - loss: 0.2551 - accuracy: 0.8973 - val_loss: 0.4575 - val_accuracy: 0.8261 - lr: 0.0012 - 482ms/epoch - 69ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 195/200\n",
      "7/7 - 1s - loss: 0.2630 - accuracy: 0.9062 - val_loss: 0.4463 - val_accuracy: 0.8261 - lr: 0.0010 - 1s/epoch - 196ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 196/200\n",
      "7/7 - 0s - loss: 0.2445 - accuracy: 0.9062 - val_loss: 0.4412 - val_accuracy: 0.8696 - lr: 8.0009e-04 - 497ms/epoch - 71ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 197/200\n",
      "7/7 - 0s - loss: 0.2982 - accuracy: 0.8750 - val_loss: 0.4435 - val_accuracy: 0.8696 - lr: 6.0009e-04 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 198/200\n",
      "7/7 - 0s - loss: 0.2589 - accuracy: 0.9152 - val_loss: 0.4618 - val_accuracy: 0.8261 - lr: 4.0010e-04 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 199/200\n",
      "7/7 - 0s - loss: 0.2744 - accuracy: 0.8750 - val_loss: 0.4494 - val_accuracy: 0.8696 - lr: 2.0010e-04 - 368ms/epoch - 53ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 200/200\n",
      "7/7 - 1s - loss: 0.2368 - accuracy: 0.8973 - val_loss: 0.4524 - val_accuracy: 0.8261 - lr: 1.0000e-07 - 1s/epoch - 154ms/step\n",
      "\n",
      "======================BEST:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "30   31.0  0.282638  0.870536  0.380544  0.956522  0.916667  0.956522  0.944565\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.75      0.82        12\n",
      "           1       0.77      0.91      0.83        11\n",
      "\n",
      "    accuracy                           0.83        23\n",
      "   macro avg       0.83      0.83      0.83        23\n",
      "weighted avg       0.84      0.83      0.83        23\n",
      "\n",
      "[[ 9  3]\n",
      " [ 1 10]]\n",
      "ACCURACY: 0.8260869565217391\n",
      "PRECISION: 0.7692307692307693\n",
      "RECALL: 0.9090909090909091\n",
      "F1: 0.8333333333333333\n",
      "ROC_AUC(Pr.): 0.8939393939393938\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.96        23\n",
      "   macro avg       0.96      0.96      0.96        23\n",
      "weighted avg       0.96      0.96      0.96        23\n",
      "\n",
      "[[11  1]\n",
      " [ 0 11]]\n",
      "ACCURACY: 0.9565217391304348\n",
      "PRECISION: 0.9166666666666666\n",
      "RECALL: 1.0\n",
      "F1: 0.9565217391304348\n",
      "ROC_AUC(Pr.): 0.9166666666666666\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.96        23\n",
      "   macro avg       0.96      0.96      0.96        23\n",
      "weighted avg       0.96      0.96      0.96        23\n",
      "\n",
      "[[11  1]\n",
      " [ 0 11]]\n",
      "ACCURACY: 0.9565217391304348\n",
      "PRECISION: 0.9166666666666666\n",
      "RECALL: 1.0\n",
      "F1: 0.9565217391304348\n",
      "ROC_AUC(Pr.): 0.9166666666666666\n",
      "\n",
      "=====================CV[9]========================\n",
      "\n",
      "207 train samples\n",
      "23 test samples\n",
      "\n",
      "Over sampling: 207 -> 208\n",
      "Add train samples: 16\n",
      "224 new train samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\tf28\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.00000010\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_28484\\4287042063.py:159: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================EPOCH 000001:======================\n",
      "   epoch      loss       acc  val_loss   val_acc  rocp        f1       rfa\n",
      "0    1.0  0.692635  0.517857  0.692639  0.652174  0.75  0.733333  0.709928\n",
      "RFA : 0.7099275443951288\n",
      "7/7 - 6s - loss: 0.6926 - accuracy: 0.5179 - val_loss: 0.6926 - val_accuracy: 0.6522 - lr: 2.0010e-04 - 6s/epoch - 858ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 2/200\n",
      "7/7 - 0s - loss: 0.6933 - accuracy: 0.4688 - val_loss: 0.6920 - val_accuracy: 0.5652 - lr: 4.0010e-04 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 3/200\n",
      "======================EPOCH 000003:======================\n",
      "   epoch     loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "2    3.0  0.69096  0.580357  0.689952  0.652174  0.825758  0.733333  0.732655\n",
      "RFA : 0.7326548171224017\n",
      "7/7 - 0s - loss: 0.6910 - accuracy: 0.5804 - val_loss: 0.6900 - val_accuracy: 0.6522 - lr: 6.0009e-04 - 392ms/epoch - 56ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 4/200\n",
      "7/7 - 0s - loss: 0.6890 - accuracy: 0.5000 - val_loss: 0.6813 - val_accuracy: 0.6522 - lr: 8.0009e-04 - 378ms/epoch - 54ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 5/200\n",
      "7/7 - 0s - loss: 0.6673 - accuracy: 0.6607 - val_loss: 0.6442 - val_accuracy: 0.6522 - lr: 0.0010 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 6/200\n",
      "7/7 - 0s - loss: 0.5899 - accuracy: 0.7366 - val_loss: 0.5610 - val_accuracy: 0.6957 - lr: 0.0012 - 414ms/epoch - 59ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 7/200\n",
      "7/7 - 0s - loss: 0.4637 - accuracy: 0.7946 - val_loss: 0.4909 - val_accuracy: 0.6957 - lr: 0.0014 - 346ms/epoch - 49ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 8/200\n",
      "7/7 - 0s - loss: 0.4830 - accuracy: 0.7545 - val_loss: 0.4865 - val_accuracy: 0.6957 - lr: 0.0016 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 9/200\n",
      "======================EPOCH 000009:======================\n",
      "   epoch     loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "8    9.0  0.41287  0.790179  0.433137  0.869565  0.886364  0.869565  0.874605\n",
      "RFA : 0.8746047331057047\n",
      "7/7 - 0s - loss: 0.4129 - accuracy: 0.7902 - val_loss: 0.4331 - val_accuracy: 0.8696 - lr: 0.0018 - 363ms/epoch - 52ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 10/200\n",
      "======================EPOCH 000010:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp    f1       rfa\n",
      "9   10.0  0.388612  0.816964  0.417201  0.869565  0.886364  0.88  0.878257\n",
      "RFA : 0.8782569070187483\n",
      "7/7 - 0s - loss: 0.3886 - accuracy: 0.8170 - val_loss: 0.4172 - val_accuracy: 0.8696 - lr: 0.0020 - 371ms/epoch - 53ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 11/200\n",
      "7/7 - 0s - loss: 0.3758 - accuracy: 0.8170 - val_loss: 0.3732 - val_accuracy: 0.8696 - lr: 0.0022 - 410ms/epoch - 59ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 12/200\n",
      "7/7 - 0s - loss: 0.3667 - accuracy: 0.7991 - val_loss: 0.7559 - val_accuracy: 0.7391 - lr: 0.0024 - 358ms/epoch - 51ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 13/200\n",
      "7/7 - 0s - loss: 0.4869 - accuracy: 0.7946 - val_loss: 0.3870 - val_accuracy: 0.8261 - lr: 0.0026 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 14/200\n",
      "======================EPOCH 000014:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "13   14.0  0.356517  0.852679  0.366707  0.869565  0.916667  0.857143  0.879348\n",
      "RFA : 0.8793478161096573\n",
      "7/7 - 0s - loss: 0.3565 - accuracy: 0.8527 - val_loss: 0.3667 - val_accuracy: 0.8696 - lr: 0.0028 - 361ms/epoch - 52ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 15/200\n",
      "7/7 - 0s - loss: 0.3080 - accuracy: 0.8482 - val_loss: 0.4450 - val_accuracy: 0.6957 - lr: 0.0030 - 323ms/epoch - 46ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 16/200\n",
      "7/7 - 0s - loss: 0.3952 - accuracy: 0.8170 - val_loss: 0.3854 - val_accuracy: 0.8261 - lr: 0.0032 - 469ms/epoch - 67ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 17/200\n",
      "======================EPOCH 000017:======================\n",
      "    epoch      loss      acc  val_loss   val_acc      rocp        f1       rfa\n",
      "16   17.0  0.341053  0.84375  0.325582  0.869565  0.924242  0.869565  0.885968\n",
      "RFA : 0.885968369469341\n",
      "7/7 - 0s - loss: 0.3411 - accuracy: 0.8438 - val_loss: 0.3256 - val_accuracy: 0.8696 - lr: 0.0034 - 378ms/epoch - 54ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 18/200\n",
      "======================EPOCH 000018:======================\n",
      "    epoch      loss    acc  val_loss   val_acc      rocp        f1       rfa\n",
      "17   18.0  0.288936  0.875  0.324225  0.869565  0.939394  0.869565  0.890514\n",
      "RFA : 0.8905138240147956\n",
      "7/7 - 0s - loss: 0.2889 - accuracy: 0.8750 - val_loss: 0.3242 - val_accuracy: 0.8696 - lr: 0.0036 - 372ms/epoch - 53ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 19/200\n",
      "7/7 - 0s - loss: 0.3740 - accuracy: 0.8348 - val_loss: 0.3338 - val_accuracy: 0.8696 - lr: 0.0038 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 20/200\n",
      "======================EPOCH 000020:======================\n",
      "    epoch      loss      acc  val_loss   val_acc      rocp        f1      rfa\n",
      "19   20.0  0.323225  0.84375  0.340245  0.913043  0.924242  0.909091  0.91502\n",
      "RFA : 0.9150197701020675\n",
      "7/7 - 0s - loss: 0.3232 - accuracy: 0.8438 - val_loss: 0.3402 - val_accuracy: 0.9130 - lr: 0.0040 - 366ms/epoch - 52ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 21/200\n",
      "7/7 - 0s - loss: 0.2879 - accuracy: 0.8482 - val_loss: 0.5553 - val_accuracy: 0.6522 - lr: 0.0042 - 382ms/epoch - 55ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 22/200\n",
      "7/7 - 0s - loss: 0.3279 - accuracy: 0.8438 - val_loss: 0.4312 - val_accuracy: 0.8696 - lr: 0.0044 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 23/200\n",
      "7/7 - 0s - loss: 0.3865 - accuracy: 0.8214 - val_loss: 0.3427 - val_accuracy: 0.8696 - lr: 0.0046 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 24/200\n",
      "7/7 - 0s - loss: 0.3148 - accuracy: 0.8661 - val_loss: 0.3386 - val_accuracy: 0.8696 - lr: 0.0048 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 25/200\n",
      "7/7 - 0s - loss: 0.3530 - accuracy: 0.8304 - val_loss: 0.3670 - val_accuracy: 0.8696 - lr: 0.0050 - 351ms/epoch - 50ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 26/200\n",
      "7/7 - 1s - loss: 0.3213 - accuracy: 0.8571 - val_loss: 0.3289 - val_accuracy: 0.8696 - lr: 0.0052 - 500ms/epoch - 71ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 27/200\n",
      "7/7 - 0s - loss: 0.2945 - accuracy: 0.8929 - val_loss: 0.3984 - val_accuracy: 0.7826 - lr: 0.0054 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 28/200\n",
      "7/7 - 0s - loss: 0.3667 - accuracy: 0.8527 - val_loss: 0.4336 - val_accuracy: 0.8696 - lr: 0.0056 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 29/200\n",
      "7/7 - 0s - loss: 0.4026 - accuracy: 0.8393 - val_loss: 0.3678 - val_accuracy: 0.8696 - lr: 0.0058 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 30/200\n",
      "7/7 - 0s - loss: 0.2990 - accuracy: 0.8795 - val_loss: 0.5442 - val_accuracy: 0.7391 - lr: 0.0060 - 346ms/epoch - 49ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 31/200\n",
      "7/7 - 0s - loss: 0.2973 - accuracy: 0.8661 - val_loss: 0.3593 - val_accuracy: 0.8696 - lr: 0.0062 - 498ms/epoch - 71ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 32/200\n",
      "7/7 - 0s - loss: 0.2976 - accuracy: 0.8750 - val_loss: 0.3282 - val_accuracy: 0.8696 - lr: 0.0064 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 33/200\n",
      "7/7 - 0s - loss: 0.3529 - accuracy: 0.8527 - val_loss: 0.3038 - val_accuracy: 0.8696 - lr: 0.0066 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 34/200\n",
      "7/7 - 0s - loss: 0.3120 - accuracy: 0.8616 - val_loss: 0.3755 - val_accuracy: 0.7826 - lr: 0.0068 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 35/200\n",
      "7/7 - 0s - loss: 0.2788 - accuracy: 0.8884 - val_loss: 0.3162 - val_accuracy: 0.8696 - lr: 0.0070 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 36/200\n",
      "7/7 - 1s - loss: 0.2819 - accuracy: 0.8750 - val_loss: 0.3363 - val_accuracy: 0.8696 - lr: 0.0072 - 530ms/epoch - 76ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 37/200\n",
      "7/7 - 0s - loss: 0.3054 - accuracy: 0.8795 - val_loss: 0.3126 - val_accuracy: 0.8696 - lr: 0.0074 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 38/200\n",
      "7/7 - 0s - loss: 0.3047 - accuracy: 0.8750 - val_loss: 0.3233 - val_accuracy: 0.9130 - lr: 0.0076 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 39/200\n",
      "7/7 - 0s - loss: 0.3093 - accuracy: 0.8616 - val_loss: 0.3941 - val_accuracy: 0.8696 - lr: 0.0078 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 40/200\n",
      "7/7 - 0s - loss: 0.3143 - accuracy: 0.8348 - val_loss: 0.3569 - val_accuracy: 0.8696 - lr: 0.0080 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 41/200\n",
      "7/7 - 1s - loss: 0.3593 - accuracy: 0.8482 - val_loss: 0.3425 - val_accuracy: 0.8696 - lr: 0.0082 - 503ms/epoch - 72ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 42/200\n",
      "7/7 - 0s - loss: 0.2997 - accuracy: 0.8973 - val_loss: 0.3745 - val_accuracy: 0.8696 - lr: 0.0084 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 43/200\n",
      "7/7 - 0s - loss: 0.3758 - accuracy: 0.8616 - val_loss: 0.4132 - val_accuracy: 0.6957 - lr: 0.0086 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 44/200\n",
      "7/7 - 0s - loss: 0.3397 - accuracy: 0.8571 - val_loss: 0.3350 - val_accuracy: 0.8696 - lr: 0.0088 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 45/200\n",
      "7/7 - 0s - loss: 0.2998 - accuracy: 0.8750 - val_loss: 0.2487 - val_accuracy: 0.8696 - lr: 0.0090 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 46/200\n",
      "7/7 - 1s - loss: 0.3320 - accuracy: 0.8750 - val_loss: 0.3432 - val_accuracy: 0.8696 - lr: 0.0092 - 503ms/epoch - 72ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 47/200\n",
      "7/7 - 0s - loss: 0.3293 - accuracy: 0.8571 - val_loss: 0.3819 - val_accuracy: 0.8696 - lr: 0.0094 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 48/200\n",
      "7/7 - 0s - loss: 0.3145 - accuracy: 0.8527 - val_loss: 0.4391 - val_accuracy: 0.8261 - lr: 0.0096 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 49/200\n",
      "7/7 - 0s - loss: 0.3381 - accuracy: 0.8438 - val_loss: 0.3188 - val_accuracy: 0.8696 - lr: 0.0098 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 50/200\n",
      "7/7 - 0s - loss: 0.2812 - accuracy: 0.8705 - val_loss: 0.3195 - val_accuracy: 0.8696 - lr: 0.0100 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 51/200\n",
      "7/7 - 0s - loss: 0.2995 - accuracy: 0.8973 - val_loss: 0.3107 - val_accuracy: 0.8696 - lr: 0.0098 - 493ms/epoch - 70ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 52/200\n",
      "7/7 - 0s - loss: 0.3080 - accuracy: 0.8616 - val_loss: 0.3391 - val_accuracy: 0.8696 - lr: 0.0096 - 361ms/epoch - 52ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 53/200\n",
      "7/7 - 0s - loss: 0.2783 - accuracy: 0.8616 - val_loss: 0.4380 - val_accuracy: 0.8696 - lr: 0.0094 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 54/200\n",
      "======================EPOCH 000054:======================\n",
      "    epoch      loss       acc  val_loss   val_acc     rocp        f1       rfa\n",
      "53   54.0  0.317171  0.857143  0.271243  0.913043  0.94697  0.916667  0.924489\n",
      "RFA : 0.9244894670717645\n",
      "7/7 - 0s - loss: 0.3172 - accuracy: 0.8571 - val_loss: 0.2712 - val_accuracy: 0.9130 - lr: 0.0092 - 363ms/epoch - 52ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 55/200\n",
      "7/7 - 0s - loss: 0.3338 - accuracy: 0.8438 - val_loss: 0.3375 - val_accuracy: 0.8696 - lr: 0.0090 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 56/200\n",
      "7/7 - 0s - loss: 0.5419 - accuracy: 0.8036 - val_loss: 2.8587 - val_accuracy: 0.7826 - lr: 0.0088 - 458ms/epoch - 65ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 57/200\n",
      "7/7 - 0s - loss: 4.9472 - accuracy: 0.6607 - val_loss: 0.3979 - val_accuracy: 0.7826 - lr: 0.0086 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 58/200\n",
      "7/7 - 0s - loss: 228.2835 - accuracy: 0.5089 - val_loss: 139.0470 - val_accuracy: 0.5217 - lr: 0.0084 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 59/200\n",
      "7/7 - 0s - loss: 651.9079 - accuracy: 0.4732 - val_loss: 40.8089 - val_accuracy: 0.4783 - lr: 0.0082 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 60/200\n",
      "7/7 - 0s - loss: 32.4690 - accuracy: 0.4955 - val_loss: 13.5072 - val_accuracy: 0.4783 - lr: 0.0080 - 341ms/epoch - 49ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 61/200\n",
      "7/7 - 1s - loss: 6.0140 - accuracy: 0.5045 - val_loss: 0.8229 - val_accuracy: 0.5217 - lr: 0.0078 - 515ms/epoch - 74ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 62/200\n",
      "7/7 - 0s - loss: 1.2887 - accuracy: 0.4821 - val_loss: 0.6973 - val_accuracy: 0.5652 - lr: 0.0076 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 63/200\n",
      "7/7 - 0s - loss: 0.8279 - accuracy: 0.5938 - val_loss: 0.6913 - val_accuracy: 0.5217 - lr: 0.0074 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 64/200\n",
      "7/7 - 0s - loss: 0.8382 - accuracy: 0.4821 - val_loss: 0.6407 - val_accuracy: 0.5217 - lr: 0.0072 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 65/200\n",
      "7/7 - 0s - loss: 0.7530 - accuracy: 0.5312 - val_loss: 0.5725 - val_accuracy: 0.5652 - lr: 0.0070 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 66/200\n",
      "7/7 - 1s - loss: 0.6678 - accuracy: 0.6205 - val_loss: 0.5437 - val_accuracy: 0.7391 - lr: 0.0068 - 517ms/epoch - 74ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 67/200\n",
      "7/7 - 0s - loss: 0.6506 - accuracy: 0.6161 - val_loss: 0.4996 - val_accuracy: 0.8261 - lr: 0.0066 - 452ms/epoch - 65ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 68/200\n",
      "7/7 - 0s - loss: 0.6189 - accuracy: 0.6830 - val_loss: 0.4909 - val_accuracy: 0.7391 - lr: 0.0064 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 69/200\n",
      "7/7 - 0s - loss: 0.5106 - accuracy: 0.7411 - val_loss: 0.4646 - val_accuracy: 0.7391 - lr: 0.0062 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 70/200\n",
      "7/7 - 0s - loss: 0.5618 - accuracy: 0.7321 - val_loss: 0.4417 - val_accuracy: 0.8696 - lr: 0.0060 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 71/200\n",
      "7/7 - 0s - loss: 0.5353 - accuracy: 0.7188 - val_loss: 0.4650 - val_accuracy: 0.7826 - lr: 0.0058 - 392ms/epoch - 56ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 72/200\n",
      "7/7 - 0s - loss: 0.5615 - accuracy: 0.7366 - val_loss: 0.4429 - val_accuracy: 0.8261 - lr: 0.0056 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 73/200\n",
      "7/7 - 0s - loss: 0.5743 - accuracy: 0.7098 - val_loss: 0.4356 - val_accuracy: 0.8696 - lr: 0.0054 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 74/200\n",
      "7/7 - 0s - loss: 0.5750 - accuracy: 0.7277 - val_loss: 0.4314 - val_accuracy: 0.8696 - lr: 0.0052 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 75/200\n",
      "7/7 - 0s - loss: 0.5441 - accuracy: 0.7455 - val_loss: 0.4373 - val_accuracy: 0.8261 - lr: 0.0050 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 76/200\n",
      "7/7 - 1s - loss: 0.4828 - accuracy: 0.7723 - val_loss: 0.4184 - val_accuracy: 0.8261 - lr: 0.0048 - 506ms/epoch - 72ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 77/200\n",
      "7/7 - 0s - loss: 0.5371 - accuracy: 0.7589 - val_loss: 0.4242 - val_accuracy: 0.8261 - lr: 0.0046 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 78/200\n",
      "7/7 - 0s - loss: 0.4986 - accuracy: 0.7500 - val_loss: 0.4279 - val_accuracy: 0.8261 - lr: 0.0044 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 79/200\n",
      "7/7 - 0s - loss: 0.5170 - accuracy: 0.7411 - val_loss: 0.4092 - val_accuracy: 0.8696 - lr: 0.0042 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 80/200\n",
      "7/7 - 0s - loss: 0.5189 - accuracy: 0.7411 - val_loss: 0.4140 - val_accuracy: 0.8261 - lr: 0.0040 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 81/200\n",
      "7/7 - 0s - loss: 0.4481 - accuracy: 0.7634 - val_loss: 0.4226 - val_accuracy: 0.8696 - lr: 0.0038 - 496ms/epoch - 71ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 82/200\n",
      "7/7 - 0s - loss: 0.4805 - accuracy: 0.7812 - val_loss: 0.4036 - val_accuracy: 0.8261 - lr: 0.0036 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 83/200\n",
      "7/7 - 0s - loss: 0.5171 - accuracy: 0.7500 - val_loss: 0.4104 - val_accuracy: 0.8261 - lr: 0.0034 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 84/200\n",
      "7/7 - 0s - loss: 0.5225 - accuracy: 0.7500 - val_loss: 0.4046 - val_accuracy: 0.8261 - lr: 0.0032 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 85/200\n",
      "7/7 - 0s - loss: 0.4549 - accuracy: 0.8170 - val_loss: 0.4345 - val_accuracy: 0.7826 - lr: 0.0030 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 86/200\n",
      "7/7 - 1s - loss: 0.4724 - accuracy: 0.7902 - val_loss: 0.4110 - val_accuracy: 0.8696 - lr: 0.0028 - 500ms/epoch - 71ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 87/200\n",
      "7/7 - 0s - loss: 0.4209 - accuracy: 0.8080 - val_loss: 0.4199 - val_accuracy: 0.7826 - lr: 0.0026 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 88/200\n",
      "7/7 - 0s - loss: 0.4694 - accuracy: 0.7812 - val_loss: 0.3978 - val_accuracy: 0.8261 - lr: 0.0024 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 89/200\n",
      "7/7 - 0s - loss: 0.4499 - accuracy: 0.7723 - val_loss: 0.3945 - val_accuracy: 0.8696 - lr: 0.0022 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 90/200\n",
      "7/7 - 0s - loss: 0.5156 - accuracy: 0.7857 - val_loss: 0.3965 - val_accuracy: 0.8261 - lr: 0.0020 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 91/200\n",
      "7/7 - 0s - loss: 0.4120 - accuracy: 0.8080 - val_loss: 0.3945 - val_accuracy: 0.8696 - lr: 0.0018 - 478ms/epoch - 68ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 92/200\n",
      "7/7 - 0s - loss: 0.4318 - accuracy: 0.8170 - val_loss: 0.3943 - val_accuracy: 0.8696 - lr: 0.0016 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 93/200\n",
      "7/7 - 0s - loss: 0.4590 - accuracy: 0.7902 - val_loss: 0.3937 - val_accuracy: 0.8696 - lr: 0.0014 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 94/200\n",
      "7/7 - 0s - loss: 0.4917 - accuracy: 0.7500 - val_loss: 0.3960 - val_accuracy: 0.8261 - lr: 0.0012 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 95/200\n",
      "7/7 - 0s - loss: 0.4598 - accuracy: 0.7723 - val_loss: 0.3954 - val_accuracy: 0.8696 - lr: 0.0010 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 96/200\n",
      "7/7 - 1s - loss: 0.4536 - accuracy: 0.8036 - val_loss: 0.3952 - val_accuracy: 0.8696 - lr: 8.0009e-04 - 504ms/epoch - 72ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 97/200\n",
      "7/7 - 0s - loss: 0.4500 - accuracy: 0.7679 - val_loss: 0.3953 - val_accuracy: 0.8696 - lr: 6.0009e-04 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 98/200\n",
      "7/7 - 0s - loss: 0.4127 - accuracy: 0.8393 - val_loss: 0.3962 - val_accuracy: 0.8261 - lr: 4.0010e-04 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 99/200\n",
      "7/7 - 0s - loss: 0.4110 - accuracy: 0.8125 - val_loss: 0.3974 - val_accuracy: 0.8261 - lr: 2.0010e-04 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 100/200\n",
      "7/7 - 0s - loss: 0.4548 - accuracy: 0.7768 - val_loss: 0.3961 - val_accuracy: 0.8261 - lr: 1.0000e-07 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00000010\n",
      "Epoch 101/200\n",
      "7/7 - 1s - loss: 0.4437 - accuracy: 0.7946 - val_loss: 0.3961 - val_accuracy: 0.8261 - lr: 2.0010e-04 - 521ms/epoch - 74ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 102/200\n",
      "7/7 - 0s - loss: 0.4161 - accuracy: 0.8348 - val_loss: 0.3963 - val_accuracy: 0.8261 - lr: 4.0010e-04 - 343ms/epoch - 49ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 103/200\n",
      "7/7 - 0s - loss: 0.4911 - accuracy: 0.7902 - val_loss: 0.3946 - val_accuracy: 0.8696 - lr: 6.0009e-04 - 325ms/epoch - 46ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 104/200\n",
      "7/7 - 0s - loss: 0.3817 - accuracy: 0.8482 - val_loss: 0.4047 - val_accuracy: 0.8261 - lr: 8.0009e-04 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 105/200\n",
      "7/7 - 0s - loss: 0.3928 - accuracy: 0.8259 - val_loss: 0.3994 - val_accuracy: 0.8261 - lr: 0.0010 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 106/200\n",
      "7/7 - 1s - loss: 0.4113 - accuracy: 0.8304 - val_loss: 0.4021 - val_accuracy: 0.8261 - lr: 0.0012 - 512ms/epoch - 73ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 107/200\n",
      "7/7 - 0s - loss: 0.3935 - accuracy: 0.8348 - val_loss: 0.3929 - val_accuracy: 0.8696 - lr: 0.0014 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 108/200\n",
      "7/7 - 0s - loss: 0.4594 - accuracy: 0.7679 - val_loss: 0.3929 - val_accuracy: 0.8261 - lr: 0.0016 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 109/200\n",
      "7/7 - 0s - loss: 0.4113 - accuracy: 0.8170 - val_loss: 0.3924 - val_accuracy: 0.8261 - lr: 0.0018 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 110/200\n",
      "7/7 - 0s - loss: 0.4127 - accuracy: 0.7991 - val_loss: 0.4191 - val_accuracy: 0.7826 - lr: 0.0020 - 368ms/epoch - 53ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 111/200\n",
      "7/7 - 0s - loss: 0.4480 - accuracy: 0.8080 - val_loss: 0.3895 - val_accuracy: 0.8261 - lr: 0.0022 - 487ms/epoch - 70ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 112/200\n",
      "7/7 - 0s - loss: 0.4223 - accuracy: 0.7812 - val_loss: 0.3918 - val_accuracy: 0.8261 - lr: 0.0024 - 380ms/epoch - 54ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 113/200\n",
      "7/7 - 0s - loss: 0.4065 - accuracy: 0.8036 - val_loss: 0.3995 - val_accuracy: 0.8261 - lr: 0.0026 - 369ms/epoch - 53ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 114/200\n",
      "7/7 - 0s - loss: 0.4236 - accuracy: 0.8170 - val_loss: 0.4053 - val_accuracy: 0.7826 - lr: 0.0028 - 371ms/epoch - 53ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 115/200\n",
      "7/7 - 0s - loss: 0.3983 - accuracy: 0.8214 - val_loss: 0.3929 - val_accuracy: 0.8261 - lr: 0.0030 - 368ms/epoch - 53ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 116/200\n",
      "7/7 - 0s - loss: 0.3903 - accuracy: 0.8170 - val_loss: 0.3862 - val_accuracy: 0.8261 - lr: 0.0032 - 360ms/epoch - 51ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 117/200\n",
      "7/7 - 0s - loss: 0.4000 - accuracy: 0.8170 - val_loss: 0.3935 - val_accuracy: 0.8261 - lr: 0.0034 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 118/200\n",
      "7/7 - 0s - loss: 0.3896 - accuracy: 0.8170 - val_loss: 0.3878 - val_accuracy: 0.8261 - lr: 0.0036 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 119/200\n",
      "7/7 - 0s - loss: 0.3228 - accuracy: 0.8304 - val_loss: 0.4226 - val_accuracy: 0.7826 - lr: 0.0038 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 120/200\n",
      "7/7 - 0s - loss: 0.3629 - accuracy: 0.8348 - val_loss: 0.3976 - val_accuracy: 0.8261 - lr: 0.0040 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 121/200\n",
      "7/7 - 0s - loss: 0.4119 - accuracy: 0.8304 - val_loss: 0.3955 - val_accuracy: 0.8261 - lr: 0.0042 - 493ms/epoch - 70ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 122/200\n",
      "7/7 - 0s - loss: 0.3926 - accuracy: 0.8125 - val_loss: 0.4209 - val_accuracy: 0.7826 - lr: 0.0044 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 123/200\n",
      "7/7 - 0s - loss: 0.3863 - accuracy: 0.8348 - val_loss: 0.4262 - val_accuracy: 0.7826 - lr: 0.0046 - 352ms/epoch - 50ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 124/200\n",
      "7/7 - 0s - loss: 0.3329 - accuracy: 0.8348 - val_loss: 0.3878 - val_accuracy: 0.8696 - lr: 0.0048 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 125/200\n",
      "7/7 - 0s - loss: 0.3447 - accuracy: 0.8125 - val_loss: 0.4030 - val_accuracy: 0.8261 - lr: 0.0050 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 126/200\n",
      "7/7 - 0s - loss: 0.3612 - accuracy: 0.8125 - val_loss: 0.4639 - val_accuracy: 0.7391 - lr: 0.0052 - 446ms/epoch - 64ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 127/200\n",
      "7/7 - 0s - loss: 0.3294 - accuracy: 0.8616 - val_loss: 0.4140 - val_accuracy: 0.9130 - lr: 0.0054 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 128/200\n",
      "7/7 - 0s - loss: 0.3918 - accuracy: 0.8304 - val_loss: 0.3856 - val_accuracy: 0.8261 - lr: 0.0056 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 129/200\n",
      "7/7 - 0s - loss: 0.4458 - accuracy: 0.8170 - val_loss: 0.3857 - val_accuracy: 0.8696 - lr: 0.0058 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 130/200\n",
      "7/7 - 0s - loss: 0.3918 - accuracy: 0.7991 - val_loss: 0.3936 - val_accuracy: 0.8261 - lr: 0.0060 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 131/200\n",
      "7/7 - 1s - loss: 0.3480 - accuracy: 0.8304 - val_loss: 0.3971 - val_accuracy: 0.8261 - lr: 0.0062 - 505ms/epoch - 72ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 132/200\n",
      "7/7 - 0s - loss: 0.3205 - accuracy: 0.8438 - val_loss: 0.3799 - val_accuracy: 0.8261 - lr: 0.0064 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 133/200\n",
      "7/7 - 0s - loss: 0.3082 - accuracy: 0.8482 - val_loss: 0.4202 - val_accuracy: 0.7826 - lr: 0.0066 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 134/200\n",
      "7/7 - 1s - loss: 0.3201 - accuracy: 0.8616 - val_loss: 0.3815 - val_accuracy: 0.9130 - lr: 0.0068 - 1s/epoch - 188ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 135/200\n",
      "7/7 - 0s - loss: 0.4361 - accuracy: 0.8125 - val_loss: 0.3657 - val_accuracy: 0.8696 - lr: 0.0070 - 492ms/epoch - 70ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 136/200\n",
      "7/7 - 0s - loss: 0.3612 - accuracy: 0.8438 - val_loss: 0.4438 - val_accuracy: 0.7391 - lr: 0.0072 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 137/200\n",
      "7/7 - 0s - loss: 0.4050 - accuracy: 0.8080 - val_loss: 0.3959 - val_accuracy: 0.8696 - lr: 0.0074 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 138/200\n",
      "7/7 - 0s - loss: 0.3543 - accuracy: 0.8482 - val_loss: 0.3525 - val_accuracy: 0.8696 - lr: 0.0076 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 139/200\n",
      "7/7 - 0s - loss: 0.3525 - accuracy: 0.8482 - val_loss: 0.3474 - val_accuracy: 0.8261 - lr: 0.0078 - 486ms/epoch - 69ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 140/200\n",
      "7/7 - 0s - loss: 0.3150 - accuracy: 0.8839 - val_loss: 0.4242 - val_accuracy: 0.7391 - lr: 0.0080 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 141/200\n",
      "7/7 - 0s - loss: 0.3348 - accuracy: 0.8527 - val_loss: 0.6527 - val_accuracy: 0.6957 - lr: 0.0082 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 142/200\n",
      "7/7 - 0s - loss: 0.3388 - accuracy: 0.8170 - val_loss: 0.3397 - val_accuracy: 0.8696 - lr: 0.0084 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 143/200\n",
      "7/7 - 0s - loss: 0.3496 - accuracy: 0.8348 - val_loss: 0.3627 - val_accuracy: 0.7826 - lr: 0.0086 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 144/200\n",
      "7/7 - 0s - loss: 0.3159 - accuracy: 0.8750 - val_loss: 0.3482 - val_accuracy: 0.8261 - lr: 0.0088 - 494ms/epoch - 71ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 145/200\n",
      "7/7 - 0s - loss: 0.2857 - accuracy: 0.8750 - val_loss: 0.3317 - val_accuracy: 0.8261 - lr: 0.0090 - 331ms/epoch - 47ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 146/200\n",
      "7/7 - 0s - loss: 0.3480 - accuracy: 0.8616 - val_loss: 0.3402 - val_accuracy: 0.8261 - lr: 0.0092 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 147/200\n",
      "7/7 - 0s - loss: 0.2887 - accuracy: 0.8705 - val_loss: 0.4132 - val_accuracy: 0.7826 - lr: 0.0094 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 148/200\n",
      "7/7 - 0s - loss: 0.3315 - accuracy: 0.8438 - val_loss: 0.3151 - val_accuracy: 0.8696 - lr: 0.0096 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 149/200\n",
      "7/7 - 0s - loss: 0.3456 - accuracy: 0.8438 - val_loss: 0.3157 - val_accuracy: 0.8261 - lr: 0.0098 - 471ms/epoch - 67ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 150/200\n",
      "7/7 - 0s - loss: 0.3145 - accuracy: 0.8527 - val_loss: 0.3119 - val_accuracy: 0.8696 - lr: 0.0100 - 357ms/epoch - 51ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 151/200\n",
      "7/7 - 0s - loss: 0.2756 - accuracy: 0.8884 - val_loss: 0.4471 - val_accuracy: 0.7391 - lr: 0.0098 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 152/200\n",
      "7/7 - 0s - loss: 0.2810 - accuracy: 0.8705 - val_loss: 0.4158 - val_accuracy: 0.7826 - lr: 0.0096 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 153/200\n",
      "7/7 - 0s - loss: 0.2864 - accuracy: 0.8929 - val_loss: 0.3727 - val_accuracy: 0.7826 - lr: 0.0094 - 344ms/epoch - 49ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 154/200\n",
      "7/7 - 1s - loss: 0.3609 - accuracy: 0.8616 - val_loss: 0.3353 - val_accuracy: 0.8261 - lr: 0.0092 - 522ms/epoch - 75ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 155/200\n",
      "7/7 - 0s - loss: 0.3289 - accuracy: 0.8259 - val_loss: 0.3153 - val_accuracy: 0.9130 - lr: 0.0090 - 381ms/epoch - 54ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 156/200\n",
      "7/7 - 0s - loss: 0.3205 - accuracy: 0.8705 - val_loss: 0.3205 - val_accuracy: 0.8261 - lr: 0.0088 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 157/200\n",
      "7/7 - 0s - loss: 0.3517 - accuracy: 0.8482 - val_loss: 0.3270 - val_accuracy: 0.8261 - lr: 0.0086 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 158/200\n",
      "7/7 - 1s - loss: 0.2988 - accuracy: 0.8616 - val_loss: 0.3092 - val_accuracy: 0.8696 - lr: 0.0084 - 545ms/epoch - 78ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 159/200\n",
      "7/7 - 0s - loss: 0.2926 - accuracy: 0.8527 - val_loss: 0.3449 - val_accuracy: 0.9130 - lr: 0.0082 - 368ms/epoch - 53ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 160/200\n",
      "7/7 - 0s - loss: 0.2775 - accuracy: 0.8884 - val_loss: 0.3158 - val_accuracy: 0.9130 - lr: 0.0080 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 161/200\n",
      "7/7 - 0s - loss: 0.3082 - accuracy: 0.8527 - val_loss: 0.3044 - val_accuracy: 0.8696 - lr: 0.0078 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 162/200\n",
      "7/7 - 0s - loss: 0.3295 - accuracy: 0.8616 - val_loss: 0.3951 - val_accuracy: 0.7826 - lr: 0.0076 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 163/200\n",
      "7/7 - 0s - loss: 0.3292 - accuracy: 0.8527 - val_loss: 0.3112 - val_accuracy: 0.8696 - lr: 0.0074 - 485ms/epoch - 69ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 164/200\n",
      "7/7 - 0s - loss: 0.2861 - accuracy: 0.8884 - val_loss: 0.3174 - val_accuracy: 0.8261 - lr: 0.0072 - 342ms/epoch - 49ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 165/200\n",
      "7/7 - 0s - loss: 0.3029 - accuracy: 0.8884 - val_loss: 0.3281 - val_accuracy: 0.9130 - lr: 0.0070 - 351ms/epoch - 50ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 166/200\n",
      "7/7 - 0s - loss: 0.3144 - accuracy: 0.8705 - val_loss: 0.3210 - val_accuracy: 0.8261 - lr: 0.0068 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 167/200\n",
      "7/7 - 0s - loss: 0.2986 - accuracy: 0.8750 - val_loss: 0.3861 - val_accuracy: 0.7826 - lr: 0.0066 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 168/200\n",
      "7/7 - 0s - loss: 0.3100 - accuracy: 0.8750 - val_loss: 0.3595 - val_accuracy: 0.7826 - lr: 0.0064 - 459ms/epoch - 66ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 169/200\n",
      "7/7 - 0s - loss: 0.2434 - accuracy: 0.8973 - val_loss: 0.3558 - val_accuracy: 0.7826 - lr: 0.0062 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 170/200\n",
      "7/7 - 0s - loss: 0.2560 - accuracy: 0.9062 - val_loss: 0.3341 - val_accuracy: 0.9130 - lr: 0.0060 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 171/200\n",
      "7/7 - 0s - loss: 0.3173 - accuracy: 0.8705 - val_loss: 0.3217 - val_accuracy: 0.8696 - lr: 0.0058 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 172/200\n",
      "7/7 - 0s - loss: 0.3020 - accuracy: 0.8527 - val_loss: 0.3179 - val_accuracy: 0.8696 - lr: 0.0056 - 346ms/epoch - 49ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 173/200\n",
      "7/7 - 0s - loss: 0.3015 - accuracy: 0.8616 - val_loss: 0.3104 - val_accuracy: 0.8696 - lr: 0.0054 - 471ms/epoch - 67ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 174/200\n",
      "7/7 - 0s - loss: 0.2537 - accuracy: 0.9018 - val_loss: 0.3363 - val_accuracy: 0.7826 - lr: 0.0052 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 175/200\n",
      "7/7 - 0s - loss: 0.2597 - accuracy: 0.8929 - val_loss: 0.3348 - val_accuracy: 0.7826 - lr: 0.0050 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 176/200\n",
      "7/7 - 0s - loss: 0.2399 - accuracy: 0.8973 - val_loss: 0.3051 - val_accuracy: 0.8261 - lr: 0.0048 - 358ms/epoch - 51ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 177/200\n",
      "7/7 - 0s - loss: 0.2733 - accuracy: 0.9062 - val_loss: 0.3090 - val_accuracy: 0.8696 - lr: 0.0046 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 178/200\n",
      "7/7 - 0s - loss: 0.2803 - accuracy: 0.8839 - val_loss: 0.3081 - val_accuracy: 0.8696 - lr: 0.0044 - 475ms/epoch - 68ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 179/200\n",
      "7/7 - 0s - loss: 0.2849 - accuracy: 0.8884 - val_loss: 0.3174 - val_accuracy: 0.7826 - lr: 0.0042 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 180/200\n",
      "7/7 - 0s - loss: 0.2789 - accuracy: 0.8661 - val_loss: 0.3538 - val_accuracy: 0.7826 - lr: 0.0040 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 181/200\n",
      "7/7 - 0s - loss: 0.2669 - accuracy: 0.8839 - val_loss: 0.3589 - val_accuracy: 0.7826 - lr: 0.0038 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 182/200\n",
      "7/7 - 0s - loss: 0.2704 - accuracy: 0.8839 - val_loss: 0.3201 - val_accuracy: 0.9130 - lr: 0.0036 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 183/200\n",
      "7/7 - 0s - loss: 0.2735 - accuracy: 0.8839 - val_loss: 0.3645 - val_accuracy: 0.7826 - lr: 0.0034 - 490ms/epoch - 70ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 184/200\n",
      "7/7 - 0s - loss: 0.2924 - accuracy: 0.8884 - val_loss: 0.3206 - val_accuracy: 0.7826 - lr: 0.0032 - 374ms/epoch - 53ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 185/200\n",
      "7/7 - 0s - loss: 0.2479 - accuracy: 0.8929 - val_loss: 0.3351 - val_accuracy: 0.8261 - lr: 0.0030 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 186/200\n",
      "7/7 - 0s - loss: 0.2525 - accuracy: 0.9062 - val_loss: 0.3186 - val_accuracy: 0.8261 - lr: 0.0028 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 187/200\n",
      "7/7 - 0s - loss: 0.2273 - accuracy: 0.9062 - val_loss: 0.3286 - val_accuracy: 0.7826 - lr: 0.0026 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 188/200\n",
      "7/7 - 0s - loss: 0.2911 - accuracy: 0.8750 - val_loss: 0.3096 - val_accuracy: 0.8261 - lr: 0.0024 - 455ms/epoch - 65ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 189/200\n",
      "7/7 - 0s - loss: 0.2528 - accuracy: 0.8929 - val_loss: 0.3236 - val_accuracy: 0.7826 - lr: 0.0022 - 383ms/epoch - 55ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 190/200\n",
      "7/7 - 0s - loss: 0.3082 - accuracy: 0.8839 - val_loss: 0.3222 - val_accuracy: 0.7826 - lr: 0.0020 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 191/200\n",
      "7/7 - 0s - loss: 0.2480 - accuracy: 0.8795 - val_loss: 0.3064 - val_accuracy: 0.8696 - lr: 0.0018 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 192/200\n",
      "7/7 - 0s - loss: 0.2661 - accuracy: 0.9018 - val_loss: 0.3152 - val_accuracy: 0.8261 - lr: 0.0016 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 193/200\n",
      "7/7 - 0s - loss: 0.2903 - accuracy: 0.8795 - val_loss: 0.3193 - val_accuracy: 0.8261 - lr: 0.0014 - 436ms/epoch - 62ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 194/200\n",
      "7/7 - 0s - loss: 0.2692 - accuracy: 0.8705 - val_loss: 0.3226 - val_accuracy: 0.8261 - lr: 0.0012 - 368ms/epoch - 53ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 195/200\n",
      "7/7 - 0s - loss: 0.3063 - accuracy: 0.8750 - val_loss: 0.3155 - val_accuracy: 0.8261 - lr: 0.0010 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 196/200\n",
      "7/7 - 0s - loss: 0.3090 - accuracy: 0.8750 - val_loss: 0.3180 - val_accuracy: 0.8261 - lr: 8.0009e-04 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 197/200\n",
      "7/7 - 0s - loss: 0.2403 - accuracy: 0.9152 - val_loss: 0.3145 - val_accuracy: 0.8261 - lr: 6.0009e-04 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 198/200\n",
      "7/7 - 0s - loss: 0.2226 - accuracy: 0.9107 - val_loss: 0.3181 - val_accuracy: 0.8261 - lr: 4.0010e-04 - 458ms/epoch - 65ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 199/200\n",
      "7/7 - 0s - loss: 0.2600 - accuracy: 0.8929 - val_loss: 0.3145 - val_accuracy: 0.8261 - lr: 2.0010e-04 - 346ms/epoch - 49ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 200/200\n",
      "7/7 - 0s - loss: 0.2492 - accuracy: 0.8973 - val_loss: 0.3146 - val_accuracy: 0.8261 - lr: 1.0000e-07 - 339ms/epoch - 48ms/step\n",
      "\n",
      "======================BEST:======================\n",
      "    epoch      loss       acc  val_loss   val_acc     rocp        f1       rfa\n",
      "53   54.0  0.317171  0.857143  0.271243  0.913043  0.94697  0.916667  0.924489\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83        12\n",
      "           1       0.82      0.82      0.82        11\n",
      "\n",
      "    accuracy                           0.83        23\n",
      "   macro avg       0.83      0.83      0.83        23\n",
      "weighted avg       0.83      0.83      0.83        23\n",
      "\n",
      "[[10  2]\n",
      " [ 2  9]]\n",
      "ACCURACY: 0.8260869565217391\n",
      "PRECISION: 0.8181818181818182\n",
      "RECALL: 0.8181818181818182\n",
      "F1: 0.8181818181818182\n",
      "ROC_AUC(Pr.): 0.9242424242424243\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91        12\n",
      "           1       0.85      1.00      0.92        11\n",
      "\n",
      "    accuracy                           0.91        23\n",
      "   macro avg       0.92      0.92      0.91        23\n",
      "weighted avg       0.93      0.91      0.91        23\n",
      "\n",
      "[[10  2]\n",
      " [ 0 11]]\n",
      "ACCURACY: 0.9130434782608695\n",
      "PRECISION: 0.8461538461538461\n",
      "RECALL: 1.0\n",
      "F1: 0.9166666666666666\n",
      "ROC_AUC(Pr.): 0.946969696969697\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91        12\n",
      "           1       0.85      1.00      0.92        11\n",
      "\n",
      "    accuracy                           0.91        23\n",
      "   macro avg       0.92      0.92      0.91        23\n",
      "weighted avg       0.93      0.91      0.91        23\n",
      "\n",
      "[[10  2]\n",
      " [ 0 11]]\n",
      "ACCURACY: 0.9130434782608695\n",
      "PRECISION: 0.8461538461538461\n",
      "RECALL: 1.0\n",
      "F1: 0.9166666666666666\n",
      "ROC_AUC(Pr.): 0.946969696969697\n",
      "\n",
      "=====================CV[10]========================\n",
      "\n",
      "207 train samples\n",
      "23 test samples\n",
      "\n",
      "Over sampling: 207 -> 208\n",
      "Add train samples: 16\n",
      "224 new train samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\tf28\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.00000010\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_28484\\4287042063.py:159: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================EPOCH 000001:======================\n",
      "   epoch     loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "0    1.0  0.69376  0.473214  0.692483  0.565217  0.719697  0.166667  0.472069\n",
      "RFA : 0.4720685057567827\n",
      "7/7 - 7s - loss: 0.6938 - accuracy: 0.4732 - val_loss: 0.6925 - val_accuracy: 0.5652 - lr: 2.0010e-04 - 7s/epoch - 958ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 2/200\n",
      "7/7 - 0s - loss: 0.6934 - accuracy: 0.4866 - val_loss: 0.6909 - val_accuracy: 0.5217 - lr: 4.0010e-04 - 385ms/epoch - 55ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 3/200\n",
      "======================EPOCH 000003:======================\n",
      "   epoch      loss      acc  val_loss   val_acc      rocp        f1       rfa\n",
      "2    3.0  0.693027  0.53125  0.688496  0.608696  0.780303  0.307692  0.554827\n",
      "RFA : 0.5548266859738142\n",
      "7/7 - 0s - loss: 0.6930 - accuracy: 0.5312 - val_loss: 0.6885 - val_accuracy: 0.6087 - lr: 6.0009e-04 - 393ms/epoch - 56ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 4/200\n",
      "======================EPOCH 000004:======================\n",
      "   epoch      loss    acc  val_loss   val_acc     rocp        f1       rfa\n",
      "3    4.0  0.686263  0.625  0.677151  0.652174  0.80303  0.555556  0.663614\n",
      "RFA : 0.6636144130819976\n",
      "7/7 - 0s - loss: 0.6863 - accuracy: 0.6250 - val_loss: 0.6772 - val_accuracy: 0.6522 - lr: 8.0009e-04 - 410ms/epoch - 59ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 5/200\n",
      "======================EPOCH 000005:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "4    5.0  0.671398  0.678571  0.624771  0.782609  0.825758  0.814815  0.806825\n",
      "RFA : 0.8068254986696372\n",
      "7/7 - 1s - loss: 0.6714 - accuracy: 0.6786 - val_loss: 0.6248 - val_accuracy: 0.7826 - lr: 0.0010 - 568ms/epoch - 81ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 6/200\n",
      "======================EPOCH 000006:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "5    6.0  0.594249  0.754464  0.486163  0.782609  0.833333  0.814815  0.809098\n",
      "RFA : 0.8090982259423645\n",
      "7/7 - 0s - loss: 0.5942 - accuracy: 0.7545 - val_loss: 0.4862 - val_accuracy: 0.7826 - lr: 0.0012 - 405ms/epoch - 58ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 7/200\n",
      "======================EPOCH 000007:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "6    7.0  0.511507  0.732143  0.435478  0.782609  0.886364  0.814815  0.825007\n",
      "RFA : 0.8250073168514553\n",
      "7/7 - 0s - loss: 0.5115 - accuracy: 0.7321 - val_loss: 0.4355 - val_accuracy: 0.7826 - lr: 0.0014 - 361ms/epoch - 52ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 8/200\n",
      "7/7 - 0s - loss: 0.4705 - accuracy: 0.7500 - val_loss: 0.3960 - val_accuracy: 0.7826 - lr: 0.0016 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 9/200\n",
      "======================EPOCH 000009:======================\n",
      "   epoch      loss      acc  val_loss   val_acc      rocp        f1      rfa\n",
      "8    9.0  0.428539  0.78125  0.360639  0.913043  0.893939  0.916667  0.90858\n",
      "RFA : 0.9085803761626734\n",
      "7/7 - 1s - loss: 0.4285 - accuracy: 0.7812 - val_loss: 0.3606 - val_accuracy: 0.9130 - lr: 0.0018 - 540ms/epoch - 77ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 10/200\n",
      "7/7 - 0s - loss: 0.4089 - accuracy: 0.8036 - val_loss: 0.3134 - val_accuracy: 0.8696 - lr: 0.0020 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 11/200\n",
      "7/7 - 0s - loss: 0.3820 - accuracy: 0.8125 - val_loss: 0.4183 - val_accuracy: 0.6957 - lr: 0.0022 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 12/200\n",
      "7/7 - 0s - loss: 0.4327 - accuracy: 0.7902 - val_loss: 0.2937 - val_accuracy: 0.8696 - lr: 0.0024 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 13/200\n",
      "======================EPOCH 000013:======================\n",
      "    epoch      loss       acc  val_loss   val_acc  rocp    f1       rfa\n",
      "12   13.0  0.369806  0.830357  0.282058  0.869565   1.0  0.88  0.912348\n",
      "RFA : 0.9123478161096572\n",
      "7/7 - 0s - loss: 0.3698 - accuracy: 0.8304 - val_loss: 0.2821 - val_accuracy: 0.8696 - lr: 0.0026 - 368ms/epoch - 53ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 14/200\n",
      "7/7 - 0s - loss: 0.3215 - accuracy: 0.8527 - val_loss: 0.2648 - val_accuracy: 0.8696 - lr: 0.0028 - 479ms/epoch - 68ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 15/200\n",
      "======================EPOCH 000015:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "14   15.0  0.319284  0.852679  0.225854  0.956522  0.969697  0.956522  0.960474\n",
      "RFA : 0.960474311928504\n",
      "7/7 - 0s - loss: 0.3193 - accuracy: 0.8527 - val_loss: 0.2259 - val_accuracy: 0.9565 - lr: 0.0030 - 403ms/epoch - 58ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 16/200\n",
      "7/7 - 0s - loss: 0.4680 - accuracy: 0.7902 - val_loss: 0.3195 - val_accuracy: 0.8261 - lr: 0.0032 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 17/200\n",
      "7/7 - 0s - loss: 0.3524 - accuracy: 0.8214 - val_loss: 0.2899 - val_accuracy: 0.8696 - lr: 0.0034 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 18/200\n",
      "7/7 - 0s - loss: 0.3224 - accuracy: 0.8616 - val_loss: 0.2438 - val_accuracy: 0.8696 - lr: 0.0036 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 19/200\n",
      "7/7 - 0s - loss: 0.3635 - accuracy: 0.8438 - val_loss: 0.2362 - val_accuracy: 0.9130 - lr: 0.0038 - 427ms/epoch - 61ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 20/200\n",
      "7/7 - 0s - loss: 0.2989 - accuracy: 0.8705 - val_loss: 0.2930 - val_accuracy: 0.8261 - lr: 0.0040 - 382ms/epoch - 55ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 21/200\n",
      "7/7 - 0s - loss: 0.3558 - accuracy: 0.8705 - val_loss: 0.2153 - val_accuracy: 0.9130 - lr: 0.0042 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 22/200\n",
      "======================EPOCH 000022:======================\n",
      "    epoch      loss       acc  val_loss   val_acc  rocp        f1       rfa\n",
      "21   22.0  0.352608  0.834821  0.214596  0.956522   1.0  0.956522  0.969565\n",
      "RFA : 0.9695652210194131\n",
      "7/7 - 0s - loss: 0.3526 - accuracy: 0.8348 - val_loss: 0.2146 - val_accuracy: 0.9565 - lr: 0.0044 - 371ms/epoch - 53ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 23/200\n",
      "7/7 - 0s - loss: 0.3875 - accuracy: 0.8214 - val_loss: 0.2431 - val_accuracy: 0.9565 - lr: 0.0046 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 24/200\n",
      "7/7 - 0s - loss: 0.3491 - accuracy: 0.8482 - val_loss: 0.2631 - val_accuracy: 0.8261 - lr: 0.0048 - 403ms/epoch - 58ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 25/200\n",
      "7/7 - 0s - loss: 0.3354 - accuracy: 0.8482 - val_loss: 0.2878 - val_accuracy: 0.8261 - lr: 0.0050 - 369ms/epoch - 53ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 26/200\n",
      "7/7 - 0s - loss: 0.3612 - accuracy: 0.8393 - val_loss: 0.2007 - val_accuracy: 0.9565 - lr: 0.0052 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 27/200\n",
      "7/7 - 0s - loss: 0.3079 - accuracy: 0.8616 - val_loss: 0.2114 - val_accuracy: 0.9565 - lr: 0.0054 - 361ms/epoch - 52ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 28/200\n",
      "7/7 - 0s - loss: 0.3607 - accuracy: 0.8571 - val_loss: 0.2179 - val_accuracy: 0.9565 - lr: 0.0056 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 29/200\n",
      "7/7 - 0s - loss: 0.2571 - accuracy: 0.8839 - val_loss: 0.3918 - val_accuracy: 0.7391 - lr: 0.0058 - 428ms/epoch - 61ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 30/200\n",
      "7/7 - 0s - loss: 0.3029 - accuracy: 0.8795 - val_loss: 0.1883 - val_accuracy: 0.9565 - lr: 0.0060 - 369ms/epoch - 53ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 31/200\n",
      "7/7 - 0s - loss: 0.3171 - accuracy: 0.8750 - val_loss: 0.3684 - val_accuracy: 0.7391 - lr: 0.0062 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 32/200\n",
      "7/7 - 0s - loss: 0.2771 - accuracy: 0.8661 - val_loss: 0.1326 - val_accuracy: 0.9565 - lr: 0.0064 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 33/200\n",
      "7/7 - 0s - loss: 0.3633 - accuracy: 0.8214 - val_loss: 0.5597 - val_accuracy: 0.7391 - lr: 0.0066 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 34/200\n",
      "7/7 - 0s - loss: 0.3816 - accuracy: 0.8214 - val_loss: 0.4572 - val_accuracy: 0.7391 - lr: 0.0068 - 493ms/epoch - 70ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 35/200\n",
      "7/7 - 0s - loss: 0.3562 - accuracy: 0.8125 - val_loss: 0.2702 - val_accuracy: 0.9130 - lr: 0.0070 - 384ms/epoch - 55ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 36/200\n",
      "7/7 - 0s - loss: 0.3271 - accuracy: 0.8750 - val_loss: 0.2396 - val_accuracy: 0.8696 - lr: 0.0072 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 37/200\n",
      "7/7 - 0s - loss: 0.3354 - accuracy: 0.8571 - val_loss: 0.2746 - val_accuracy: 0.9130 - lr: 0.0074 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 38/200\n",
      "7/7 - 0s - loss: 0.3010 - accuracy: 0.8839 - val_loss: 0.1520 - val_accuracy: 0.9565 - lr: 0.0076 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 39/200\n",
      "7/7 - 0s - loss: 0.3531 - accuracy: 0.8348 - val_loss: 0.6685 - val_accuracy: 0.7826 - lr: 0.0078 - 467ms/epoch - 67ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 40/200\n",
      "7/7 - 0s - loss: 0.3510 - accuracy: 0.8348 - val_loss: 0.2389 - val_accuracy: 0.9565 - lr: 0.0080 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 41/200\n",
      "7/7 - 0s - loss: 0.3319 - accuracy: 0.8393 - val_loss: 0.2141 - val_accuracy: 0.9130 - lr: 0.0082 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 42/200\n",
      "7/7 - 0s - loss: 0.3078 - accuracy: 0.8527 - val_loss: 0.2796 - val_accuracy: 0.7826 - lr: 0.0084 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 43/200\n",
      "7/7 - 0s - loss: 0.3241 - accuracy: 0.8438 - val_loss: 0.1899 - val_accuracy: 0.9565 - lr: 0.0086 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 44/200\n",
      "7/7 - 0s - loss: 0.3364 - accuracy: 0.8571 - val_loss: 0.2321 - val_accuracy: 0.9565 - lr: 0.0088 - 478ms/epoch - 68ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 45/200\n",
      "7/7 - 0s - loss: 0.2963 - accuracy: 0.8527 - val_loss: 0.2001 - val_accuracy: 0.8696 - lr: 0.0090 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 46/200\n",
      "7/7 - 0s - loss: 0.3245 - accuracy: 0.8661 - val_loss: 0.2236 - val_accuracy: 0.8696 - lr: 0.0092 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 47/200\n",
      "7/7 - 0s - loss: 0.3059 - accuracy: 0.8884 - val_loss: 0.2406 - val_accuracy: 0.8261 - lr: 0.0094 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 48/200\n",
      "7/7 - 0s - loss: 0.3280 - accuracy: 0.8527 - val_loss: 0.2037 - val_accuracy: 0.9565 - lr: 0.0096 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 49/200\n",
      "7/7 - 1s - loss: 0.3098 - accuracy: 0.8482 - val_loss: 0.1491 - val_accuracy: 0.9130 - lr: 0.0098 - 511ms/epoch - 73ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 50/200\n",
      "7/7 - 0s - loss: 0.3384 - accuracy: 0.8750 - val_loss: 0.3916 - val_accuracy: 0.7391 - lr: 0.0100 - 359ms/epoch - 51ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 51/200\n",
      "7/7 - 0s - loss: 0.6088 - accuracy: 0.7277 - val_loss: 0.3296 - val_accuracy: 0.8261 - lr: 0.0098 - 328ms/epoch - 47ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 52/200\n",
      "7/7 - 0s - loss: 8.5881 - accuracy: 0.5625 - val_loss: 4.5619 - val_accuracy: 0.3478 - lr: 0.0096 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 53/200\n",
      "7/7 - 0s - loss: 1040.2013 - accuracy: 0.5670 - val_loss: 681.9078 - val_accuracy: 0.4783 - lr: 0.0094 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 54/200\n",
      "7/7 - 0s - loss: 764.3597 - accuracy: 0.5536 - val_loss: 124.3217 - val_accuracy: 0.5217 - lr: 0.0092 - 459ms/epoch - 66ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 55/200\n",
      "7/7 - 0s - loss: 17.6223 - accuracy: 0.6250 - val_loss: 1.9645 - val_accuracy: 0.8261 - lr: 0.0090 - 381ms/epoch - 54ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 56/200\n",
      "7/7 - 0s - loss: 1.8991 - accuracy: 0.7455 - val_loss: 0.5959 - val_accuracy: 0.6957 - lr: 0.0088 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 57/200\n",
      "7/7 - 0s - loss: 0.5977 - accuracy: 0.7902 - val_loss: 0.2711 - val_accuracy: 0.8696 - lr: 0.0086 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 58/200\n",
      "7/7 - 0s - loss: 0.5424 - accuracy: 0.8036 - val_loss: 0.3112 - val_accuracy: 0.8696 - lr: 0.0084 - 361ms/epoch - 52ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 59/200\n",
      "7/7 - 0s - loss: 0.4538 - accuracy: 0.8214 - val_loss: 0.2969 - val_accuracy: 0.8696 - lr: 0.0082 - 444ms/epoch - 63ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 60/200\n",
      "7/7 - 0s - loss: 0.4933 - accuracy: 0.8036 - val_loss: 0.2240 - val_accuracy: 0.9130 - lr: 0.0080 - 413ms/epoch - 59ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 61/200\n",
      "7/7 - 1s - loss: 0.4276 - accuracy: 0.8259 - val_loss: 0.2521 - val_accuracy: 0.8261 - lr: 0.0078 - 1s/epoch - 204ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 62/200\n",
      "7/7 - 0s - loss: 0.4447 - accuracy: 0.7902 - val_loss: 0.2581 - val_accuracy: 0.8696 - lr: 0.0076 - 492ms/epoch - 70ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 63/200\n",
      "7/7 - 0s - loss: 0.3879 - accuracy: 0.8348 - val_loss: 0.3170 - val_accuracy: 0.8261 - lr: 0.0074 - 344ms/epoch - 49ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 64/200\n",
      "7/7 - 0s - loss: 0.4338 - accuracy: 0.8259 - val_loss: 0.2558 - val_accuracy: 0.8696 - lr: 0.0072 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 65/200\n",
      "7/7 - 0s - loss: 0.3512 - accuracy: 0.8438 - val_loss: 0.2609 - val_accuracy: 0.9130 - lr: 0.0070 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 66/200\n",
      "7/7 - 0s - loss: 0.4655 - accuracy: 0.8080 - val_loss: 0.2458 - val_accuracy: 0.9130 - lr: 0.0068 - 434ms/epoch - 62ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 67/200\n",
      "7/7 - 0s - loss: 0.4838 - accuracy: 0.7946 - val_loss: 0.2474 - val_accuracy: 0.8696 - lr: 0.0066 - 371ms/epoch - 53ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 68/200\n",
      "7/7 - 0s - loss: 0.4324 - accuracy: 0.8214 - val_loss: 0.2480 - val_accuracy: 0.9130 - lr: 0.0064 - 322ms/epoch - 46ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 69/200\n",
      "7/7 - 0s - loss: 0.4481 - accuracy: 0.8080 - val_loss: 0.2638 - val_accuracy: 0.8696 - lr: 0.0062 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 70/200\n",
      "7/7 - 0s - loss: 0.4019 - accuracy: 0.8080 - val_loss: 0.2560 - val_accuracy: 0.9130 - lr: 0.0060 - 332ms/epoch - 47ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 71/200\n",
      "7/7 - 0s - loss: 0.3858 - accuracy: 0.7946 - val_loss: 0.2647 - val_accuracy: 0.8696 - lr: 0.0058 - 491ms/epoch - 70ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 72/200\n",
      "7/7 - 0s - loss: 0.3703 - accuracy: 0.8527 - val_loss: 0.2672 - val_accuracy: 0.8696 - lr: 0.0056 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 73/200\n",
      "7/7 - 0s - loss: 0.3800 - accuracy: 0.8125 - val_loss: 0.2529 - val_accuracy: 0.9130 - lr: 0.0054 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 74/200\n",
      "7/7 - 0s - loss: 0.4683 - accuracy: 0.7902 - val_loss: 0.2584 - val_accuracy: 0.8696 - lr: 0.0052 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 75/200\n",
      "7/7 - 0s - loss: 0.4438 - accuracy: 0.7946 - val_loss: 0.2595 - val_accuracy: 0.9130 - lr: 0.0050 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 76/200\n",
      "7/7 - 0s - loss: 0.3696 - accuracy: 0.8259 - val_loss: 0.3196 - val_accuracy: 0.8261 - lr: 0.0048 - 479ms/epoch - 68ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 77/200\n",
      "7/7 - 0s - loss: 0.4396 - accuracy: 0.8304 - val_loss: 0.2620 - val_accuracy: 0.8696 - lr: 0.0046 - 330ms/epoch - 47ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 78/200\n",
      "7/7 - 0s - loss: 0.3577 - accuracy: 0.8259 - val_loss: 0.2916 - val_accuracy: 0.8261 - lr: 0.0044 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 79/200\n",
      "7/7 - 0s - loss: 0.3744 - accuracy: 0.8661 - val_loss: 0.2572 - val_accuracy: 0.8696 - lr: 0.0042 - 329ms/epoch - 47ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 80/200\n",
      "7/7 - 0s - loss: 0.4208 - accuracy: 0.8036 - val_loss: 0.2608 - val_accuracy: 0.9130 - lr: 0.0040 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 81/200\n",
      "7/7 - 1s - loss: 0.3803 - accuracy: 0.8214 - val_loss: 0.2493 - val_accuracy: 0.8696 - lr: 0.0038 - 526ms/epoch - 75ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 82/200\n",
      "7/7 - 0s - loss: 0.3794 - accuracy: 0.8259 - val_loss: 0.2310 - val_accuracy: 0.9130 - lr: 0.0036 - 380ms/epoch - 54ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 83/200\n",
      "7/7 - 0s - loss: 0.3805 - accuracy: 0.8348 - val_loss: 0.2667 - val_accuracy: 0.8261 - lr: 0.0034 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 84/200\n",
      "7/7 - 0s - loss: 0.3894 - accuracy: 0.8304 - val_loss: 0.2244 - val_accuracy: 0.9130 - lr: 0.0032 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 85/200\n",
      "7/7 - 0s - loss: 0.3272 - accuracy: 0.8571 - val_loss: 0.2513 - val_accuracy: 0.8696 - lr: 0.0030 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 86/200\n",
      "7/7 - 0s - loss: 0.3376 - accuracy: 0.8393 - val_loss: 0.2243 - val_accuracy: 0.9130 - lr: 0.0028 - 467ms/epoch - 67ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 87/200\n",
      "7/7 - 0s - loss: 0.4648 - accuracy: 0.8214 - val_loss: 0.2404 - val_accuracy: 0.9130 - lr: 0.0026 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 88/200\n",
      "7/7 - 0s - loss: 0.3986 - accuracy: 0.8259 - val_loss: 0.2339 - val_accuracy: 0.9130 - lr: 0.0024 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 89/200\n",
      "7/7 - 0s - loss: 0.3837 - accuracy: 0.8214 - val_loss: 0.2473 - val_accuracy: 0.9130 - lr: 0.0022 - 353ms/epoch - 50ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 90/200\n",
      "7/7 - 0s - loss: 0.3882 - accuracy: 0.8438 - val_loss: 0.2397 - val_accuracy: 0.9130 - lr: 0.0020 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 91/200\n",
      "7/7 - 0s - loss: 0.3432 - accuracy: 0.8482 - val_loss: 0.2436 - val_accuracy: 0.9130 - lr: 0.0018 - 461ms/epoch - 66ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 92/200\n",
      "7/7 - 0s - loss: 0.3493 - accuracy: 0.7991 - val_loss: 0.2455 - val_accuracy: 0.9130 - lr: 0.0016 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 93/200\n",
      "7/7 - 0s - loss: 0.3713 - accuracy: 0.8259 - val_loss: 0.2409 - val_accuracy: 0.9130 - lr: 0.0014 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 94/200\n",
      "7/7 - 0s - loss: 0.3119 - accuracy: 0.8616 - val_loss: 0.2459 - val_accuracy: 0.9130 - lr: 0.0012 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 95/200\n",
      "7/7 - 0s - loss: 0.3760 - accuracy: 0.8348 - val_loss: 0.2349 - val_accuracy: 0.9130 - lr: 0.0010 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 96/200\n",
      "7/7 - 0s - loss: 0.3756 - accuracy: 0.8348 - val_loss: 0.2282 - val_accuracy: 0.9130 - lr: 8.0009e-04 - 499ms/epoch - 71ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 97/200\n",
      "7/7 - 0s - loss: 0.3187 - accuracy: 0.8661 - val_loss: 0.2283 - val_accuracy: 0.9130 - lr: 6.0009e-04 - 427ms/epoch - 61ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 98/200\n",
      "7/7 - 0s - loss: 0.3100 - accuracy: 0.8705 - val_loss: 0.2260 - val_accuracy: 0.9130 - lr: 4.0010e-04 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 99/200\n",
      "7/7 - 0s - loss: 0.2829 - accuracy: 0.9018 - val_loss: 0.2267 - val_accuracy: 0.9130 - lr: 2.0010e-04 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 100/200\n",
      "7/7 - 0s - loss: 0.3659 - accuracy: 0.8214 - val_loss: 0.2267 - val_accuracy: 0.9130 - lr: 1.0000e-07 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00000010\n",
      "Epoch 101/200\n",
      "7/7 - 0s - loss: 0.3463 - accuracy: 0.8348 - val_loss: 0.2272 - val_accuracy: 0.9130 - lr: 2.0010e-04 - 371ms/epoch - 53ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 102/200\n",
      "7/7 - 0s - loss: 0.3623 - accuracy: 0.8393 - val_loss: 0.2265 - val_accuracy: 0.9130 - lr: 4.0010e-04 - 360ms/epoch - 51ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 103/200\n",
      "7/7 - 0s - loss: 0.3544 - accuracy: 0.8393 - val_loss: 0.2289 - val_accuracy: 0.9130 - lr: 6.0009e-04 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 104/200\n",
      "7/7 - 0s - loss: 0.3125 - accuracy: 0.8661 - val_loss: 0.2341 - val_accuracy: 0.9130 - lr: 8.0009e-04 - 327ms/epoch - 47ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 105/200\n",
      "7/7 - 0s - loss: 0.3097 - accuracy: 0.8571 - val_loss: 0.2227 - val_accuracy: 0.9130 - lr: 0.0010 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 106/200\n",
      "7/7 - 0s - loss: 0.3374 - accuracy: 0.8438 - val_loss: 0.2259 - val_accuracy: 0.9130 - lr: 0.0012 - 486ms/epoch - 69ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 107/200\n",
      "7/7 - 0s - loss: 0.3740 - accuracy: 0.8393 - val_loss: 0.2209 - val_accuracy: 0.9130 - lr: 0.0014 - 369ms/epoch - 53ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 108/200\n",
      "7/7 - 0s - loss: 0.3727 - accuracy: 0.8393 - val_loss: 0.2232 - val_accuracy: 0.9130 - lr: 0.0016 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 109/200\n",
      "7/7 - 0s - loss: 0.2697 - accuracy: 0.8839 - val_loss: 0.2363 - val_accuracy: 0.9565 - lr: 0.0018 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 110/200\n",
      "7/7 - 0s - loss: 0.3169 - accuracy: 0.8616 - val_loss: 0.2282 - val_accuracy: 0.9130 - lr: 0.0020 - 326ms/epoch - 47ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 111/200\n",
      "7/7 - 0s - loss: 0.3191 - accuracy: 0.8705 - val_loss: 0.2196 - val_accuracy: 0.9130 - lr: 0.0022 - 478ms/epoch - 68ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 112/200\n",
      "7/7 - 0s - loss: 0.3593 - accuracy: 0.8705 - val_loss: 0.2304 - val_accuracy: 0.9565 - lr: 0.0024 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 113/200\n",
      "7/7 - 0s - loss: 0.3668 - accuracy: 0.8482 - val_loss: 0.2145 - val_accuracy: 0.9130 - lr: 0.0026 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 114/200\n",
      "7/7 - 0s - loss: 0.3865 - accuracy: 0.8438 - val_loss: 0.2155 - val_accuracy: 0.9130 - lr: 0.0028 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 115/200\n",
      "7/7 - 0s - loss: 0.3447 - accuracy: 0.8438 - val_loss: 0.3260 - val_accuracy: 0.7826 - lr: 0.0030 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 116/200\n",
      "7/7 - 0s - loss: 0.3457 - accuracy: 0.8438 - val_loss: 0.2215 - val_accuracy: 0.9130 - lr: 0.0032 - 471ms/epoch - 67ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 117/200\n",
      "7/7 - 0s - loss: 0.3843 - accuracy: 0.8080 - val_loss: 0.2504 - val_accuracy: 0.8696 - lr: 0.0034 - 370ms/epoch - 53ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 118/200\n",
      "7/7 - 0s - loss: 0.3714 - accuracy: 0.8482 - val_loss: 0.2130 - val_accuracy: 0.9130 - lr: 0.0036 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 119/200\n",
      "7/7 - 0s - loss: 0.3148 - accuracy: 0.8482 - val_loss: 0.2081 - val_accuracy: 0.9130 - lr: 0.0038 - 358ms/epoch - 51ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 120/200\n",
      "7/7 - 0s - loss: 0.3409 - accuracy: 0.8661 - val_loss: 0.1953 - val_accuracy: 0.9130 - lr: 0.0040 - 337ms/epoch - 48ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 121/200\n",
      "7/7 - 0s - loss: 0.3621 - accuracy: 0.8393 - val_loss: 0.2393 - val_accuracy: 0.8696 - lr: 0.0042 - 459ms/epoch - 66ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 122/200\n",
      "7/7 - 0s - loss: 0.3785 - accuracy: 0.8616 - val_loss: 0.2152 - val_accuracy: 0.9130 - lr: 0.0044 - 368ms/epoch - 53ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 123/200\n",
      "7/7 - 0s - loss: 0.3293 - accuracy: 0.8571 - val_loss: 0.2154 - val_accuracy: 0.9130 - lr: 0.0046 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 124/200\n",
      "7/7 - 0s - loss: 0.3309 - accuracy: 0.8393 - val_loss: 0.3192 - val_accuracy: 0.7826 - lr: 0.0048 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 125/200\n",
      "7/7 - 0s - loss: 0.3642 - accuracy: 0.8571 - val_loss: 0.1995 - val_accuracy: 0.9130 - lr: 0.0050 - 345ms/epoch - 49ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 126/200\n",
      "7/7 - 0s - loss: 0.3270 - accuracy: 0.8527 - val_loss: 0.2459 - val_accuracy: 0.8696 - lr: 0.0052 - 449ms/epoch - 64ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 127/200\n",
      "7/7 - 0s - loss: 0.3765 - accuracy: 0.8170 - val_loss: 0.2605 - val_accuracy: 0.8261 - lr: 0.0054 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 128/200\n",
      "7/7 - 0s - loss: 0.3342 - accuracy: 0.8571 - val_loss: 0.4352 - val_accuracy: 0.7391 - lr: 0.0056 - 360ms/epoch - 51ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 129/200\n",
      "7/7 - 0s - loss: 0.3680 - accuracy: 0.8482 - val_loss: 0.2724 - val_accuracy: 0.7826 - lr: 0.0058 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 130/200\n",
      "7/7 - 0s - loss: 0.3439 - accuracy: 0.8616 - val_loss: 0.2363 - val_accuracy: 0.8696 - lr: 0.0060 - 336ms/epoch - 48ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 131/200\n",
      "7/7 - 0s - loss: 0.3550 - accuracy: 0.8616 - val_loss: 0.2650 - val_accuracy: 0.8696 - lr: 0.0062 - 460ms/epoch - 66ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 132/200\n",
      "7/7 - 0s - loss: 0.2832 - accuracy: 0.8884 - val_loss: 0.2838 - val_accuracy: 0.7826 - lr: 0.0064 - 371ms/epoch - 53ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 133/200\n",
      "7/7 - 0s - loss: 0.3570 - accuracy: 0.8214 - val_loss: 0.2403 - val_accuracy: 0.9130 - lr: 0.0066 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 134/200\n",
      "7/7 - 0s - loss: 0.3156 - accuracy: 0.8705 - val_loss: 0.2111 - val_accuracy: 0.9130 - lr: 0.0068 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 135/200\n",
      "7/7 - 0s - loss: 0.3636 - accuracy: 0.8304 - val_loss: 0.1948 - val_accuracy: 0.9565 - lr: 0.0070 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 136/200\n",
      "7/7 - 0s - loss: 0.3411 - accuracy: 0.8214 - val_loss: 0.2257 - val_accuracy: 0.9565 - lr: 0.0072 - 466ms/epoch - 67ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 137/200\n",
      "7/7 - 0s - loss: 0.3284 - accuracy: 0.8571 - val_loss: 0.2283 - val_accuracy: 0.9565 - lr: 0.0074 - 382ms/epoch - 55ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 138/200\n",
      "7/7 - 0s - loss: 0.2991 - accuracy: 0.8884 - val_loss: 0.2661 - val_accuracy: 0.8261 - lr: 0.0076 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 139/200\n",
      "7/7 - 0s - loss: 0.3572 - accuracy: 0.8393 - val_loss: 0.3716 - val_accuracy: 0.7826 - lr: 0.0078 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 140/200\n",
      "7/7 - 0s - loss: 0.3174 - accuracy: 0.8527 - val_loss: 0.3801 - val_accuracy: 0.7391 - lr: 0.0080 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 141/200\n",
      "7/7 - 0s - loss: 0.3222 - accuracy: 0.8527 - val_loss: 0.2081 - val_accuracy: 0.9130 - lr: 0.0082 - 436ms/epoch - 62ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 142/200\n",
      "7/7 - 0s - loss: 0.2926 - accuracy: 0.8482 - val_loss: 0.1854 - val_accuracy: 0.9130 - lr: 0.0084 - 381ms/epoch - 54ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 143/200\n",
      "7/7 - 0s - loss: 0.3709 - accuracy: 0.8214 - val_loss: 0.4051 - val_accuracy: 0.8261 - lr: 0.0086 - 357ms/epoch - 51ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 144/200\n",
      "7/7 - 0s - loss: 0.3957 - accuracy: 0.8080 - val_loss: 0.2432 - val_accuracy: 0.8261 - lr: 0.0088 - 368ms/epoch - 53ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 145/200\n",
      "7/7 - 0s - loss: 0.2804 - accuracy: 0.8795 - val_loss: 0.1720 - val_accuracy: 0.9565 - lr: 0.0090 - 358ms/epoch - 51ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 146/200\n",
      "7/7 - 0s - loss: 0.2967 - accuracy: 0.8571 - val_loss: 0.1479 - val_accuracy: 0.9565 - lr: 0.0092 - 382ms/epoch - 55ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 147/200\n",
      "7/7 - 0s - loss: 0.2989 - accuracy: 0.8884 - val_loss: 0.3840 - val_accuracy: 0.7391 - lr: 0.0094 - 381ms/epoch - 54ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 148/200\n",
      "7/7 - 0s - loss: 0.2849 - accuracy: 0.8839 - val_loss: 0.1945 - val_accuracy: 0.8696 - lr: 0.0096 - 369ms/epoch - 53ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 149/200\n",
      "7/7 - 0s - loss: 0.2807 - accuracy: 0.8929 - val_loss: 0.1539 - val_accuracy: 0.9130 - lr: 0.0098 - 348ms/epoch - 50ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 150/200\n",
      "7/7 - 0s - loss: 0.3010 - accuracy: 0.8973 - val_loss: 0.1851 - val_accuracy: 0.9565 - lr: 0.0100 - 360ms/epoch - 51ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 151/200\n",
      "7/7 - 0s - loss: 0.3090 - accuracy: 0.8795 - val_loss: 0.1802 - val_accuracy: 0.9130 - lr: 0.0098 - 370ms/epoch - 53ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 152/200\n",
      "7/7 - 0s - loss: 0.2651 - accuracy: 0.8973 - val_loss: 0.2329 - val_accuracy: 0.8696 - lr: 0.0096 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 153/200\n",
      "7/7 - 0s - loss: 0.3581 - accuracy: 0.8438 - val_loss: 0.2384 - val_accuracy: 0.8696 - lr: 0.0094 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 154/200\n",
      "7/7 - 0s - loss: 0.4168 - accuracy: 0.8393 - val_loss: 0.2112 - val_accuracy: 0.8696 - lr: 0.0092 - 338ms/epoch - 48ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 155/200\n",
      "7/7 - 0s - loss: 0.2849 - accuracy: 0.8482 - val_loss: 0.1810 - val_accuracy: 0.9130 - lr: 0.0090 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 156/200\n",
      "7/7 - 0s - loss: 0.3121 - accuracy: 0.8527 - val_loss: 0.1785 - val_accuracy: 0.9565 - lr: 0.0088 - 482ms/epoch - 69ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 157/200\n",
      "7/7 - 0s - loss: 0.3017 - accuracy: 0.8750 - val_loss: 0.1767 - val_accuracy: 0.9565 - lr: 0.0086 - 369ms/epoch - 53ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 158/200\n",
      "7/7 - 0s - loss: 0.2850 - accuracy: 0.8750 - val_loss: 0.2093 - val_accuracy: 0.8696 - lr: 0.0084 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 159/200\n",
      "7/7 - 0s - loss: 0.2854 - accuracy: 0.8929 - val_loss: 0.1678 - val_accuracy: 0.9130 - lr: 0.0082 - 335ms/epoch - 48ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 160/200\n",
      "7/7 - 0s - loss: 0.3385 - accuracy: 0.8571 - val_loss: 0.1783 - val_accuracy: 0.9565 - lr: 0.0080 - 339ms/epoch - 48ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 161/200\n",
      "7/7 - 0s - loss: 0.3251 - accuracy: 0.8795 - val_loss: 0.2150 - val_accuracy: 0.9130 - lr: 0.0078 - 464ms/epoch - 66ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 162/200\n",
      "7/7 - 0s - loss: 0.2981 - accuracy: 0.8884 - val_loss: 0.1649 - val_accuracy: 0.9565 - lr: 0.0076 - 360ms/epoch - 51ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 163/200\n",
      "7/7 - 0s - loss: 0.2490 - accuracy: 0.9062 - val_loss: 0.1783 - val_accuracy: 0.9565 - lr: 0.0074 - 340ms/epoch - 49ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 164/200\n",
      "7/7 - 0s - loss: 0.2745 - accuracy: 0.8750 - val_loss: 0.1759 - val_accuracy: 0.9565 - lr: 0.0072 - 333ms/epoch - 48ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 165/200\n",
      "7/7 - 0s - loss: 0.2867 - accuracy: 0.8571 - val_loss: 0.2078 - val_accuracy: 0.8696 - lr: 0.0070 - 334ms/epoch - 48ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 166/200\n",
      "7/7 - 0s - loss: 0.3216 - accuracy: 0.8482 - val_loss: 0.1962 - val_accuracy: 0.8696 - lr: 0.0068 - 481ms/epoch - 69ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 167/200\n",
      "7/7 - 0s - loss: 0.3435 - accuracy: 0.8527 - val_loss: 0.1759 - val_accuracy: 0.9130 - lr: 0.0066 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 168/200\n",
      "7/7 - 0s - loss: 0.3601 - accuracy: 0.8571 - val_loss: 0.2899 - val_accuracy: 0.8261 - lr: 0.0064 - 365ms/epoch - 52ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 169/200\n",
      "7/7 - 0s - loss: 0.3295 - accuracy: 0.8393 - val_loss: 0.1777 - val_accuracy: 0.9565 - lr: 0.0062 - 368ms/epoch - 53ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 170/200\n",
      "7/7 - 0s - loss: 0.2894 - accuracy: 0.8839 - val_loss: 0.2699 - val_accuracy: 0.8696 - lr: 0.0060 - 347ms/epoch - 50ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 171/200\n",
      "7/7 - 0s - loss: 0.2840 - accuracy: 0.8438 - val_loss: 0.1860 - val_accuracy: 0.9565 - lr: 0.0058 - 424ms/epoch - 61ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 172/200\n",
      "7/7 - 0s - loss: 0.2822 - accuracy: 0.8839 - val_loss: 0.1569 - val_accuracy: 0.9565 - lr: 0.0056 - 370ms/epoch - 53ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 173/200\n",
      "7/7 - 0s - loss: 0.3300 - accuracy: 0.8571 - val_loss: 0.1904 - val_accuracy: 0.9130 - lr: 0.0054 - 344ms/epoch - 49ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 174/200\n",
      "7/7 - 0s - loss: 0.2780 - accuracy: 0.8616 - val_loss: 0.2072 - val_accuracy: 0.9565 - lr: 0.0052 - 346ms/epoch - 49ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 175/200\n",
      "7/7 - 0s - loss: 0.2504 - accuracy: 0.8929 - val_loss: 0.1797 - val_accuracy: 0.9565 - lr: 0.0050 - 324ms/epoch - 46ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 176/200\n",
      "7/7 - 1s - loss: 0.2520 - accuracy: 0.8929 - val_loss: 0.1802 - val_accuracy: 0.9565 - lr: 0.0048 - 673ms/epoch - 96ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 177/200\n",
      "7/7 - 0s - loss: 0.2589 - accuracy: 0.8839 - val_loss: 0.1953 - val_accuracy: 0.9130 - lr: 0.0046 - 341ms/epoch - 49ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 178/200\n",
      "7/7 - 0s - loss: 0.2512 - accuracy: 0.8884 - val_loss: 0.1722 - val_accuracy: 0.9565 - lr: 0.0044 - 349ms/epoch - 50ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 179/200\n",
      "======================EPOCH 000179:======================\n",
      "     epoch      loss       acc  val_loss  val_acc  rocp   f1  rfa\n",
      "178  179.0  0.238932  0.879464  0.182296      1.0   1.0  1.0  1.0\n",
      "RFA : 0.9999999999999999\n",
      "7/7 - 0s - loss: 0.2389 - accuracy: 0.8795 - val_loss: 0.1823 - val_accuracy: 1.0000 - lr: 0.0042 - 384ms/epoch - 55ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 180/200\n",
      "7/7 - 1s - loss: 0.2386 - accuracy: 0.8884 - val_loss: 0.2199 - val_accuracy: 0.8696 - lr: 0.0040 - 564ms/epoch - 81ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 181/200\n",
      "7/7 - 0s - loss: 0.2666 - accuracy: 0.8839 - val_loss: 0.1547 - val_accuracy: 0.9565 - lr: 0.0038 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 182/200\n",
      "7/7 - 0s - loss: 0.2718 - accuracy: 0.8929 - val_loss: 0.1416 - val_accuracy: 0.9565 - lr: 0.0036 - 366ms/epoch - 52ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 183/200\n",
      "7/7 - 0s - loss: 0.2434 - accuracy: 0.8884 - val_loss: 0.1379 - val_accuracy: 0.9565 - lr: 0.0034 - 344ms/epoch - 49ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 184/200\n",
      "7/7 - 0s - loss: 0.2896 - accuracy: 0.8795 - val_loss: 0.1373 - val_accuracy: 0.9565 - lr: 0.0032 - 357ms/epoch - 51ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 185/200\n",
      "7/7 - 0s - loss: 0.2754 - accuracy: 0.8795 - val_loss: 0.1489 - val_accuracy: 0.9565 - lr: 0.0030 - 423ms/epoch - 60ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 186/200\n",
      "7/7 - 0s - loss: 0.2273 - accuracy: 0.9062 - val_loss: 0.1582 - val_accuracy: 1.0000 - lr: 0.0028 - 405ms/epoch - 58ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 187/200\n",
      "7/7 - 0s - loss: 0.2496 - accuracy: 0.8839 - val_loss: 0.1517 - val_accuracy: 1.0000 - lr: 0.0026 - 356ms/epoch - 51ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 188/200\n",
      "7/7 - 0s - loss: 0.2846 - accuracy: 0.8884 - val_loss: 0.1270 - val_accuracy: 0.9565 - lr: 0.0024 - 350ms/epoch - 50ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 189/200\n",
      "7/7 - 0s - loss: 0.2341 - accuracy: 0.8884 - val_loss: 0.1934 - val_accuracy: 0.9565 - lr: 0.0022 - 354ms/epoch - 51ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 190/200\n",
      "7/7 - 0s - loss: 0.2256 - accuracy: 0.8884 - val_loss: 0.1355 - val_accuracy: 0.9565 - lr: 0.0020 - 404ms/epoch - 58ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 191/200\n",
      "7/7 - 0s - loss: 0.1815 - accuracy: 0.9286 - val_loss: 0.1497 - val_accuracy: 0.9565 - lr: 0.0018 - 405ms/epoch - 58ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 192/200\n",
      "7/7 - 0s - loss: 0.2612 - accuracy: 0.8750 - val_loss: 0.1406 - val_accuracy: 0.9565 - lr: 0.0016 - 346ms/epoch - 49ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 193/200\n",
      "7/7 - 0s - loss: 0.2385 - accuracy: 0.9107 - val_loss: 0.1526 - val_accuracy: 0.9565 - lr: 0.0014 - 358ms/epoch - 51ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 194/200\n",
      "7/7 - 0s - loss: 0.2123 - accuracy: 0.8973 - val_loss: 0.1281 - val_accuracy: 0.9565 - lr: 0.0012 - 361ms/epoch - 52ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 195/200\n",
      "7/7 - 0s - loss: 0.2296 - accuracy: 0.8929 - val_loss: 0.1345 - val_accuracy: 0.9565 - lr: 0.0010 - 370ms/epoch - 53ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 196/200\n",
      "7/7 - 0s - loss: 0.3141 - accuracy: 0.8661 - val_loss: 0.1542 - val_accuracy: 0.9565 - lr: 8.0009e-04 - 368ms/epoch - 53ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 197/200\n",
      "7/7 - 0s - loss: 0.2171 - accuracy: 0.9152 - val_loss: 0.1402 - val_accuracy: 0.9565 - lr: 6.0009e-04 - 353ms/epoch - 50ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 198/200\n",
      "7/7 - 0s - loss: 0.2210 - accuracy: 0.9107 - val_loss: 0.1349 - val_accuracy: 0.9565 - lr: 4.0010e-04 - 356ms/epoch - 51ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 199/200\n",
      "7/7 - 0s - loss: 0.2674 - accuracy: 0.8884 - val_loss: 0.1392 - val_accuracy: 0.9565 - lr: 2.0010e-04 - 365ms/epoch - 52ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 200/200\n",
      "7/7 - 0s - loss: 0.1948 - accuracy: 0.9107 - val_loss: 0.1361 - val_accuracy: 0.9565 - lr: 1.0000e-07 - 395ms/epoch - 56ms/step\n",
      "\n",
      "======================BEST:======================\n",
      "     epoch      loss       acc  val_loss  val_acc  rocp   f1  rfa\n",
      "178  179.0  0.238932  0.879464  0.182296      1.0   1.0  1.0  1.0\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.96        23\n",
      "   macro avg       0.96      0.96      0.96        23\n",
      "weighted avg       0.96      0.96      0.96        23\n",
      "\n",
      "[[11  1]\n",
      " [ 0 11]]\n",
      "ACCURACY: 0.9565217391304348\n",
      "PRECISION: 0.9166666666666666\n",
      "RECALL: 1.0\n",
      "F1: 0.9565217391304348\n",
      "ROC_AUC(Pr.): 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        23\n",
      "   macro avg       1.00      1.00      1.00        23\n",
      "weighted avg       1.00      1.00      1.00        23\n",
      "\n",
      "[[12  0]\n",
      " [ 0 11]]\n",
      "ACCURACY: 1.0\n",
      "PRECISION: 1.0\n",
      "RECALL: 1.0\n",
      "F1: 1.0\n",
      "ROC_AUC(Pr.): 1.0\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        23\n",
      "   macro avg       1.00      1.00      1.00        23\n",
      "weighted avg       1.00      1.00      1.00        23\n",
      "\n",
      "[[12  0]\n",
      " [ 0 11]]\n",
      "ACCURACY: 1.0\n",
      "PRECISION: 1.0\n",
      "RECALL: 1.0\n",
      "F1: 1.0\n",
      "ROC_AUC(Pr.): 1.0\n",
      "             acc       prec     recall         f1       rocp\n",
      "count  10.000000  10.000000  10.000000  10.000000  10.000000\n",
      "mean    0.934783   0.907509   0.974242   0.938517   0.965152\n",
      "std     0.042253   0.059247   0.041525   0.038163   0.029493\n",
      "min     0.869565   0.846154   0.909091   0.880000   0.916667\n",
      "25%     0.913043   0.848901   0.937500   0.918269   0.946970\n",
      "50%     0.956522   0.916667   1.000000   0.954451   0.958333\n",
      "75%     0.956522   0.923077   1.000000   0.959130   0.996212\n",
      "max     1.000000   1.000000   1.000000   1.000000   1.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAK9CAYAAAAT0TyCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3yT1f4H8M+TnXSXDlYLCCpTNpUhyFCWIKBMRUXAq9eBgl7AAYr3Ii6ceBG8WpUfSwRFWQplSVm2gChDdoEOWjqTNM16fn8EHghtaQNNnzT5vF+vvDzPyUnypY1tvznnfI8giqIIIiIiIiIiKpdC7gCIiIiIiIh8HRMnIiIiIiKiCjBxIiIiIiIiqgATJyIiIiIiogowcSIiIiIiIqoAEyciIiIiIqIKMHEiIiIiIiKqABMnIiIiIiKiCjBxIiIiIiIiqgATJyIiIiIiogowcSIioiqVmJgIQRCkm0qlQr169fDYY4/h/PnzZT5GFEV8++236N69O8LDw2EwGNCqVSvMmjULJpOp3NdatWoV+vfvj6ioKGg0GtStWxcjRoxAUlKSt/55REQUoARRFEW5gyAiIv+RmJiIcePGYdasWWjUqBEsFgt27dqFxMRENGzYEH/++Sd0Op003uFwYMyYMVi+fDnuuusuDBs2DAaDAdu3b8fixYvRvHlzbNy4EbGxsdJjRFHE448/jsTERLRt2xYPPvggateujYyMDKxatQopKSnYsWMHunTpIseXgIiI/JBK7gCIiMg/9e/fHx06dAAATJgwAVFRUXj77bexevVqjBgxQhr3zjvvYPny5XjxxRfx7rvvSv1PPPEERowYgSFDhuCxxx7DunXrpPvef/99JCYm4vnnn8fcuXMhCIJ03yuvvIJvv/0WKpW8v+JMJhOCgoJkjYGIiKoOl+oREVG1uOuuuwAAJ06ckPqKi4vx7rvv4rbbbsNbb71V6jGDBg3Co48+ivXr12PXrl3SY9566y00bdoU7733nlvSdNnYsWPRqVOn68bjdDrx0UcfoVWrVtDpdIiOjka/fv3w+++/AwBOnz4NQRCQmJhY6rGCIOD111+Xrl9//XUIgoBDhw5hzJgxiIiIQLdu3aT4zpw5U+o5pk+fDo1Gg7y8PKlv9+7d6NevH8LCwmAwGNCjRw/s2LHjuv8OIiKqHkyciIioWpw+fRoAEBERIfX99ttvyMvLw5gxY8qdIXrkkUcAAD///LP0mNzcXIwZMwZKpfKG4xk/fjyef/55xMXF4e2338a0adOg0+mkBO1GDB8+HGazGbNnz8bEiRMxYsQICIKA5cuXlxq7fPly3HvvvdLXIykpCd27d0dhYSFmzpyJ2bNnIz8/H7169cKePXtuOCYiIqoaXKpHREReUVBQgJycHFgsFuzevRtvvPEGtFot7rvvPmnMoUOHAACtW7cu93ku33f48GG3/7Zq1eqGY9u8eTMSExPx3HPP4aOPPpL6p0yZgpvZ+tu6dWssXrzYre/OO+/EsmXL8NJLL0l9e/fuxcmTJ6VZK1EU8eSTT6Jnz55Yt26dNIv2j3/8Ay1atMCrr76KX3755YbjIiKim8cZJyIi8oo+ffogOjoacXFxePDBBxEUFITVq1ejfv360piioiIAQEhISLnPc/m+wsJCt/9e7zEV+f777yEIAmbOnFnqvrKW/lXWk08+Wapv5MiRSElJcVuiuGzZMmi1Wtx///0AgP379+PYsWMYM2YMLl68iJycHOTk5MBkMqF3797Ytm0bnE7nDcdFREQ3j4kTERF5xbx58/Drr79ixYoVGDBgAHJycqDVat3GXE5+LidQZbk2uQoNDa3wMRU5ceIE6tati8jIyBt+jrI0atSoVN/w4cOhUCiwbNkyAK7Zpe+++w79+/eX/i3Hjh0DADz66KOIjo52u33xxRcoKSlBQUFBlcZKRESe4VI9IiLyik6dOklV9YYMGYJu3bphzJgxOHr0KIKDgwEAzZo1AwD88ccfGDJkSJnP88cffwAAmjdvDgBo2rQpAODgwYPlPqYqlDfz5HA4yn2MXq8v1Ve3bl3cddddWL58OV5++WXs2rULaWlpePvtt6Uxl2eT3n33XbRp06bM5778NSMiInlwxomIiLxOqVTirbfeQnp6Oj799FOpv1u3bggPD8fixYvLTUi++eYbAJD2RnXr1g0RERFYsmTJdZOY62ncuDHS09ORm5tb7pjLRRvy8/Pd+suqkFeRkSNH4sCBAzh69CiWLVsGg8GAQYMGucUDuGbT+vTpU+ZNrVZ7/LpERFR1mDgREVG1uPvuu9GpUyd8+OGHsFgsAACDwYAXX3wRR48exSuvvFLqMWvWrEFiYiL69u2LO++8U3rM1KlTcfjwYUydOrXMYg6LFi26biW6Bx54AKIo4o033ih13+XnCw0NRVRUFLZt2+Z2/2effVb5f/RVr6dUKrFkyRJ89913uO+++9zOeGrfvj0aN26M9957D0ajsdTjs7OzPX5NIiKqWlyqR0RE1eall17C8OHDkZiYKBVSmDZtGvbt24e3334bO3fuxAMPPAC9Xo/ffvsNixYtQrNmzfD111+Xep6//voL77//PjZv3owHH3wQtWvXRmZmJn744Qfs2bMHycnJ5cbRs2dPjB07Fh9//DGOHTuGfv36wel0Yvv27ejZsyeeeeYZAK6De+fMmYMJEyagQ4cO2LZtG/7++2+P/90xMTHo2bMn5s6di6KiIowcOdLtfoVCgS+++AL9+/dHixYtMG7cONSrVw/nz5/H5s2bERoaip9++snj1yUioiokEhERVaGvvvpKBCDu3bu31H0Oh0Ns3Lix2LhxY9Fut7v1f/XVV2LXrl3F0NBQUafTiS1atBDfeOMN0Wg0lvtaK1asEO+9914xMjJSVKlUYp06dcSRI0eKW7ZsqTBOu90uvvvuu2LTpk1FjUYjRkdHi/379xdTUlKkMWazWRw/frwYFhYmhoSEiCNGjBAvXLggAhBnzpwpjZs5c6YIQMzOzi739RYuXCgCEENCQsTi4uIyx+zbt08cNmyYWKtWLVGr1YoNGjQQR4wYIW7atKnCfw8REXmXIIo3cWAFERERERFRAOAeJyIiIiIiogowcSIiIiIiIqoAEyciIiIiIqIKMHEiIiIiIiKqABMnIiIiIiKiCjBxIiIiIiIiqkDAHYDrdDqRnp6OkJAQCIIgdzhERERERCQTURRRVFSEunXrQqG4/pxSwCVO6enpiIuLkzsMIiIiIiLyEWfPnkX9+vWvOybgEqeQkBAAri9OaGiozNEQEREREZFcCgsLERcXJ+UI1xNwidPl5XmhoaFMnIiIiIiIqFJbeFgcgoiIiIiIqAJMnIiIiIiIiCrAxImIiIiIiKgCTJyIiIiIiIgqwMSJiIiIiIioAkyciIiIiIiIKsDEiYiIiIiIqAJMnIiIiIiIiCrAxImIiIiIiKgCTJyIiIiIiIgqwMSJiIiIiIioAkyciIiIiIiIKsDEiYiIiIiIqAJMnIiIiIiIiCrAxImIiIiIiKgCTJyIiIiIiIgqwMSJiIiIiIioAkyciIiIiIiIKsDEiYiIiIiIqAJMnIiIiIiIiCrAxImIiIiIiKgCsiZO27Ztw6BBg1C3bl0IgoAffvihwsds2bIF7dq1g1arRZMmTZCYmOj1OImIiIiIKLDJmjiZTCa0bt0a8+bNq9T4U6dOYeDAgejZsyf279+P559/HhMmTMCGDRu8HCkREREREQUylZwv3r9/f/Tv37/S4+fPn49GjRrh/fffBwA0a9YMv/32Gz744AP07dvXW2FWq9zCAjhFJ0INwdCo1QAAi7UExmIzVEolwoNDb2hsvrEQdocDwXoDdBotAMBqs6HQbPRorEJQIDI07MpYUxHsdjsMWj0MOt11x2ZlX0CJrQShQaEID3P1W0tKkHnxAgAgvm6cNDbnYg7MJcUINgQjMjzC47F2mw3p2ZkAgLrRtaG69PXJzc+D0Wz0aKxBq0dUrSjp9dLSzwIAateKgUarve7YUyePAADq1I6HzmAAAFzMzUZh/kVotXrUrddAGnv8r4NwOB2oXT8eYRGRAIAL6eeRdzEbWp0eDW+9/bpj87Iv4EJmOjRqDRo1bX4lhiOHYLVZPRobU7suIqJjAAAFebnIPJcGpUKJJi1aVX5s85aA2QwAOHP8b5RYLAivFYWYOnUBAOaiIpw7cwoAcFvLO6Tn9WRs+pmTMBYZERoRidr16gMALMXFSDtxzOOx8Y1vhU6vBwBknj+HwrxcBIcEo26DW6Tn+PvPPzweW79BIxhCQlzfz4x05F/MgVanQ4Mmt0lj//pjLwCgQcPbEHzp/5nM9HO4mJMBrT4ITW698j068ud+OJw21KnXCJGX3mvZWRm4kHUOKrUWtze744bGHj38B+y2EsTE1kd0bB0AQO7FHGScP+XRWKVCjaYt20hjjx87hJJiE2pF1UHtuvU9HmssLMCZ038DAFrc0VEae+bkURiNhQgLj0L9+EYejy02m3Hy+F8AgFuatID+0v+f59JOoSA/x6OxwcGhaHDLlf8/y/p+ejLWk+99VbxPLn8/b/Z9cu3382bfJ+V9P2/2fXL19/Nm3yflfT9v9H1S3veTPyPKH8ufETXzZ8TZs2cRFxcHGAyAIKDGEH0EAHHVqlXXHXPXXXeJkyZNcuv78ssvxdDQ0HIfY7FYxIKCAul29uxZEYBYUFBQBVFXvTZrN4ixSfvEH3Zvl/reWbNcjE3aJ/ZfudRt7J0//STGJu0TF237Rer79JcfxNikfWLvH1a4je3x40oxNmmf+Pmm1VLfV1vWirFJ+8Suq390G3vvKtfrfbBupdS3fEeSGJu0T+ywZq3b2EErF4uxSfvE2T9fiW1tSrIYm7RPbLVuo9vYuxa7Yhi/cIHUt3LjOjE2aZ8Yt2G329he37pieGjhF1Lf5t2u562zMcVtbL/E/xNjk/aJD37xpdT317EjYmzSPjE2aZ+Yl58v9Q/+8hsxNmmfOPjLb6S+vPx8aexfx45I/Q9+8aUYm7RP7Jf4f26vV2djihibtE/cvDtZ6nto4RdibNI+sde3y93GNtr0mxibtE/8/ucrzzFpyXuu7+faRLext2/aJMYm7RPnL/xA6nsx8R3X93OdewytNq4XY5P2iXM/f1fqe+WLOWJs0j7xrg3uMXT4dbUYm7RPfOvzt6W+WZ+/LcYm7RM7/bLKbWyXX1aIsUn7xBkL50h97853xdD215/dxt69fokYm7RPnPrVO1LfpwvmirFJ+8Tmv24QxS5dRBHgjTfeeOONN954K/tmNIpyKygoECubG9So4hCZmZmIjY1164uNjUVhYSGKi4vLfMxbb72FsLAw6RYXF1fmODnkm4owdOX/YejK/0O+qUjucIiqjN5iAZKT5Q6DiIiIqMoIoiiKcgcBAIIgYNWqVRgyZEi5Y2677TaMGzcO06dPl/rWrl2LgQMHwmw2Q39p6czVSkpKUFJSIl0XFhYiLi4OBQUFCA0NLTW+OuUU5KFl6hkAwJ/tGiAqLIJL9S5/bWroUj2tICB935sAgDv6fIi0c67vb1lL9QSHgH0fuKbgB77zANLOHPebpXoqSwkad7rz0jc+C2cyznOpHrhUj8twAmcZjqdjuVTv+t97/owofyx/RtSMnxHPv/AcVq7ZAPGqv8nPnD6NqPh42ZfqFRYWIiwsrFK5QY1KnLp374527drhww8/lPq++uorPP/88ygoKKjU63jyxfE2s8WC93/9AQAw5Z4hUuJBNZetuAjbdrYBAHTvvB9qfUi5Y0vyivDFdNcPxAlvdYQ2ovyxNY7JBAQHu9pGIxAUJG88PqjEUYIZO2YAAGZ1nQWtUitzRERERN4hXJUcaTQat0kNuXmSG8haHMJTnTt3xtq1a936fv31V3Tu3FmmiG6OQafDa4NGyR2G15mNRXjv7bcBAC9OnQpDsB8lCNdQKNWIE56U2tej1GvQvnGB1PYaUZSKNFQbk6l6X68GcjgdWHvK9fNsZueZgFLmgIiIiLykSZMmOH78ONq1a4eUlBS5w7lhsiZORqMRx48fl65PnTqF/fv3IzIyEvHx8Zg+fTrOnz+Pb775BgDw5JNP4tNPP8W//vUvPP7440hKSsLy5cuxZs0auf4JVAmiU4RTqZLa/kyp0eG2ni9VaqxKp8WdLw31bkCiCHTrxv1GPkitVONfHf8ltYmIiPzFunXrMGLECBQVufbwHzt2DGazGYZLyxhrKlkTp99//x09e/aUridPngwAePTRR5GYmIiMjAykpaVJ9zdq1Ahr1qzBCy+8gI8++gj169fHF198UWNLkdvtdmw7tB8A0L15G6hUNWoCsNK0Bj1GDLpPalM1MpvlTZq6dnWVGqVS1Ao1xjYfK3cYREREVWrgwIHSCrGIiAjk5eUBQI1PmgAf2uNUXXxpj1NZxSGoZnM6HLDkngcA6CLrQaEsf/2V0+7AxT9OAgBq3XELFCovrNW6eq9RVlb17zWqaeczEBER0Q0LDQ2VZpkAICgoCEajUcaIKua3e5z8UYS1ckUtqApUw14fR3ERdu/pAQDolrATiusUh7DlFWHlPFd1n8feCPFOcYir9xoFBbFIgw9xik5kmDIAAHWC6kAh1KjTIYiIiCRmsxlB1/yNcdddd2Hbtm0yReQdnHEir7OWlOCXlSsAAPcOe1Aq4V2tuNeH1e18jNlmRsLiBADA7jG7YVDX/CUMREQUeFatWoVhw4a59S1ZsgSjRtWMAmiccSKfYispwe9HXWfl9CwpkSdxknuvj9y418gn6VXc80dERDXbAw884HZtMpn8Yj9TWZg4kdcpVSqEKK60ZSfHXh+5ca+RzzGoDdjz0B65wyAiIropRqMRQUFBCA4Odtvf5I+4VE9G+aYi/GP9DwCAz/sNQXiQ/55vJDseyEpERER008xmM4KDg/H5559j4sSJcodz0zzJDbgbWUZ2ux1bI1tha2Qr2O12ucPxXaLoSnxu9lYNHNZiHFj/PA6sfx4Oa/F1x9pMFvz07CL89Owi2EyWaomPiIiI6EZ9/fXXCAoKgiiKeOKJJ2D2ctEtX+MD66YCl0Grx+MFf15qN5U5Gh9Vw4o6OB125Gh+utR+E9crMO602pBmqyu1EaSrhgjJV1gdVszePRsA8HLCy9AoNTJHREREVL5OnTph7969cochKyZOMjLodJg95GG5w/A6s7EIc99+GwAweepUGII9WJJY1UUdvFwkQaFUo47zEal9PUqtGq3r50ptCix2px3fH/seAPCvjv9i4kRERD5Lp9OhpKREug4PD5cOtg0kTJzI60SnCLtSJbVvWFUUdfBykQSlRofmfWZWaqzKoEO3Vx/0Wizk29QKNZ5t+6zUJiIi8jU5OTmIjo5263vwwQfx3XffyRSRvJg4ychut2P/KVeZ7jaNboXKFyrOeYHWoMeg3r2k9g3jAa7kR9RKNZ644wm5wyAiIirXG2+84Xa9Y8cOdOnSRaZo5MeqejLKKchDy9QzAIA/2zVAVFiErPH4pBpWDc/pdMJmygcAqIPCoVCUX3/FaXeg8FQGACC0UR0oVNfbEUVERERU/SIjI5Gfnw+n0yl3KF7BA3BrEL2D1dT8iaPEhN/2dgQAdO+8Hwp9+fu5bEVm/N/7fwMAJrwVBm0Ey9EHElEUkVfiWh8eoY2AwHO2iIjIB6jVruXjNpsNAJCbmytnOD6FiZOMosIicKrPnXKH4XV2mw2bf14NAOh532Co1NzPcZngdMgdAsmk2F6MHst6AAB2j9kNg9o/T1knIqKa4dr9TK1bt8aBAwdkjMj3MHEiryspLsaOAwcBAF163+PXiZNSG4S77zoEABAqqKqnjQjBPxfcUx1hEREREZVr5syZmDVrllvfV199JVM0vouJE3mdoFTAAKfU9mcKhQJQaOUOg2oAg9qAg48elDsMIiIKcLfeeiuOHz8uXQuC4Lf7mW4WEycZFZqNeGat6xyXTwc8gFBDsMwReYchKBj/en1WxQOJiIiIqNqoVCo4HFe2DdStWxfnz5+XMSLf5t8f//s4q82GX2q1xi+1WsN6aQMe1WwOqwV//jINf/4yDQ7r9Qt/2EwWrJu8COsmL4LNxCIhREREVL2uTpqeeeYZJk0V4IyTjHQaHcbk/XGpfbvM0VBVcDpsyFK5DoW73fEKlNCVP9Zqw0lzXamNoPLHkv+xOqz4IOUDAMAL7V+ARqmROSIiIgo0hw8fRrNmzXD48GE0bdpU7nB8HhMnGQXr9Zg77BG5w/A6s8mID9+eAwB4fuo0GIL8c0kiACiUasTah0vt61Fq1WgenS21KbDYnXYsOrwIAPBs22eZOBERkdfFx8fj7Nmz0kG2TZs2RYAd6XpTmDiR14kOJ6wKldT2Z0qNDi3vnVOpsSqDDj3fHOnliMhXqRVqTGw1UWoTERF5k0KhkJKkrl27MmG6AUycZOR0OHAyKx0AcEtsXSiUSpkj8g6tQY97u3aW2uUSRcBsdu8zmbwYGZF81Eo1nmv3nNxhEBGRn0tNTUX79u3d+mbMmCFTNDUbEycZ5RoL0e3oRQDAn0HBiAqLkDki71Cp1OhyT9/rDxJFoFs3IDm5eoLyEqfTCdHhKvQhKNWu8uTXGWvJzgcA6KLDrzuWiIiIyFMTJ07EF1984dZ35swZxMfHyxRRzcbEiXyD2Xz9pKlrV8BgqL54bpCjxIRtO9sAALp33g+FPqTcsbYCE76auR8AMOGtjtBGlD+W/I8oiii2FwMA9Co9BEGQOSIiIvIn9erVQ3p6unStUqlgYxXnm8LESUZRYRHI7Omfs0xXs9tsSN64AQDQpU9fqNQV7OfIygKCgtz7DAaAf1iSHym2FyNhcQIAYPeY3TCoff+DASIiqjlq164tJU7NmzfHX3/9JXNENR8TJ/K6kuJiJO3+HQDQvmv3ihOnoKDSiVMNodQGoVvHvVL7etRhQfjHh10BAAoNiwMQERHRzTGbzTBcWqGTkpKCqKgovPjii5g2bZrMkfkHJk7kdYJSAa3TLrX9mUKhgDYkstJjFTqtlyMiX6VX6bF7zG6pTUREdDNGjx6NpUuXQq/Xw3yp2FZOTo7MUfkXQQywWoSFhYUICwtDQUEBQkND5Y3FbMTkdd8DAOb2fwChBv8936hCJhMQfOnfbzTW2BknIiIiouoWGRmJvLw86ZoH2laeJ7kBZ5xkZLXZ8HNkawDAHG7W8wsOqwVHt70NALi9+1QoNbpyx9rNFmx+YyUAoOfMYVAZyh9LREREdC2z2Yygaz5sbteuHZMmL/HvdVM+TqfR4YHcA3gg9wB01/kDm2oOp8OGDMU3yFB8A6fj+smwo8SGvwtq4++C2nCUMHEONDaHDR+nfoyPUz+GrYL3ChER0bXWrVtXKmlasGABUlJSZIrI/3HGSUbBej3mPfCo3GF4ndlkxMfvuGZhnvvXVBiC/HdJokKpQpR1kNS+HqVWjdvCMqU2BRab04aFBxcCACa0mgC1ku8BIiKqnHnz5uGZZ55x6zOZTFJhCPIOJk7kdaLDCYuglNr+TKnRo3W/Dys1VmXQ4Z63x3g3IPJZKoUKDzd7WGoTERFV1rhx46TE6epiEORd/G0tI6fDgVxjIQAgMjgUCqVS5oi8Q6vXo3v7NlKbiACNUoOpnabKHQYREdUQZrMZW7duRf/+/WEwGLBy5UrMmzcPGzdulDu0gMGqejLKKchDy9QzAIA/2zVAVJj/H4ZbrgCsqud0OuG0uva2KDRqKBTcckhERESlLV26FKNHjwYAZGdnIyoqSuaI/Aer6hHJxFZchG072gMAundNgVofUv7YAhO+mO46LHfCWx2hjSh/LBEREQWmu+++G1u3bpWub7/9dly8eFHGiAIXEycZRQaH4s92DaS2v7LbbUjdvg0A0O6u7lCp/HwTvMIhdwRUA5htZiQsTgAA7B6zGwY1N/QSEZE7g8GA4uJi6To4OJhJk4yYOMlIoVQGxPK8EnMx1m7dDgBo0b4jVKH+mzgpNQZ0brVZal+POiwI495oI7WJiIiIgLLPZxowYADWrFkjU0QEMHGiaiAoBKgddqntzxRKJQzR8ZUbq1DAEBvp5YjIV+lVemwduVVqExERXXZt0rR27Vr0799fpmjoMiZOMjIWF2Pq2uUAgLcHjECwn1acMwSH4JU3/33dMU6Hs8zTmO1WB0QRUKoEKJSuEU6nCIfNCUEAVBplpcZCANRXj7U5IDoBhUqA8gbGik4RdpurtLpae2Wsw+aE0ylCoRSgVFU81uFwQlCL0hk+oiii2O6akter9BAEV6Jpc9hgc9qgVqgrPValUEGj1EivZ7a5SpXqVDooBIXnY5022Bw2KBVKaJVaaWyxvRiiKEKr1EKpUHo81u60w+qwQiEooFNdOQjaYrfAKTo9GqtRaqTS3g6nAyWOEgiC4JaYeDK2xFECh9MBtVINtcL1dXeKTljsFgBwW17nyVirwwq7044QdQjPbyIiolLq1KmDjIwMADyfyZewjJeMLFYLvo9sje8jW8NitcgdjqzO/FX2et1V76diwaStSDuUK/WdP5KHBZO2YsU77idj//TJASyYtBUn9+dIfVknC7Bg0lYse3OP29j1n/+JBZO24u/dWVLfxfNGLJi0Ff/32k63sRu/OoQFk7bi0PZ0qa8guxgLJm1F4rQdbmO3LPoDmxe8gtRV/4bj0vfUVGDFgklb8cUL29zGbl18GAuf34TlL34Cu9k1tshWhITFCUhYnAC7aJfGfrzvYyQsTsDH+z6W+uyiXRpbZCuS+hceXIiExQl4Z+87bq/XdUlXJCxOQLY5W+pbdHgREhYn4I2db7iN7fNdHyQsTsDZorNS34q/VyBhcQKmb5/uNva+lfchYXECjuUfk/rWnFyDhMUJeGHzC25jH1z9IBIWJ+CPnD+kvk1pm5CwOAFPbXzKbezDax9GwuIE7M7cLfUlpycjYXECHlv/mNvYib9MRMLiBGw9d2XzbOqFVCQsTsDon0e7jX026VkkLE7A+tPrpb7DuYeRsDgBQ34Y4jb2pa0vIWFxAn48/qPUdzL/JBIWJ6Df9/3cxs7YMQMJixOw9MhSqS/DlIGExQm4e/ndbmNn756NhMUJ+Oqvr0BERAQAY8eOldrp6emYMWMGRFFk0uRDOOMkI41ajftyD1xqN5E5GqoaDihbfIdCAE7HC1BCV+5IJ+wA1BAzQ1BsNSLEUP5YIiIi8k85OTmIjo4GAGzcuFGaaXrjjTeu9zCSAc9xIq8zm4yY955r9uPpF/8FQ5DrvCanU0TGsXwAQO1YFZQRl74fV53jVNOW6llNZvy55XlAENC6z4dQavTljjUXFWLjvx6DKNrQ74Ml0OqDuVQvwJbqXf39JCKiwDN37lxMmTLFrS/A/jSXnSe5ARMn8jpTYSHenTsXAPDS5MkIuvR1t5U4sGCSa2nVE7M7QF0rzPWAADkAl4iIiAJXq1at8Oeff0rXgiDA6XTKGFFg4gG4NYgoihBtTiiumuEQ7U6IThGCQoBwucDApXEAIKgV0uxCWWMBwGl1eD5WpZCq3okOJ0SHCEEQIKhvbqxGq0NCi6YAALVaBVtxEQRBASh0iKhzaWbJYsS1n7s77DY4bRYIggIq3ZVEym4xQRSdUKh1UF46E8rpsMNhLfZoLCBArQ++MrakGKLTDoVKA6VaewNjHXBYXTM11zv4loiIiAKbWq2G3X5lL3NMTAyysrKu8wjyBSwOIaPs/FzU2XIAdXccRE5BntRfsO4U0mcko3BTmtQn2pxIn5GM9BnJUgIFAIWb0pA+IxkF6065PfflsU6TTeor2nYO6TOSkf/jCbexGW/uQvqMZDjyS6Q+484MpM9IRu73f7uNzXx7D9JnJMOebZb6TClZSJ+RjItLjriNzZqb4or3Qgn6Dx+F/sNHIePA99i2sw1SNo6FWqPEmJkJGDMzAft2jij19ck4sBrbdrbB3k3D3fr3bBqKbTvbIPPPdVLfhUO/YtvONti9aZDb2N83jsa2nW2Qvu97qe/isR3YtrMNdm3u6zY29ddHsW1nG6Tt/T+pL//Mfmzb2QbJW+92G7t/4z+wbWcbnNq18MrXLPMYtu1sg9+2dyn1byEiIiICgJCQELekacKECUyaagjOOMnJdmWVpGjl1GygMRZcxKnO3QAAjXb+huCwWjJHRERERN526tQpqRhESkoK2rVrJ3NEVFnc4yQjh92Ok2fOQbQ50bhJPJQqVx7rb0v17KIVB393zfi0bDcECoilltTZcrKgjq7turi0x8nfl+oZCy7ibIIrcYrbzcSJiIjIX7Vq1QqzZs3C0KFDAQCpqalMmHwEi0Nchy8lToGiMC8Le/e5lq91bJuM0IhYAK4qeGs+c53nM/DRxlBFBlZxCIfDjgtpruWNMfFNoVRyApiIiMjfXL2fiYfZ+h5PcgPucSKvExQCLMVBsBQHSbNUACCKwLkjeTh3JA+Blb67KJUq1GnUEnUatWTSRERE5GeOHDkCQRDc9jOtXr1axojoZvGvNRkVFZkw+8ctAICX778bISH+OcsSEhaDgQP/KNWvVAnoM6651CYiIiLyBy+++CLef/99t77Dhw+jadOmMkVEVYGJk4wsFgu+qlcPADDZYvHbxKk8CqUCtydc2tdkMskbjAysxWYkzZ0MAOg1eS40ek7dExER1XTx8fE4e/asdK1UKt1mnajmYuIkI41Wg4EZF1zt9o1ljoaqm9VajAbfug4Atj5TzMSJiIiohktOTnZLmm655RacOHHiOo+gmoSJk4zCQkPwvzH3yh2G15mNeVi/zlVFpl//VTAERwAAnE4R2WlFAIDoWoqA23Cn1mhxoks8AOAWjVbmaIiIiOhmdenSBUFBQTCZTJgxYwbeeOMNuUOiKsTEibzObrMirNZZqX2Zw+bEijm/AwCemN0h4BInrT4Y9325Qe4wiIiI6CaMGzcO33//PQoLCwEARqNR5ojIW5g4kddpdEGw5N8ntSUCEBKpk9pERERENUlsbCwuXHBtu2jdujUOHDggc0TkTUycZJSdfREd96cBAPa2iUd0tH8egKrTB2PgsI9K9as1Sjwy23W+UyAWhyAiIqKaS6FQ4OrjUJ1Op4zRUHUItNVRPseiEmBhKe6AZCy4iJTWzZHSujmMBRflDoeIiIgqITk5GYIguCVN77//Pg4ePChjVFQdOOMko/CwUPwUrpba/spht+HwH+sAAM3u6A+lSi1zRL7DUBKAJ/8SERHVUMOHD8eKFSvc+rKzsxEVFSVTRFSdmDjJSK1Ro2PbFnKH4XWmolxk5b8AAIgvSkBoRCwAwG5z4Jcv/gIA3Du6YcC9GfXBYdCt+J/UJiIiIt92ddKk0WhQUlIiYzRU3bhUj6qFzaqFzepeclt0AqcO5ODUgRyIAbgsWKlUoVHLLmjUsguUykBLG4mIiGqeHTt2AADatWvHpCkA8a81GZlMZrz3QxIA4MUhvRAU5J8HoIZGxKJfv0Ol+hUqAXc/dLvUJiIiIvIl69atw4ABA3D48GE0bdoUXbp0cdvbRIGFiZOMzOZi/LdufQDAP83Ffps4lUepVKDFXfVcFwFYVc9aYsbWea8CAHo8/W9otIH1/SciIvJlAwcOxNq1awEAzZo1Y8JETJzkpNaocE+Wq5qaul0jmaOh6ma1FKP+AlfRDOv4V5g4ERER+YjQ0FAUFRVJ13q9XsZoyFcwcZJReFgYvh3VW+4wvM5szMO6tSMAAP0HLIchOAIAIDpF5Ga6ZpoiQwLvDFylWo2T7WoDABqqWWmQiIhIbmazGUFBQW59d911F7Zt2yZTRORLmDiR19ltVoRHnZTaV/qdWDprDwDgidkdEGipg94QioGLN8sdBhEREQFYunQpRo8e7da3ZMkSjBo1SqaIyNcwcSLvEUXAbIbGAVjOdwcAaO7Alf1MJQ4Ea+2udgDucSIiIiLf8eabb7pdm0wmGAxcRk9XCGKA7XQrLCxEWFgYCgoKEBoq76Gz2dkXcVfqaQDA9nYNER1dS9Z4qpQoAt26AcnJnj/WaASumSYnIiIi8ja1Wg2dTue2v4n8mye5Ac9xklm+Rol8jVLuMKqe2XxjSVPXrkCAfLpjKsrFzo4tsLNjC5iKcuUOh4iIKKCYzWYIgoCIiAipz2azMWmicjFxklF4WCgWq21YrLYhPEze2S9vcpw/h5MpG3AyZQMc+XmuGaXybtu3A0JglIkQnSLCi5wIL3JCdAbUxC8REZGsFi5cKBWByM/Px8yZM2WOiGoC7nGSkVqjRq9uHeUOw+tMzhKcyn8KABDlTEZoUDgAwG5zIOmbIwCAXo80hUrthzNv16EzhACJc6+0iYiIyOs6deqEvXv3uvVNnTpVpmioJmHiRDfvUhEIN9cUe3A4SidFohM4tjcLANDz4aZeC89XqdQaNLuzv9xhEBERBQytVgur9UqF34iICOTmcrk8VQ4TJxmZTGZ8+mMSAOCZ+3shKKgG7u2pRBGI0IhY3HvP36X6FSoB3YbfKrWJiIiIvCEnJwfR0dFufaNGjcKSJUtkiohqIiZOMjKbi/FBnfoAgMfNxTUzcaqoCMR1ij0olQq07h3npcB8n7XEjN++egsA0G3cdGi0NfD7T0REVANcW1Z8x44d6NKli0zRUE3FxElGao0Kd2XnXWo3kjmaKpCVVbqMuMEQMMUePGW1FKPOhytc7dHPM3EiIiLyEoPBgAULFuCpp56C3W6XOxyqoXiOE90ckwkIDna1yzl/yWwqwLqfXSdx979vCQxBYQBcVeWKci0AgJBIHQRFYCVYxeZCbBnr2uN097froDfw/UhERFRV1Go17HY7D7Kl6/IkN+CME3md3WpBePRRqY1LiZPd5sS3r+4EADzxUQ+otYFVVU9vCEX/73fIHQYREZFfSUtLQ4MGDaTr4OBgOJ1OGSMif8FznMjrNDo9irK7oii7KzQ6vdt9Ko0CKg3fhkRERHTzZs6c6ZY0AcDvv/8uUzTkb7hUT0YXc/PQZ9dxAMDGO5ugVmREBY/wQZVYqkdERETkbbfeeiuOHz8uXQuCwJkmqhCX6tUQTocTGXq11KbAYirKRUq/HgCA9uu3IigkUuaIiIiIaqbL+5kui4uLQ1pamowRkT/iGikZhYaGYKHDiIUOI0JDQ+QOx2scDgcyzx1C5rlDcDgccofjM0SniOiLdkRftEN0BtTELxERUZUKCbnyd9QzzzzDpIm8gjNOMtJqNRjUp5vcYXidqTAHf/09CADQMSgZoRGxAACHzYltS11FI7qPuh1KdWDl8TpDCErmzZTaREREVHk5OTmIiooCAOTm5qJWrVrYsWMHmjZtKnNk5K+YOJFsnE4Rh3ZkAAC6jbgNgVVTD1CpNWjTe5TcYRAREdU48fHxOHv2LG655RacOHECAHDx4kWZoyJ/x8RJRsXFFixYvQkA8MTg3tDrdTJH5B2hEbHo3etEqX6FUkDC4FukNhEREVFFFAoFLtc2O3nypMzRUCBh4iQjo9GEt2LqAQDGGE1+mziVR6lSoMOAhnKHIRub1YKdSz8CAHQeNQlqTWB9/4mIiDyRmpqK9u3bu/XNmDFDpmgoEDFxkpFKpUTHi4VSmwJLSbEJ0bMTXe37JzBxIiIiKsfEiRPxxRdfuPWdOXMG8fHxMkVEgYiJk4wiIsLx04Pd5Q7D68ymAqz/6VEAQL9BX8MQFAYAEEUR5sISFBsLEFE7CkqlK3m0FpthtRZDrdFCqw+WnsdY4Fq7rA8Og1LpeutaS8ywWoqhVKuhN4Te0FhTUS5EpwidIQQqtQaAazaopNgEhVIJQ3D4DY01G/PhdDig1QdJSZHdZoXFXASFUgmFUom0xq6iEA2UTJyJiIjKcvfdd2Pr1q3StUqlgs1mkzEiClSBVcaMZGG3WhAWcxBhMQdht1qkfqvFjsSpyVj25l/IPntljfKmt5/F2YRu+HXSSLfnOd6tG84mdEP6iQNS3+YPp+JsQjdsenKY29hDPe/C2YRuOLn/yg/a7Qtm4WxCN2x+7D63sfvv6Y6zCd1wZNdaqW/Ht+/gbEI3bHuov9vYvQN74mxCN/yxcanUt+f7z3A2oRt+e7CP29jkIb1wNqEbUlb/T+pLXfc1ziZ0w65BPWEIDkffNXvQd80et4SLiIiIrpg/f77UbtmyJZMmkg1nnMjrNDo9CrM7SO3LzMY8qV1sKqj2uIiIiMg3JScnAwC6dOmCpk2bYurUqYiJicHkyZNljowCmSBeLksSIAoLCxEWFoaCggKEhoZW/AAvupibh/t2/A0A+LnrbagVGSFrPDfEZAKCLy2nMxqBoCCPn8JYcBGGkAgoFK4J0EBZqsdZJiIiotKGDx+OFStWAAAC7M9UkoEnuQFnnGTkdDhxKlgrtQNVcFgtt2uN3gCN3lDhOADQaA3QaG9ubFBIZKk+tUZXZrEGT8aWlRip1JoyYyMiIiIgIiIC+fn50vXgwYOxevVq+QIiugoTJxmFhobgA1PupXZzmaPxHofDAVNhDgAgKPRKEQgiIiIiADCbzQi6ZtVKx44dmTSRT2FxCBlptRqMvq8XRt/XC1qtRu5wvMZUmIO9+7pg774uUgIFAOaiQnzzz7n45p9zYS4qlDFCIiIiksu6detKJU0LFizAnj17ZIqIqGyccSLZlFgsKHK2kdqGEHn3nBEREVH1MpvNGDBggFufyWSCwVB6aT2R3Jg4yai42IJv1m4GADwyoCf0ev88ADUoNAod2yZL7ct0OgPCtPsvtTvJERoRERHJyGAwQKPRwGq1Qq/Xw2w2yx0SUbmYOMnIaDRhZmQdAMAwo8m3EydRBMr6YWYyVfhQpVKJ0IjYUv36kGA8/BHLihIREQUSs9mMwYMHY+PGjQCAkpISzJs3D08//bTMkRFdH/c4yUilUuKOfBPuyDdBpfLhggmiCHTr5io7fu0ttnRCRERERFSWpUuXIigoCJs2bUL37t2lfiZNVBPwHCeq2NVnNZWna1dg+3ZAEErdZSkuxLrVEwEA/QcvhE7v+ro7HA4U5rrOWwqNrMVqe0RERH6se/fu2L59u3StUCjgcDhkjIiI5ziRN2VllX3IrcFQZtIEAFZLMUKjf5falxOnwtyLWPzanwCAMW+2RER0jHdiJiIiIlkZDAYUFxdL18HBwSgqKpIxIiLPcakeeSYoqOxbOUkTAKg0OhRcaIWCC62gKuOgWCIiIvJPZrMZgiC4JU0DBgxg0kQ1EpfqySgvLx/Dth4GAKzs0QwREeGyxlOuq5fqGY1lzzjdAC7VIyIi8m/CNR+srl27Fv3795cpGqLSuFSvhrDbHTgcppfagUapVHJ5HhERkR9bu3atdE4Tz2eimo5L9WQUHByE2fmZmJ2fieDgqpnFISIiIpJTVFQUcnJyAAD9+/fHmTNnIIoikyaq8WRPnObNm4eGDRtCp9MhISEBe/bsue74Dz/8ELfffjv0ej3i4uLwwgsvwGKxVFO0VUuv1+Hxof3w+NB+vn2G000qzMvCpqTG2JTUGIV5WVJ/cZERiybNxaJJc1FcZJQxQiIiIrpZOTk5EAQBFy9eRHR0tNQfHx8vY1REVUfWxGnZsmWYPHkyZs6cidTUVLRu3Rp9+/bFhQsXyhy/ePFiTJs2DTNnzsThw4fxv//9D8uWLcPLL79czZFTVbBYzCgoaYOCkjawWHhSOBERUU01Z84ct2QJcBWGIPInsu5xmjt3LiZOnIhx48YBAObPn481a9bgyy+/xLRp00qNT05ORteuXTFmzBgAQMOGDTF69Gjs3r27WuOuKiUlVixfvxUAMKJfD2i1Gpkjguuw22t/0JlMN/WUQaFRaHHbT1L7Mq1OhxDF/kvtDjf1GkRERCSPFi1a4NChQ9K1IAhwOp0yRkTkHbIlTlarFSkpKZg+fbrUp1Ao0KdPH+zcubPMx3Tp0gWLFi3Cnj170KlTJ5w8eRJr167F2LFjy32dkpISlJSUSNeFhYVV94+4SYWFRXgp1PXpTL/CIkRH15I3IFEEunUDkpOr9GmVSiVq129eqt8QEopHPptcpa9FRERE1UetVsNut0vXMTExyMrKus4jiGou2Zbq5eTkwOFwIDY21q0/NjYWmZmZZT5mzJgxmDVrFrp16wa1Wo3GjRvj7rvvvu5SvbfeegthYWHSLS4urkr/HTdDpVLi1iILbi2yQKXygVLcZvP1k6auXV0H3RIREVHAGzt2rFvSNGHCBCZN5NdkLw7hiS1btmD27Nn47LPPkJqaipUrV2LNmjV48803y33M9OnTUVBQIN3Onj1bjRFfX0REOLYPvhPbB9/pe2c4ZWW5zmy6+rZ9+3UPui2PpbgQPyx7BD8sewSWYt+Z8SMiIqIb9+2330rnNKWkpGDhwoUyR0TkXbIt1YuKioJSqSz1yURWVhZq165d5mNee+01jB07FhMmTAAAtGrVCiaTCU888QReeeUVKBSl80CtVgutVlv1/wB/FxRUZQfdWi3FCIneIbV1etfhYnnZF7D4tT8BAGPebMkznYiIiHxc48aNYbFYcP78eQDgXiYKKLLNOGk0GrRv3x6bNm2S+pxOJzZt2oTOnTuX+Riz2VwqOVIqXUvcRFH0XrB0U1QaHfKzb0d+9u1Qafy37DoREZE/U6lUOHnyJNLT0zFx4kS5wyGqdrJW1Zs8eTIeffRRdOjQAZ06dcKHH34Ik8kkVdl75JFHUK9ePbz11lsAgEGDBmHu3Llo27YtEhIScPz4cbz22msYNGiQlEDVJHl5+Riz2TXjsrhnS99brldFDEFheGDk2lL9oZG1cP+U+lKbiIiIfM+RI0fQrFkzt76wsDCZoiGSj6yJ08iRI5GdnY0ZM2YgMzMTbdq0wfr166WCEWlpaW4zTK+++ioEQcCrr76K8+fPIzo6GoMGDcJ//vMfuf4JN8Vud2BfRLDUDjRKpRL1b71N7jCIiIioHC+++CLef/99t74zZ87wUFsKSIIYYGvcCgsLERYWhoKCAoSGhsoaS3GxBV/+lAQAeHxQL+j1Mi9jM5mAYFciB6OxyvY4ERERUc0THx/vVlRLqVS6VdEj8gee5AayzjgFOr1eh6dHDJA7DK8rzMvCrt/vAgDc2WE7QiNcM4rFRUasemMBAGDozCegDwmWLUYiIiJyd3XS1KRJExw7dkzGaIjkV6PKkVPNpVQ6oFS6L0e0WMzIM7ZBnrENLBazTJERERFRWS7vMZ81axaTJiIwcZJVSYkV36/fgu/Xb0FJiVXucLwmKCQSDesmomHdRASFREr9Wp0OweIfCBb/gFbHantERERyGjduHARBgNns+jBz2rRpEEURr732msyREfkG7nGSUXb2RbT60zUNfrBlHKKjZa4sxz1OREREASk2NhYXLlyQrgPsz0MKYNzjVEMolArEmaxSm4iIiKi6KRQKt0SpZcuWMkZD5Lv417qMakVGYO99nbD3vk6oFRkhdzheYyk2YvV3T2D1d0/AUmyUOxwiIiICkJycDEEQ3JKm999/HwcPHpQxKiLfxRkn8jqrxYSgWpuktk7vWg6Yl30BS19OAQCMmt0eEdExssVIREQUSIYPH44VK1a49WVnZyMqKkqmiIh8HxMn8jqVWoP8nFuk9tWcSq0cIREREQW09evXS22tVguLxSJjNEQ1AxMnGeUXFGDcL/sBAF/d2wbhYWHyBuQlhuAIPDDi11L9IeER6DM+RGoTERFR9SgqKoJCoUCHDh2wZ88eucMhqhGYOMnIZrVjZ1SY1A40KrUat3fsKHcYREREfm/dunUYMGAA7rjjDhw4cAAA4HQ6ZY6KqGZh4iQjg0GPFzPPu9qdbpM5GiIiIvJHAwcOxNq1awEAf/zxB9LS0hAfHy9zVEQ1DxMnGQUFGfDi6IFyh+F1hXlZSN7dEwDQJWEzQiNiAQAWsxE//vsLAMD9r06AzhAsW4xERET+KDQ0FEVFRdK1Xq9n0kR0g1iOnKqFWlMCtabEra/YZEZO7h3Iyb0DxSazTJERERH5H7PZDEEQ3JKmHj16wGzm71uiG8XESUYlJVas37IT67fsREmJVe5wvCYoJBKx4R8gNvwDBIVESv0qjQYG+58w2P+ESqO5zjMQERFRZa1atQpBQUFufUuWLMGWLVvkCYjIT3CpnowKC4vwmKgHABwsLEJ0dC2ZI/IOpUqNlu0Gl+oPCQvHuC+ekyEiIiIi/xUbG+t2bTKZYDAYZIqGyH9wxklGCqUC0RY7oi12KJT8VhAREdHN69KlCx5++GGEhIRAFEUmTURVRBBFUZQ7iOpUWFiIsLAwFBQUIDQ0VO5wfIvJBARfKtBgNALXTPPfKEuxEZvWvQIA6N3/P9DpWQSCiIioqpjNZmlpXoD9WUd00zzJDTjNQV5ntZigC/8ZuvCfYbWYpP687Av4fPyP+Hz8j8jLviBjhERERDXTwoUL3fYzRUVFyRgNkX/jHifyOpVag4KLcVL7anZ1iBwhERER1Xjt27dHamqqW19aWppM0RD5PyZOMsovKMCT61MAAB/d0wqxkdHSfcX2YoiiCK1SC6VCCQCwOW2wOWxQKpTQKrXXHWt32mF1WKEQFNCpdNJYi90Cp+gse6zdgisjr4zVKDVQKVxvFYfTgRJHCQRBgF6lr9xYnQ7Dhm+RxpY4SuBwOqAPDUGXB1xLCkLCI272y0lERBQwtFotrNYrFXkjIiKQm5srY0RE/o9L9WRktdqwJSYSW2Ii8dbet93uu2/lfUhYnIBj+cekvjUn1yBhcQJe2PyC29gHVz+IhMUJ+CPnD6lvU9omJCxOwFMbn3Ib+/Dah5GwOAG7M3dLfcnpyUhYnIB//PoPt7ETf5mIhMUJ2Hpuq9SXeiEVCYsTMPrn0W5jn016FgmLE7D+9Hqp73DuYSQsTsCQH4a4jX1p60tIWJyAn8+sQdt7eqPtPb2hUquv96UiIiKiSwRBcEuaRo0axaSJqBpwxklGGp0WI84fRqZYAEes/57jRERERFVHpVLBbrcDAHbs2IEuXbrIHBFRYGBVPR9gtpkrtfzO60v1zMXQhV/aVGo0wqJVVslSPVPRRezZORAA0L3bRmiCw+BwOqCwOXHw800AgLZP9YNKd+XfRERERFccOXIETZs2la7r1KmDjIwMGSMi8g+e5AaccfIBBnXp8xWuTkouUyvUUCtKL2kra6xKoZISmKtdnUSVGqsSKxyrVChhUJSO93pj7UIRdHpXNT3R6UrwoARKjEXYc9RVHOKOYisTJyIiojKo1WrY7XYMGTIEq1atAgAmTUQyYOIko5KSEqzfuwMA0K9jV2i1/pk46IPDEK59Q2pfJqhVqI10qU1ERERXpKWloUGDBtL1Dz/8IF8wRMTESU5ZeTn4hy0SALAnLwfxtevJHJF3qNU6tO/6cKl+TbAeD8wv3U9ERBToZs6ciVmzZrn1paSkyBQNEQFMnGQXbHPKHQIRERH5kMaNG+PkyZPStSAIcDr59wKR3Jg4ySi+dj0c99NZpqvZrMVIWudaqter/0yoNaX3ZBEREZHrPKb8/HzpOj4+HmfOnJEvICKS8Bwn8rpiUyFUId9BFfIdik2FUn9JgRH/G78S/xu/EiUFRhkjJCIi8g0ffvih1J4yZQqTJiIfwsSJvE5QKVGYVxuFebUhqJRX7nCKsKjDYVGHA86AqopPREQkefPNN2E2mwEAjz76KB577DEcPnwY7733nsyREdHVeI6TjHLyLuKJDXsBAAv6dkRURC1Z44HJBAQHu9pGIxAU5NWXc9jsOL/1DwBAvR53QMnKekREFGDq1auH9HRXhdkA+5OMyCfwHKcawlxiQXJsbakdaJRqFeL7tJM7DCIiIlkoFAq3ZGndunXo37+/jBER0fVwqZ6MgvVBGHr+KIaeP4pgvXdnd4iIiMg3pKamQhAEt6RpxowZTJqIfBxnnGQUGRaO/z48Uu4wvK6o4AK2bOsLALi7+waEhMUAABwlNhz86lcAQKtx90CpVcsWIxERUXUYN24cEhMT3fqys7MRFRUlT0BEVGlMnMjrRKcIQ1Ch1L7MbrZgx34dAKCZ2cLEiYiI/FpSUpJb0qRSqWCz2eQLiIg8wsRJRiUlJdh1yFUc4c7md0Cr1cockXfog8OgxxSpfZmgVCDKmSG1iYiI/FmvXr2kdsuWLXHw4EEZoyEiT7GqnozSMs+j0+FsAMCeZtGIl/sw3GquqkdEROTvkpKSMGvWLGzZsgUAYDabsWHDBgwdOlTewIgIgGe5AT/ml5nSKULJM4yIiIj8zvDhw9G7d29s3boVM2fOBAAYDAYmTUQ1FGec6AovzTjZrMXYttF1iF/3Pi9CrdFXyfMSERH5qoiICOTn50vXtWrVQk5OjnwBEVGZOONEPqXYVAjoEgFdoqt9SUmBEV+PX46vxy9HSYFRvgCJiIiqiNlshiAIbklTx44dmTQR+QEmTuR1gkqJooIoFBVEQVApr9zhFGFUR8GojgK4XJGIiGq4devWIeia1RoLFizAnj17ZIqIiKoSq+rJKCfvIp5ZvxMA8Gm/zoiKqCVzRN4REhKFIUN3l+pXBevRt59OahMREdVkAwYMcLs2mUwwGAwyRUNEVY0zTjIyl1iwpXZ9bKldH+YSi9zhVDulWoUmQ7qgyZAuUKqZwxMRUc02Zcqlozf0eoiiyKSJyM8wcZJRsD4IA9KPY0D6cQTrWfqbiIioJjGbzW5L89577z2YTCaYzWYZoyIib+HH/DKKDAvHlw89KHcYXldUlIOkTf0BAL16r0NISBQAwFFiw5GlWwEATUf1gFKrli1GIiIiTyxduhSjR48GACgUCjidTgDgLBORH2PiRF4n2h0IDs2V2pfZzRZs2ema9Gwy2MLEiYiIaoTu3btj+/bt0nWAnexCFLCYOMnIbrPh2Lk0AMCt9eOhUvtn4qAPCoXS9oTUvkxQKhBhz5LaREREvs5gMKC4uFi6DgkJQWFh4XUeQUT+ggfgyigt8zw6Hc4GAOxpFo342vVkjcdbB+ASERHVdNfuZwJcVfTWrFkjU0REVBU8yQ0440RERERUgcaNG7tdr127Fv3795cpGiKSAxMnGdWtFYMjHVyZbbBOJ3M03mOzWbBry3wAwJ13Pwm12n//rURE5J8yMjIgCAIAns9EFKi4sURGKrUa4SEhCA8J8dv9TQBQbCyAVfkJrMpPUGwskPqthSYsGr8Ui8YvhbXQJGOEREREpel0OnTv3l26FkWR5zMRBTAmTuR1gkoJkzEcJmM4BJVS6hcdThSoY1CgjoHocMoYIRER0RU5OTkQBAElJSXYvn07li5dKndIROQDuFRPRrkF+Zj08xYAwEf33Y3IsHBZ4/GWkJAoDB6cUqpfZdChV3dBahMREclt7ty5mDJliltffHy8TNEQkS9h4iQjY7EJv9ZtKLX9NXEqj1KrRrMxPeUOg4iICADQokULHDp0SLoWBEE62JaIiImTjIL1Qbgnfa+r3a6hvMEQEREFMLVaDbvdLl3HxsYiMzNTxoiIyNcwcZJRZFg4vn1oiNxheF1RUQ42/ToIAND7np8QEhIFAHDY7Djxw04AQOMhnaFU8+1IRETVLy0tzS1pmjBhAhYuXChjRETki/iXKnmdaHcgJPyC1L7MbizGr5tsAIAGfYqhjAiRJT4iIgps8fHxGDBgANauXYuUlBS0a9dO7pCIyAcxcZKR3WaD0WIB4DrHyV9LkuuDQuE0PSy1LxOUCoTYsqU2ERFRdWncuDFOnjwJURQBAGvWrJE5IiLydYJ4+SdGgCgsLERYWBgKCgoQGhpa8QO8KC3zPDoddiUOe5pFI752PVnjgckEBAe72kYjEBQkbzxEREReoFKp4HC4VkBoNBqUlJTIHBERycWT3IAf8xMREVFAOHLkCARBkJImAHj22WdljIiIapKbWqpXUlICrVZbVbEEnLq1YrC5oUVq+yubzYLU5P8DALTr8hDUap7ZRERE1evZZ5/Fp59+6tZ35swZntFERJXmUeK0bt06LF26FNu3b8fZs2fhdDoRFBSEtm3b4t5778W4ceNQt25db8Xqd1RqNZo1aix3GF5XbCyA0TH7Uvs+qCNciZO10ITvp/wIAHjg/fuhCeXSQCIiqnr16tVDenq6dK1SqWCz2WSMiIhqokot1Vu1ahVuu+02PP7441CpVJg6dSpWrlyJDRs24IsvvkCPHj2wceNG3HLLLXjyySeRnZ3t7bipBhEUAorNwSg2B0NQCFK/6HAiV1kbucraEB08YJCIiLwjIyNDajdp0oRJExHdkEoVh+jcuTNeffVV9O/fHwpF+bnW+fPn8cknnyA2NhYvvPBClQZaVXypOERuQT5e/HkjAOC9+/ogMixc1niquziEo8SGQ4uSAADNH+4FpdY/qwoSEZH8FAoF3njjDbz22mtyh0JEPsST3IBV9WTEqnpERERVb9y4cUhMTORBtkRUIa9W1du8efMNB0buDFod7s48h7szz0Gr1MBW4oDDfmXJmiiKsJU4YCtx4Or81mF3ej7W5r4UThrrdB9bljLHOlzPa7c53MdaXWOdHowlIiKqKjExMUhMTAQAfPHFF/IGQ0R+xeOqev369UP9+vUxbtw4PProo4iLi/NGXAEhKqIWlo6+DwCwdclR/Ln1IDoObIhOg24BAFiL7fhi8nYAwJPz7oZS6doftOvHk9j/axra3BOPrg80AQA4nSIWTNoKAJgw9y5oDa5lbynrTmPvmtNo2aMeeoy+XXrtL17YBqdTxKNvdUVwhKsy4sGt59CmjDgTp+2AtdiOh964E+GxBgDAoe3p2Lb0bzRuF41+T7SSxv7fazthKrBixCsdER0Xcul5/8bxCxMgCED/gT/BEBwBAFj6xm6EBdvRqrUO8fe0g1LN85iJiOjGKRQKtw8PW7ZsKWM0RORvPJ5xOn/+PJ555hmsWLECt9xyC/r27Yvly5fDarV6Iz7yA07YEBGTgfDoDNhtV94nggCcPWPHjm8PAFYePkhERDcmKSkJgiC4JU3vv/8+Dh48KGNURORvbmqPU2pqKr766issWbIEADBmzBiMHz8erVu3rrIAq5ov7XG6msPmhNMpQqEUoFS58llRFGG3upbPqTQKCIJrxslhd8Lp8HCsQoBSfSVPvrxETqVWSJXuHAVFUIZf+ppctcepzLEOJ5x2EYICUKmVV57X6gBEQKlWQHFprMlYiK2/vAEIQK9+b0Cnd+2jKikw4tTIUVCKNjT+4Qco9Poq+3oSEVFgGDt2LBYtWuTWl52djaioKJkiIqKapFqLQ6Snp2PBggWYM2cOVCoVLBYLOnfujPnz56NFixY389Re4UuJ06mzZ9Ht7xwAwG+3RaGR3MseWRyCiIhqmDlz5mD69OkAAK1WC4vFInNERFSTeLU4BADYbDasWLECAwYMQIMGDbBhwwZ8+umnyMrKwvHjx9GgQQMMHz78hoIPNA6FAMdVZxsRERHR9ZnNZpjNZgDAtGnT0LJlS3Ts2JFJExF5lce78Z999lksWbIEoihi7NixeOedd9w2XwYFBeG9995D3bp1qzRQf1QnKhqL0tKltr+y2Sw4tO9nAEDztvdBrdbJHBEREdVU69atw4ABAwBA2tPEvUxEVB08TpwOHTqETz75BMOGDYNWqy1zTFRUFMuWV4JOr0Ofrglyh+F1xcYC5BinXmrfBXWEK3FylpTg3HPPAQDqf/wxFOW8n4iIiABXZd8NGzZI16NHj5b2WRMReZvHidPMmTPRpUsXqFTuD7Xb7UhOTkb37t2hUqnQo0ePKguSajZBIaCkRC+1JQ4HTFu3SW0iIqLyhISEwGg0Std6vZ5JExFVK4+LQyiVSmRkZCAmJsat/+LFi4iJiYHDx/8A9qXiENkXc/HGql0AgJlD70R0rUhZ46nu4hCizYaCn1xL+MIG3QdBrfbq6xERUc1jNpsRdM3vox49emDLli3yBEREfsWT3MDjGSdRFKVS11e7ePFiqR9sdH1GswkrGrv2gk0xm+RPnKqZoFYjfNhQucMgIiIfVVbStGTJEowaNUqmiIgokFU6cRo2bBgAQBAEPPbYY277mxwOB/744w906dKl6iP0YwadDt3PF7naLevJHA0REZFvMRgMbtcmk6lUHxFRdal04hQWFgbANeMUEhIC/VWHlWo0Gtx5552YOHFi1Ufox2Kjo7H8Yf+tpneZ2ZiH9WsfAAD0G/A9DMERAADR4UDJ338DALS33QZBqSz3OYiIKHAkJSWhV69eAFzJUrt27XDkyBGZoyKiQFfpxOmrr74CADRs2BAvvvgil+XVdKIIXDoDQ2IyeeWl7DYrwqLOSG0phJISnBrqmsm8PTUFAj9FJCIKaFcvzfv000/x9NNPw2AwMGkiIp/g8QG4M2fOZNJU04ki0K2bqxDE1bfYWK+8nEYXBHNeX5jz+kKju+q9IwhQxcRAFRMDlLFvjoiIAsfChQvd/r545plnZIyGiKi0SlXVa9euHTZt2oSIiAi0bdu2zOIQl6WmplZpgFXNl6rqnTp7FvcczgEA/NosCo3i4qrnha+unleWrl2B7duZzBARUbVo3759qb8fuJ+JiKpDlVfVu//++6ViEPfff/91EyfyjFEt89cyK6t02XGDgUkTERFVC61WC6v1yjLuyMhIXLx4UcaIiIjK5vE5TjWdL804WYot+GlbMgBgUPcu0Ol11fPC1Xxek8Nuw/EjWwEATZr2gFLF85qIiAhQKBS4+s+QUaNG8VBbIqpWnuQGHu9xmjBhAg+dqyI6vQ7D+/bC8L69qi9pkoGpKBfnLvwD5y78A6aiXKnfWVKCc5Oex7lJz8NZUiJjhEREJIeHHnpIau/YsYNJExH5NI8Tp+zsbPTr1w9xcXF46aWXcODAAW/ERX7GblfDbr9mpsnhQNGGDSjasAFwOOQJjIiIqtVTTz0ltb/99ls888wzEEWRZ0ESkc+7oaV6eXl5+O6777B48WJs374dTZs2xUMPPYQxY8agYcOGXgiz6vjSUr3cggLM+m47AGDG8LsQeemsLK+r5qV65RFtNuQtXw4AiBgxAoKaS/iIiPyZWq2G3W6HQqGAgx+YEZEP8CQ3uOk9TufOncOSJUvw5Zdf4tixY7Db7TfzdF7nS4nTqbNn0fm4awPszia15KmqJ2PiREREgSEtLQ0NGjRw62PVPCLyBV7d43Q1m82G33//Hbt378bp06cR66VzgPyVVqPBnZlm3JlphlajkTscIiKiKjdz5sxSSdPhw4eZNBFRjVOpcuTX2rx5MxYvXozvv/8eTqcTw4YNw88//4xevXpVdXx+rW5sLH4Y7f/JptlUgHU/jwAA9L9vOQxBriWJotMJW1oaAEAdHw9BcVN5PBER+ZjGjRvj5MmT0jWX6BFRTeZx4lSvXj3k5uaiX79+WLBgAQYNGiSd8URUFrvVgvDo41IblxMniwUn+vUHANyemgKBnz4SEfmN0aNHuyVN8fHxOHPmjIwRERHdHI8/4n/99deRkZGBVatW4cEHH2TSRBXS6PQwXuwB48Ue0Oj0bvcpQkKgCAmRKTIiIvKWq0uLT5kyhUkTEdV4PABXRmfOpWPgwUwAwJpWtdGgft3qeWEWhyAiIi949tlnkZOTIyVNaZeWY8fHx8sZFhFRuTzJDSq1VG/YsGFITExEaGgohg0bdt2xK1eurHykAc4pOpCjU0htIiKimqpevXpIT08HAEycOBG9evViwkREfqVSiVNYWBgEQQAAhIaGSm26OdGRtfCf3IOX2rfJHI33OOw2nEvbDwCoH98GShXPayIi8icKhQJXL2CZPXs2C0YRkd/hUr1AVM1L9QrzsrB3n+tE+I5tkxEa4aok6LRakTljJgCg9qw3oGBJdiKiGiU1NRXt27d365s1axZee+01mSIiIvKMV89x6tWrF/Lz88t8UX66ROVxOgU4ndfMVNrtKPjhBxT88APg4wcnExGRu3HjxpVKmrKzs5k0EZHf8njGSaFQIDMzEzExMW79Fy5cQL169WCz2ao0wKrmSzNOuQUFmLN8CwBg2oi7ERkWVj0v7CPFIUSrFbnffgsAiBw7FgJnnIiIaoyrl+2rVCqf//1PRFSWKi8OAQB//PGH1D506BAyMzOla4fDgfXr16NevXo3EG7gKigsxDdNXKepP1VYWH2Jk48QNBrUGj9e7jCIiOgGdOzYEXv37kXLli1x8OBBucMhIvK6SidObdq0gSAIEAShzCV5er0en3zySZUG5++0Gg3aX7C42s0520JERL4rKSkJAwcORHFxMQBgz549MJvNMPDwciIKEJVOnE6dOgVRFHHLLbdgz549iI6Olu7TaDSIiYmBUqn0SpD+qm5sLNaMjJU7DK8zmwqw7ueHAQD971sEQ5BrZk10OmHPzgYAqKKjISg83nJHRETVYPjw4VixYgUA1well5MnJk1EFEgqnTg1aOBaUuZ0Or0WDPknu9WC8OhDUhuXEyeLBcd73A0AuD01BQJ/ARMR+ZyIiIgyi0IREQWaSn3Ev3r1amnT5+rVq69789S8efPQsGFD6HQ6JCQkYM+ePdcdn5+fj6effhp16tSBVqvFbbfdhrVr13r8ulR9NDo9CrMTUJidAI1O736nSuW6ERGRTzGbzRAEwS1pSkhIkGabiIgCTaWq6l1dSU9xneVUgiDA4XBU+sWXLVuGRx55BPPnz0dCQgI+/PBDfPfddzh69Gipqn0AYLVa0bVrV8TExODll19GvXr1cObMGYSHh6N169aVek1fqqp35lw6hu3PAACsbFMHDerXrZ4X9pGqekRE5JtWrVqFYcOGufUlJibi0UcflSkiIiLvqPKqelcvz6vKpXpz587FxIkTMW7cOADA/PnzsWbNGnz55ZeYNm1aqfFffvklcnNzkZycDLVaDQBo2LBhlcVT3ZyiA+eDlFKbiIjIFzz44INu1yaTifuZiCjgVclu/BtZ+2y1WpGSkoI+ffpcCUahQJ8+fbBz584yH7N69Wp07twZTz/9NGJjY9GyZUvMnj37urNcJSUlKCwsdLv5iujIWpiefgbT088gOrKW3OF4jcPhQM6FU8i5cMqjGUkiIpJHUVERAFchCFEUmTQREeEGEqe3334by5Ytk66HDx+OyMhI1KtXDwcOHKj08+Tk5MDhcCA21r2qXGxsrNsZUVc7efIkVqxYAYfDgbVr1+K1117D+++/j3//+9/lvs5bb72FsLAw6RYXF1fpGL0tOMiASQ/dj0kP3Y/gIP/9pWQqzMGBP/vgwJ99YCrMkfqdVisyZ81C5qxZcFqtMkZIRBTYLu9nurzaw2AwQBRFmM1mmSMjIvIdHidO8+fPl5KPX3/9FRs3bsT69evRv39/vPTSS1Ue4NWcTidiYmKwYMECtG/fHiNHjsQrr7yC+fPnl/uY6dOno6CgQLqdPXvWqzGSB+x25C1egrzFSwC7Xe5oiIgC0tKlSxF0aa/r22+/jZycnAoeQUQUmDwuZ5aZmSklTj///DNGjBiBe++9Fw0bNkRCQkKlnycqKgpKpRJZWVlu/VlZWahdu3aZj6lTpw7UarXbeVHNmjVDZmYmrFYrNJrSh8hqtVpotdpKx1WdCoqK8O6yjQCAl0b2QVhIiMwReUdQaBS6dnbNRqo1V6rqCSoVop5+WmoTEVH16tq1K5KTk936uCyPiKhsHs84RURESLM269evl/YoiaLo0f4VjUaD9u3bY9OmTVKf0+nEpk2b0Llz5zIf07VrVxw/ftytQMXff/+NOnXqlJk0+brc/Hx80bgRvmjcCLneOiNDFF1V9K69VSOlUgmdPhg6fbBb0itoNIh+9hlEP/sMhBr4/SMiqskMBoNb0hQSEsL9TERE1+Fx4jRs2DCMGTMG99xzDy5evIj+/fsDAPbt24cmTZp49FyTJ0/GwoUL8fXXX+Pw4cN46qmnYDKZpCp7jzzyCKZPny6Nf+qpp5Cbm4tJkybh77//xpo1azB79mw8fWnWoqZRqVRomWtFy1wrVN6YcRFFoFs3V+nxq2/X7CsjIqLAcXk/09XnMQ0aNMiniicREfkij/9a/+CDD9CwYUOcPXsW77zzDoIvnQeUkZGBf/7znx4918iRI5GdnY0ZM2YgMzMTbdq0wfr166WCEWlpaW7nRsXFxWHDhg144YUXcMcdd6BevXqYNGkSpk6d6uk/wyfE1amDjQ/U8d4LmM3ANUsw3HTtClTDJ4uW4kKsXe1KhgcM/go6vatGviiKcF6q3KQICYEgCF6PhYgo0H388cdu12vXrpU+BCUiovJV6gBcf+JLB+B63dUH3WZllT7o1mAAqiFZKczLwt59XQAAHdsmIzTClRg7zWYcbdceAHB7agoUXB5CRFQt4uPjcfbsWZ7PREQBr8oPwL3WsWPHsHnzZly4cKHUgbgzZsy4kackbwsKKp04VRONTo+C7DZSm4iIqpdWq4XNZpN+Z6elpckcERFRzePxjNPChQvx1FNPISoqCrVr13ZbXiUIAlJTU6s8yKrkSzNOZzMyMHLPeQDAsk71EFenipftXT3jZDTKljiVRxTFK2XIVSou1SMiqmI5OTmIjo6Wrhs0aIDTp0/LFxARkY/x6ozTv//9b/znP/+psfuKfIndbsfJUJXUDjSCIABqtdxhEBH5pTlz5rgVWAKAxYsXyxQNEVHN53HilJeXh+HDh3sjloATHVkLL6T97mp3uk3maLzH4XDAZnVVb1Jr9G4lyYmIqOq1aNEChw4dkq4FQSi1tJ6IiDzjceI0fPhw/PLLL3jyySe9EU9ACQ4yYOqjQ+UOw+tMhTllFocQrVZc+PAjAEDM85N4lhMRURVQq9VuqxhiY2ORmZkpY0RERP7B48SpSZMmeO2117Br1y60atUK6muWWj333HNVFhz5N9FuR+6XXwIAop95mokTEVEVuDppmjBhAhYuXChjNERE/sPj4hCNGjUq/8kEASdPnrzpoLzJl4pDFBQV4ZMVvwIAnn3wHoSFhFTtC/hIcQiHw4G8i64KThG14qWlepxxIiKqejk5OYiJicHvv/+Odu3ayR0OEZFP8yQ34DlOMjp19iw6H78IANjZpBYaxcVV7Qv4SOJERETe07hxY5w8eRJLlizBqFGj5A6HiKhG8SQ3UNzoi1itVhw9ejQgq8FVFZVKhVvzbbg13waV6oaO1CIiogCmVCqllR6jR4+WORoiIv/mceJkNpsxfvx4GAwGtGjRQjpE79lnn8WcOXOqPEB/FlenDrYP7YjtQztW/RlOPsRSXIhVy8Zg1bIxsBQXSv2iKEK02Vy3wJr4JCK6KUeOHClVKW/KlCkyRkRE5P88TpymT5+OAwcOYMuWLdDpdFJ/nz59sGzZsioNjvyD1VKM0OjdCI3eDaulWOoXi4txpNUdONLqDojFxdd5BiIiuuzZZ59Fs2bN3PrOnDmD9957T6aIiIgCg8frw3744QcsW7YMd955p+sA00tatGiBEydOVGlw5B9UGh3ys5tLbSIiujGX9zNdplKpYLPZZIyIiChweJw4ZWdnIyYmplS/yWRyS6SoYmczMvDIzrMAgG86x/ntcj1DUBgeGPlTqX5Br8dte3ZLbSIiur6mTZtKiVOTJk1w7NgxmSMiIgocHi/V69ChA9asWSNdX06WvvjiC3Tu3LnqIgsAdrsdhyM0OByhCcgiG4IgQBkaCmVoKJNuIqJymM1mqb1mzRrUrVsXs2bNYtJERFTNPJ5xmj17Nvr3749Dhw7Bbrfjo48+wqFDh5CcnIytW7d6I0a/FRkejidP/e5qd2gsczRERORrxo0bh8TERLcleefPn5c5KiKiwHRD5zidOHECc+bMwYEDB2A0GtGuXTtMnToVrVq18kaMVcqXznHyOh85x6kwLwu7U7oCABLa70BoRCwA1wG4OZ8vAABE/eMJHoBLRHSVmJgYZGdnS9ebNm1Cr169ZIyIiMj/eJIb3NDhQY0bN8bChQtvKDgKTApF6fxctNuRM28eAKDW+MeZOBERXaJQKNyOabjjjjuYNBERyazSiZPdbofD4YBWq5X6srKyMH/+fJhMJgwePBjdunXzSpD+ymgyY/7KXwAATw67F8FBBpkj8o6gkEg0abhUaktUKkSMGS21iYgCXVJSEnr37u3W9/7772Py5MkyRURERJdVeqneuHHjoNFo8PnnnwMAioqK0KJFC1gsFtSpUweHDh3Cjz/+iAEDBng14JvlS0v1Tp09i87HLwIAdjaphUZxcVX7Aj6yVI+IiCq2bt26Ur9Ds7OzERUVJVNERET+z5PcoNJV9Xbs2IEHHnhAuv7mm2/gcDhw7NgxHDhwAJMnT8a7775741EHIIWgRIMiOxoU2aEQlHKHQ0REMurfv7/U1mq1EEWRSRMRkQ+pdOJ0/vx53HrrrdL1pk2b8MADDyAsLAwA8Oijj+Kvv/6q+gj9WIP6dbF7cAfsHtwBDerXlTscr7EUF+LH7x7Hj989DktxodzhEBH5DLPZjKVLl0rXKSkp6N27NywWi4xRERFRWSqdOOl0OhQXF0vXu3btQkJCgtv9RqOxaqMjv2C1FCO41lYE19oKq+XKe8hpNuNwy1Y43LIVnFedU0JEFAjWrVuHoKAgjB49GqmpqQCAdu3aYePGjTJHRkREZal04tSmTRt8++23AIDt27cjKyvLrcLPiRMnULeu/86a0I1TaXTIz26C/OwmUGl07nfa7a4bEVEA6devn9t+pu7du8sYDRERVUalS5nNmDED/fv3x/Lly5GRkYHHHnsMderUke5ftWoVunbt6pUg/dXZjAxM+C0NAPBFt3jEXfX19CeGoDA8MHJDqX5Bp0OTrVukNhFRIAgJCXFboaHX67lig4ioBqh04tSjRw+kpKTgl19+Qe3atTF8+HC3+9u0aYNOnTpVeYD+zG6340CUVmoHGkGhgDo2Vu4wiIiqhdlsRtA11U179OiBLVu2yBMQERF5xKPDc5o1a4ZmzZqVed8TTzxRJQEFksjwcIw7keJqd2gsczRERORN1yZNK1euxNChQ2WKhoiIPFWpxGnXrl248847K/WEZrMZp06dQosWLW4qsEAQFhKCtyYMkTsMryvMy8LOvT0AAJ07bkVohGuWSbRakXtp31zk2LEQNBrZYiQi8ramTZviyJEjAACTyQSDwT8PPSci8leVKg4xduxY9O3bF9999x1MJlOZYw4dOoSXX34ZjRs3RkpKSpUGSTWfSmWDSmVz6xPtdlx49z1cePc9iAG4VJGI/N/gwYOl9uHDh/HWW29BFEUmTURENVClZpwOHTqE//73v3j11VcxZswY3Hbbbahbty50Oh3y8vJw5MgRGI1GDB06FL/88gtatWrl7bj9gtFkxqKfNwEAHr6vN4KD/PMXaVBIJOrHfC61JSoVwoYMkdpERP7i6v1MoaGhKCx0nWE3bdo0OcMiIqKbIIiiKHrygN9//x2//fYbzpw5g+LiYkRFRaFt27bo2bMnIiMjK34CmRUWFiIsLAwFBQUIDQ2VNZZTZ8+i8/GLAICdTWqhUVxc1b6AyQQEB7vaRiNwzfp6IiKqevPmzcMzzzzj1ufhr1oiIqomnuQGHn/M36FDB3To0OGGg6MrFIISsWan1CYiopqtbdu22L9/v1sfkyYiIv9wQ+uj7HY7tmzZghMnTmDMmDEICQlBeno6QkNDEXx5hoMq1KB+XRyo7/+HBluKjfh17b8AAPcMeAc6Pd8jROR/tFotrFardB0ZGYmLFy/KGBEREVWlShWHuNqZM2fQqlUr3H///Xj66aeRnZ0NAHj77bfx4osvVnmAVPNZLSYYIjbAELEBVsuV4iJOsxlHO3bC0Y6d4DSbZYyQiOjmREVFuSVNo0aNYtJERORnPE6cJk2ahA4dOiAvLw96vV7qHzp0KDZt2lSlwZF/UKk1KMhpgIKcBlCp3UuOO4uK4CwqkikyIqKqkZaWJrV37NiBJUuWyBgNERF5g8dL9bZv347k5GRorjlzp2HDhjh//nyVBRYI0rOy8OTmkwCA+T1vQd3YWJkj8g5DcASGjUgq1S/odGi8fp3UJiKqSVq1aoVx48Zh8uTJMBgMOHz4MJo2bSp3WERE5CUezzg5nU44HI5S/efOnUNISEiVBBUoSqxW7InVY0+sHiVXLfEIFIJCAU3DhtA0bAhB4fFbkYhINmq1Gn/++SemTJmCnJwcAGDSRETk5zz+a/Xee+/Fhx9+KF0LggCj0YiZM2diwIABVRmb3wsLDcVDx9Pw0PE0hMlcGp2IiCqWlpYGQRBgv+rQ7mXLlskYERERVRePz3E6d+4c+vbtC1EUcezYMXTo0AHHjh1DVFQUtm3bhpiYGG/FWiV86Rwnr/ORc5yKCi5ge3IvAMBdXZIQEuZ6j4g2G/KWLwcARIwYAUGtliU+IqLKmDZtGt5++223Pi7PIyKq2bx6jlP9+vVx4MABLFu2DAcOHIDRaMT48ePx0EMPuRWLILpMdIrQaoulttRvsyHrzX8DAMKHDmXiREQ+q1GjRjh9+rR0rVAoyly2TkRE/svjxGnbtm3o0qULHnroITz00ENSv91ux7Zt29C9e/cqDdCfWYot+O6XrQCA4ff2gE5/EwUSRBG4tqS3yVT22GqmDw5DVPDbUluiVCKkb1+pTUTki9LS0tySpgYNGrhdExFRYPB4qZ5SqURGRkapJXkXL15ETEyMz38C50tL9U6dPYvOx13nfOxsUguN4uJu7IlEEejWDUhOLn+MjEv1iIhquqioKFy8eBFTpkzBe++9J3c4RERURby6VE8URQiCUKr/4sWLCOIf5h4Ltzpv/knM5usnTV27AgbDzb8OEVGAePbZZ/G///0P5ksz+Zcr5xERUeCqdOI0bNgwAK4qeo899hi0Wq10n8PhwB9//IEuXbpUfYR+rFFcHI7c6CxTebKySs8sGQxAGcludbEUG7F5/esAgJ79XodOHyxbLEREFalXrx7S09MBuM4o5LI8IiICPEicwsJce1NEUURISIhbIQiNRoM777wTEydOrPoIyTNBQT63JM9qMUETtupS+yUpcXIWF+NE334AgMYb1kPB4iJEJDOFQoGrV7DreDg3ERFdUunE6auvvgLg+vTtxRdf5LI8qjSVWoOC3HpSWyKKsF+4ILWJiOSSmpqK9u3bu/XNmjULr732mkwRERGRr/G4OERN50vFIdKzsvDsxmMAgE/63Iq6sbE39kQ+cl6Tp0SHAyV//w0A0N52GwRW1iMiGYwbNw6JiYlufdnZ2YiKipInICIiqjZeLQ4BACtWrMDy5cuRlpYGq9Xqdl9qauqNPGVAKrFasaNusNQONIJSCV2zZnKHQUQB7uqkSa1Wl/q9RkREBAAKTx/w8ccfY9y4cYiNjcW+ffvQqVMn1KpVCydPnkT//v29EaPfCgsNxYgT5zHixHmE6oPgNJvhLClxG+M0m+E0myE6r1TfE222MsdeHi9eVRJetNtdYy0W93HFxZUfa7G4xtrtV8Y6HJ6PLS52H1tSAue1Z08REVWzM2fOAABatmzJpImIiMrlceL02WefYcGCBfjkk0+g0Wjwr3/9C7/++iuee+45FBQUeCNGvxUZFoaPJwzExxMGQljzE462a4+MV151G3O8dx8cbdce1hMnpL78VatwtF17nJ88pdRz/t21GyyHDknXhevW4Wi79jj3z3+6jTs1fDiOtmsP8+8pUp9xyxYcbdceaeMedxt75uGxONquPYy//Sb1mXbtwtF27XF61Gi3sWcnPoGj7dqjaONGqS9711asTeqMtZvuRFHBBan/3D+fxqnhI5C/chVEm+26XysioqqSlJQEQRCQlJQEAIiPj4coijh48KDMkRERkS/zOHFKS0uTyo7r9XoUFRUBAMaOHYslS5ZUbXTkF0SnCL3BCL3BCNF51ZY6lRLWEyeQv2IFoLqhVaNERB4ZPnw4evfuDQDSf4mIiCrD4+IQt9xyC77//nu0bdsWHTp0wMSJE/GPf/wDv/zyC0aNGoXc3FxvxVolfKk4RLHJjI2/bAYA9O59F3QqFaBUQnHVGVmXl7IJOh0EhSvPFW021wzN5bFXFYdwXrgAITJSKrQg2u0QrVZAoYDiqrK6zuJiQBQhaLUVj7VYAKcTgkYD4VKCIzocEEtKKjXWajEhNfkbAALa3/UY1GrXeGdJCeBwQNDryzxUmYioKkVERCA/P1+61mq1sFyz3JiIiAKLV4tD9OrVC6tXr0bbtm0xbtw4vPDCC1ixYgV+//136ZBcqpyM8+cwMdxVpjs5MxO33HZbqTEKg6FUn6BWQ1Cry3xOhcEAXFWdTlCppATGbVwZZyaVO7aMc0wEpRJCGbGVNVajC8KdvZ4qPfaqBJGIyFvMZnOpIzQSEhKwa9cumSIiIqKayOPEacGCBXBeKlTw9NNPo1atWkhOTsbgwYPxj3/8o8oD9Hc6e0BVgyciqlarVq0q9aFeYmIiHn30UZkiIiKimsqjpXp2ux2zZ8/G448/jvr163szLq/xpaV6VcbHz3GyWYuxZcMcAMDdfadBrSk920VE5A3du3fH9u3bpWuTyQRDGbPlREQUmDzJDTwqDqFSqfDOO+/AflWpaaKKFJsKoQhaBEXQIhSbCuUOh4gCyLZt26DX62EwGCCKIpMmIiK6YR5X1evduze2bt3qjVjITwkqJYryY1CUHwNBpaz4AUREN8hsNkMQBOiv2sdpNpthMplkjIqIiPyBx3uc+vfvj2nTpuHgwYNo3759qQ23gwcPrrLg/F3m+fN46ZfjAIB3722C2vXqyRyRd4SERGHIsJ1yh0FEfm7p0qUYPdp1tpzFYsHYsWPx7bffyhwVERH5C4/LkSsU5U9SCYIAh8Nx00F5ky/tcTr599/oct5Vbjy5nqHMqnqV4uN7nIiIvK1r165ITk526+N+JiIiqohXy5FfrqhHNy84NBwjfjnmajfrKHM0REQ1k8FgQHFxsXQdEhKCwkLupyQioqrlceJEVSemdgw+fmag3GF4XVFRDjZv7gsA6NlzA0JComSOiIj8QVnnMw0aNAirV6+WKSIiIvJnTJzI60S7A0HB+VKbiMgbNm3ahF69eskdBhER+SkmTjKylpQgdfdeAEC7hI7QaLUyR+Qd+uAwaBzPSm0ioqpgMBiwdu1aDB48GDabTe5wiIjIz3lcHKKmY3EIIqKaS6vVwmq14syZM4iPj5c7HCIiquG8dgAuERGRHHJyciAIAqxWKwCgQYMGMkdERESB5oYSpxMnTuDVV1/F6NGjceHCBQDAunXr8Ndff1VpcP7ulttuQ2bPNsjs2ebGZ5tqAJu1GFs2vI0tG96GzVpc8QOIiK4yZ84cREdHu/Xt2LFDpmiIiChQeZw4bd26Fa1atcLu3buxcuVKGI1GAMCBAwcwc+bMKg+Qar5iUyEc6gVwqBeg2MQSwURUec2aNcP06dOla0EQIIoiunTpImNUREQUiDxOnKZNm4Z///vf+PXXX6HRaKT+Xr16YdeuXVUaHPkHQaWEsTASxsJICCql3OEQUQ2h0+lw5MgR6bp27do8S5CIiGTjceJ08OBBDB06tFR/TEwMcnJyqiSoQJF5/jzGL9yE8Qs3IfP8ebnD8ZqQkCjcP2Qv7h+yl2c4EVGlxcbGSu0nn3wSGRkZMkZDRESBzuPEKTw8vMxfXvv27UO9evWqJKhAYTaZsKZJLaxpUgtmk0nucIiIZJeWlia1z5w5g9q1a+Pw4cP473//K2NUREREN5A4jRo1ClOnTkVmZiYEQYDT6cSOHTvw4osv4pFHHvFGjH4rODQcQ49mYejRLASHhssdDhGRrBo2bIgGDRogJiZG6svIyEDTpk1ljIqIiMjF43OcrFYrnn76aSQmJsLhcEClUsHhcGDMmDFITEyEUunbe1h86RynKuPj5zgVFeVg08aBAIDefdZwuR4RlaJUKt32L5lMJhgMBhkjIiKiQOBJbqDy9Mk1Gg0WLlyI1157DX/++SeMRiPatm2LW2+99YYDJv8m2h0ICcuR2kRElx05cgTNmjVz65s6dSqTJiIi8jkeJ06//fYbunXrhvj4eJ7afpOsJSVIO3UKDpsJDZo0h06vBwDYrCY4bCVQqLTQaK/MHllMuQAAjT4cCoXiylhTLnTXPLc0VhcKhVJ1aWwxHLZiKJRqaHQhNzbWnAeIItS6YCiVrqqKdnsJ7CUmQKGETh9WaqxGZwAsjwEA9EF+MstHRDftqaeewvz58936zpw5w98tRETkkzxOnHr16oV69eph9OjRePjhh9G8eXNvxBUQ0k6dQrcMCwAltuIQbm/VHgCwY8M/4Qj6DShsj95Dlkvjd+zuCADo0HoTwmo1BADs/HUKHNiAntc89/bfEqBQO9Hqtu8RU78NAGBP0iuwaH6EI78J7h22QRq7dWtnqHR23B7/P9RvcjcAIHX7bBiFxbDlx6PfsM3S2C2bukIdVIJbYj9EoxaDAAAHdsxFvuML2PJj0W9Y8pWxv/aAOsSE+hFvoveA16rgK0ZE/mLo0KH44YcfpGuVSgWbzSZfQERERBXwuDhEeno6pkyZgq1bt6Jly5Zo06YN3n33XZw7d84b8fk1h+1KJT2rtUjGSIiIqtfChQul9u23386kiYiIfJ7HxSGudurUKSxevBhLlizBkSNH0L17dyQlJVVlfFXOl4pDWIqLceb4IVitRbi1ecKNL9XLz4Uu9tLSlkvFIXxpqZ5SY8C5c64S9g0aNJBiJ6LAkpqaijNnzkhnAc6dOxcmkwmvvcYZaSIikocnucFNJU4A4HA4sG7dOrz22mv4448/4HD49uZ/X0qcqoyPV9WzWq2YPXs2AODll1+GRqOROSIiqm5jx47FokWLALBiHhER+Q5PcoMb/uh/x44d+Oc//4k6depgzJgxaNmyJdasWXOjT0d+Ljo6GtHR0XKHQUQyiIqKkpImwLVXloiIqKbxuDjE9OnTsXTpUqSnp+Oee+7BRx99hPvvv5+fHt6A9HNn8MZOV7I5s/NA1K3fQOaIvEOj0eDpp5+WOwwikoEgCG7Xd9xxB3bt2iVTNERERDfO4xmnbdu24aWXXsL58+fx888/Y/To0UyablBRXg5+jOqCH6O6oCgvR+5wiIiqTFJSUqmk6f3338eBAwdkioiIiOjmeDzjtGPHDm/EEZAMIaHod8xV6tvQ6NqC4kRENVfv3r3drrOzsxEVFSVTNERERDevUonT6tWr0b9/f6jVaqxevfq6YwcPHlwlgQWCuIa3IrHhrXKH4XU2mw1LliwBAIwePRpqtVrmiIjI20JCQlBUVAStVguLxSJ3OERERDetUonTkCFDkJmZiZiYGAwZMqTccYIg+HxVPap+oiji5MmTUpuI/I/ZbMY999wjrUooLCzE119/jUcffVTmyIiIiKpGpRInp9NZZpuoMpRKJYYNGya1ici/rFq1Svp/vEWLFvjrr78AgEkTERH5FY+LQ3zzzTcoKSkp1W+1WvHNN99USVCB4ujBFNTevB+1N+/H0YMpcofjNUqlEnfccQfuuOMOJk5EfqZfv35S0gQAhw8fljEaIiIi7/E4cRo3bhwKCgpK9RcVFWHcuHFVEhQREfm+kJAQbNiwQbrW6/VclUBERH7L48RJFMVSJWYB4Ny5cwgLC6uSoAJFgybNscR2Gktsp9GgSXO5w/Eap9OJ8+fP4/z58/yjisgPmM1mCIIAo9Eo9fXo0QNms1nGqIiIiLyr0uXI27ZtC0EQIAgCevfuDZXqykMdDgdOnTqFfv36eSVIf6XT69Hz3iFyh+F1drsdCxcuBAC8/PLL0Gg0MkdERDcjODjY7XrlypUYOnSoTNEQERFVj0onTper6e3fvx99+/Z1+8Wp0WjQsGFDPPDAA1UeIPkHzkYS+Y/ff/8d7du3BwCYTCYegk5ERAFBED2sD/31119j5MiR0Ol03orJqwoLCxEWFoaCggKEhobKGkv6uTOYvd11LtbLdw1G3foNbuyJTCbgciJrNAJBQVUUIRGRS2RkJDZu3Ih27doBAHJycnigLRER1Xie5AYeJ041nS8lTkcPpqBHjqvK3NYoB25v1f7GnoiJExF5idlsRtBVP1MC7FcGERH5OU9yg0oVh4iMjEROTg4AICIiApGRkeXeqPIMIaHoWZiKnoWpMITIm8QREV1r3rx5bkkTAOl3ARERUaCp1B6nDz74ACEhIVK7rKp65Lm4hrdiScNb5Q7D62w2G1asWAEAePDBB6FWq2WOiIgq0rZtW+zfv9+tj7NNREQUyCqVOF19+vtjjz3mrVioMkQRuLbkr8kkTyyVJIoijh49KrWJyLdptVpYrVbpOjIyEhcvXpQxIiIiIvl5fI5TamoqDh48KF3/+OOPGDJkCF5++WW3X7TkBaIIdOvm2s909S02Vu7IrkupVGLQoEEYNGgQlEql3OEQ0XXMnDnT7Wf5ww8/zKSJiIgIN1AcomPHjpg2bRoeeOABnDx5Es2bN8ewYcOwd+9eDBw4EB9++KGXQq0avlYc4t5sGwDgl2h1xcUhri4CUZauXYHt2wEupSSim6BWq2G327Fjxw506dJF7nCIiIi8xpPcoNLnOF32999/o02bNgCA7777Dj169MDixYuxY8cOjBo1yucTJ19TIlwu6+7w7IFZWaWr5xkMTJqIyGPNmjVDVlYWcnNzAbj2JRIREZE7jxMnURThdDoBABs3bsR9990HAIiLi2O1JQ/Vb3grPv3rO1e7w3DPHhwUVGPKjjudTum9ERUVBYXC4xWiROQll2eXAGDgwIFYs2aNzBERERH5Jo8Tpw4dOuDf//43+vTpg61bt+K///0vAODUqVOI9fG9Nr4mKCQUD44aL3cYXme32/HZZ58BAF5++WVoNBqZIyKitLQ0NGjgfuh2fHy8TNEQERH5Po8/+v/www+RmpqKZ555Bq+88gqaNGkCAFixYgXXwlO5DAYDDAaD3GEQEYBp06aVSpoOHz4sfRBGREREpXlcHKI8FosFSqXS58/o8aXiEBcy0zFn43IAwLQ+IxBTu+71H3B1cQijscYs1SMi39GoUSOcPn1aulYoFHA4PNxjSURE5Cc8yQ1ueLNJSkoKFi1ahEWLFiE1NRU6ne6Gk6Z58+ahYcOG0Ol0SEhIwJ49eyr1uKVLl0IQBAwZMuSGXlduedkZWFzvbiyudzfysjPkDoeIAsDVSVODBg2YNBEREVWSx4nThQsX0LNnT3Ts2BHPPfccnnvuOXTo0AG9e/dGdna2xwEsW7YMkydPxsyZM5GamorWrVujb9++uHDhwnUfd/r0abz44ou46667PH5NX6HRB6Gr6Q90Nf0BjZ6zR0TkfUuWLAEATJ061S2JIiIiouvzeKneyJEjcfLkSXzzzTdo1qwZAODQoUN49NFH0aRJE+mXcmUlJCSgY8eO+PTTTwG4KrDFxcXh2WefxbRp08p8jMPhQPfu3fH4449j+/btyM/Pxw8//FCp1/OlpXoeq6FL9Ww2G1avXg0AGDx4sM8v5yTyJ0899RTmz5+P7OxsREVFyR0OERGRT/HqOU7r16/Hxo0bpaQJAJo3b4558+bh3nvv9ei5rFYrUlJSMH36dKlPoVCgT58+2LlzZ7mPmzVrFmJiYjB+/Hhs3779uq9RUlKCkpIS6bqwsNCjGOnmiaKIgwcPAgAGDRokczREgaNu3brIyHAtA46OjkYVbWklIiIKSB4nTk6ns8wZA7VaLZ3vVFk5OTlwOBylypjHxsbiyJEjZT7mt99+w//+9z/s37+/Uq/x1ltv4Y033vAoLqpaSqUSffv2ldpE5H0KhcItUbr99ttljIaIiKjm83iPU69evTBp0iSkp6dLfefPn8cLL7yA3r17V2lw1yoqKsLYsWOxcOHCSi85mT59OgoKCqTb2bNnvRqjJ44eTMGtSdtwa9I2HD2YInc4XqNUKtG5c2d07tyZiRORl6WmpkIQBLekadasWeV+GEVERESV4/GM06efforBgwejYcOGiIuLAwCcPXsWLVu2xKJFizx6rqioKCiVSmRlZbn1Z2VloXbt2qXGnzhxAqdPn3Zb7nV5lkulUuHo0aNo3Lix22O0Wi20Wq1HcVWnIuHyWkpWtiKimzN27NhSP4e5t4mIiKhqeJw4xcXFITU1FRs3bpQ+wWzWrBn69Onj8YtrNBq0b98emzZtkkqKO51ObNq0Cc8880yp8U2bNpX2ylz26quvoqioCB999JGUyNUU9Rveijl7v3K1O4yTORrvcTqdKCgoAACEhYVBobjhKvhEdB0bNmyQ2mq1GlarVcZoiIiI/IvHiRMACIKAe+65B/fcc89NBzB58mQ8+uij6NChAzp16oQPP/wQJpMJ48a5EolHHnkE9erVw1tvvQWdToeWLVu6PT48PBwASvXXBEEhoXjs8Ulyh+F1drsdH330EQDg5ZdfhkajkTkioqrncDhgs9lkjSEtLQ0tW7ZE48aN8eOPP8JiscgaDxERkS/QaDRV8sH9DSVOmzZtwgcffIDDhw8DcM04Pf/88zc06zRy5EhkZ2djxowZyMzMRJs2bbB+/XqpYERaWhpnKPwAS5CTvxJFEZmZmcjPz6/217ZYLMjKyoJSqUT9+vUBAD/99BMA4NSpU9UeDxERkS9SKBRo1KjRTX947/E5Tp999hkmTZqEBx98EJ07dwYA7Nq1CytWrMAHH3yAp59++qYC8jZfOsfpQmY65q5fDACY3G8MYmrXvf4Daug5TkT+LCMjA/n5+YiJiYHBYIAgCNXyumfOnEFRUZF03ahRIwTxZwIREZEbp9OJ9PR0qNVqxMfHl/o97Ulu4HHiVL9+fUybNq3UHqR58+Zh9uzZOH/+vCdPV+18KXE6ejAFPXJcVea2Rjlwe6v2138AEycin+JwOPD3338jJiYGtWrVqrbX3bdvHxyOKwVlBEFA+/YV/PwgIiIKUAUFBUhPT0eTJk1KrYLyJDfweA1cfn4++vXrV6r/3nvvlQoAUOVo9EHoUHwIHYoPQaNnEkRU01ze02QwGKrl9RwOB37//Xe3pCkoKIhJExER0XVcXqJ39e/PG+HxHqfBgwdj1apVeOmll9z6f/zxR9x33303FUygadSkKX5u0lTuMLzObrdj7dq1AIABAwZApbqhrXVEPqs6lucVFhbi77//dutr2LAhS40TERFVoKp+T3v8F2zz5s3xn//8B1u2bHHb47Rjxw5MmTIFH3/8sTT2ueeeq5IgqWZzOp1ITU0FgDJnK4moYteeR9e2bVseKE1ERFSNPE6c/ve//yEiIgKHDh3CoUOHpP7w8HD873//k64FQWDiRABclUx69eoltYmo8hwOB5RKJbRaLWrXro0LFy6gXbt2cod1Q6xWK5o3b45vvvkGXbp0kTscv5KYmIjExERs2bJF7lB8XsOGDZGYmIi7775b7lB83mOPPYaGDRvi9ddflzuUGuH111/H6dOnkZiYKHcofmXatGkwmUz45JNP5A7F8z1Op06dqtTt5MmT3ojXrxw7dAAtN21Ey00bcezQAbnD8RqVSoXu3buje/fuXKZHVEmX9zPt27dP6qtfv36VJ02ZmZl49tlnccstt0Cr1SIuLg6DBg3Chg0bEBUVhTlz5pT5uDfffBOxsbGw2WzIyMjAmDFjcNttt0GhUOD5558v8zHz589Ho0aNykya/vGPf0CpVOK7774rdd9jjz0mHZJ+tS1btkAQBLdS8FarFe+88w5at24Ng8GAqKgodO3aFV999dV1z9n6z3/+gy5dusBgMEjnA1ZEFEXMmDEDderUgV6vR58+fXDs2DG3Mbm5uXjooYcQGhqK8PBwjB8/Hkaj0W3MH3/8gbvuugs6nQ5xcXF45513Sr3Wd999h6ZNm0Kn06FVq1bS8ufynD59GoIglLo9/PDD0pjnnnsO7du3h1arRZs2bSr1bwZcxaAaNmwInU6HhIQE7Nmz57rjbTYbZs2ahcaNG0On06F169ZYv359qXHnz5/Hww8/jFq1akGv16NVq1b4/fffpfur6ut9rccee0z6+mg0GjRp0gSzZs2C3W4HcOV9dvkWHR2NAQMG4ODBgxV+rbZs2YJ27dpBq9WiSZMmlfqDevny5WjTpg0MBgMaNGiAd9991+3+lStX4p577kF0dDRCQ0PRuXNnt8OvAeC///0v7rjjDoSGhkpj1q1b5zZmwYIFuPvuuxEaGlrq/6PyJCYmSl8HhUKBOnXqYOTIkUhLS3MbV1hYiFdeeUV6z9auXRt9+vTBypUrcb2aZDfynrRYLHj66adRq1YtBAcH44EHHkBWVpbbmLS0NAwcOBAGgwExMTF46aWXpO/vZZX5Xnn63r9a06ZNodVqkZmZWeb9mzdvxoABA1CrVi0YDAY0b94cU6ZMcSu6JooiFixYgISEBAQHByM8PBwdOnTAhx9+CLPZfFNfo2tlZWXhscceQ926dWEwGNCvX79S/7/dfffdpX7GPPnkk6WeKzExEXfccQd0Oh1iYmLcKnCX97Nq165d0pgXX3wRX3/9tU/kFvz4X0ZOhx05iijkKKLgdNgrfgARBYScnBy3hOnyUteqdvr0abRv3x5JSUl49913cfDgQaxfvx49e/bEpEmT8PDDD+Orr74q9ThRFJGYmIhHHnkEarUaJSUliI6OxquvvorWrVuX+VqiKOLTTz/F+PHjS91nNpuxdOlS/Otf/8KXX355w/8eq9WKvn37Ys6cOXjiiSeQnJyMPXv24Omnn8Ynn3yCv/7667qPHT58OJ566qlKv94777yDjz/+GPPnz8fu3bsRFBSEvn37uh08/NBDD+Gvv/7Cr7/+ip9//hnbtm3DE088Id1fWFiIe++9Fw0aNEBKSgreffddvP7661iwYIE0Jjk5GaNHj8b48eOxb98+DBkyBEOGDMGff/5ZYYwbN25ERkaGdJs3b57b/Y8//jhGjhxZ6X/zsmXLMHnyZMycOROpqalo3bo1+vbtiwsXLpT7mFdffRWff/45PvnkExw6dAhPPvkkhg4d6vYez8vLQ9euXaFWq7Fu3TocOnQI77//PiIiIqQxVfH1Lk+/fv2QkZGBY8eOYcqUKXj99ddLJSxHjx5FRkYGNmzYgJKSEgwcOBBWq7Xc5zx16hQGDhyInj17Yv/+/Xj++ecxYcKEUknO1datW4eHHnoITz75JP7880989tln+OCDD/Dpp59KY7Zt24Z77rkHa9euRUpKCnr27IlBgwaV+pBlzpw5SElJwe+//45evXrh/vvvd/t/wGw2o1+/fnj55Zcr/PpcLTQ0FBkZGTh//jy+//57HD16FMOHD5fuz8/PR5cuXfDNN99g+vTpSE1NxbZt2zBy5Ej861//qrCQmKfvyRdeeAE//fQTvvvuO2zduhXp6ekYNmyYdL/D4ZC+V8nJyfj666+RmJiIGTNmSGMq8726kff+Zb/99huKi4vx4IMP4uuvvy51/+eff44+ffqgdu3a+P7773Ho0CHMnz8fBQUFeP/996VxY8eOxfPPP4/7778fmzdvxv79+/Haa6/hxx9/xC+//HLDX6NriaKIIUOG4OTJk/jxxx+xb98+NGjQAH369IHJZHIbO3HiRLefMdd+8DN37ly88sormDZtGv766y9s3LgRffv2LfWa1/6surroUVRUFPr27Yv//ve/5cZcbcQAU1BQIAIQCwoK5A5FzMu9KH46/x3x0/nviHm5Fyt+gNEoioDrZjR6P8Aq4nQ6RaPRKBqNRtHpdModDlGVKS4uFg8dOiQW/z97Zx5XY/r//9dZOqfTXtqLFtUJk60hxdAQMYaYMZJlsgzDp5lhjHWMKeaLxjLGWAcpS0RGGOuE7Ik2SasUQ8XQQou28/790e/cujunOhmEuZ+Px/2o+7re13W/r/Vc131f1/sqL1fwK62ootKKKladr6iqodKKKnpWVa1UtqZGRqmpqXTt2jWKjrlKF6Kv0qUrV6m6ulqprJzK6poX0n/QoEFkYWFBJUr6k8LCQkpKSiIAdOHCBZZfVFQUAaDU1FSFcH369KHp06cruF+7do34fD49efJEwS8kJIR69OhBRUVFpKGhQXfv3mX5+/r6kpeXl0I4uR6FhYVERPTzzz8Tn8+n+Ph4BdnKykql6axPcHAw6erqNiknk8nI1NSUVqxYwbgVFRWRWCymPXv2EBFRSkoKAaBr164xMsePHycej0f3798nIqINGzaQvr4+VVRUMDJz584lqVTK3I8cOZIGDx7Mer6Liwt9+eWXLL379OnD3GdnZxMASkhIaDIt/v7+1KlTpybliIi6d+9Ofn5+zH1NTQ2Zm5vTsmXLGgxjZmZG69atY7l98sknNGbMGOZ+7ty51KtXrwbjeFn5TURkZWVFUVFRzL2y+tW/f3/q0aMHESnWMyKiw4cPEwC6fv16gzrPmTOHOnTowHLz9vYmT0/PBsP4+PjQiBEjWG6//fYbWVpaNvr72b59e1q0aFGD/kRE+vr6tHXrVgV3ZemT4+vrS/7+/sy9svbx22+/scZV06ZNI01NTVaey3n69ClVVVU1qieR6nWyqKiI1NTUKDw8nHFLTU0lABQdHU1ERMeOHSM+n0/5+fmMzMaNG0lHR4dpd6qUlSp139/fn3x9fRX0HD9+PM2bN4+OHz9ODg4OLL+///6bRCIRzZgxQ2ka5eWyd+9eAkAHDx5UkJHJZFRUVKQ0vCp5VJ/09HQCQMnJyaz0GhkZ0ZYtWxi3hvp7OQUFBSSRSOjUqVMNyqjaV23fvp0sLS0blWmMxn6vmzM34L44tSB6+gbw+3I2/L6cDT19g5ZW55VRVVWFFStWYMWKFY0uleHgeJdo/+NJtP/xJApKn7+R3nw+C+1/PAn/Q+wvH84/nUL7H0/i5MWrzLKiE1llGHvwIfZmC1lGIHr9HIX2P57ErX+eLz/aH3ev2foVFBTgxIkT8PPzU3pwrp6eHpycnNCtWzeFr0DBwcFwc3ODo6PqVkEvXLgABwcHaGtrK/gFBQVh7Nix0NXVxaBBg154f0BoaCg8PDzQpUsXBT81NTUmnfLlRv+G7Oxs5Ofnw8PDg3HT1dWFi4sLoqOjAQDR0dHMUho5Hh4e4PP5iImJYWR69+7NOs3e09MT6enpKCwsZGTqPkcuI3/Oq4TH4zHlUVlZibi4OJYufD4fHh4ejepSUVEBdXV1lptEIsHFixeZ+8OHD+P999/HZ599BmNjY3Tp0gVbtmxh/F9WfquKRCJp8GtScXExwsLCAIBVbu7u7hg/fjxz/yLl1lBe3bt3D3fu3FEaRiaT4enTpzAwUD6OqKmpQVhYGEpLSxmjXi+Lhw8fIiIiAgKBAAKBADKZDGFhYRgzZgzMzc0V5LW0tJgl+wEBAbC2tv5Xz4+Li0NVVRUrnx0dHdGmTRtWvXBycoKJiQkj4+npiSdPnjBf4Joqqxet+wDw9OlThIeHY+zYsejfvz+Ki4tx4cIFxj88PByVlZWYM2eO0vDypcOhoaGQSqXw8vJSkOHxeNDV1QXwfGlpTk6OynlUn4qKCgBg1UU+nw+xWMxqt3K9DA0N8d5772H+/PmsJYORkZGQyWS4f/8+2rVrB0tLS4wcORJ///23wjOHDh0KY2Nj9OrVC4cPH1bw7969O+7du8ekq6XgJk4cHBwcbwh1l/5LJJJX+qxbt26BiJqc/EyaNAnh4eHMhO7p06fYv38/Jk6c2Kzn3blzR+lAKjMzE1euXGGW5siXB1LzzmZn4lJlMqerqwupVNrs+Osi36dQdzAmv5f75efnw9jYmOUvFAphYGDAklEWR91nNCTT0F6Juri5uUFLS4u56i7nUgWpVMoMyB49eoSamppm6+Lp6YlffvkFmZmZkMlkiIyMxIEDB5CXl8fI3L59Gxs3boS9vT1OnjyJadOm4ZtvvmGWNb2s/G4KIsKpU6dw8uRJxqiRHEtLS2Zfye7duzF06FBWfWvTpg3MzMyY+4bK7cmTJygvL28wrw4cOIDTp09DJpMhIyODWapVN7/qsnLlSpSUlGDkyJEs9xs3bkBLSwtisRhTp05FREQE2rdvr1I+NEZxcTG0tLSgqakJExMTREVFMS9gHj16hMLCQpXaoaGhIdq2bfuvdMnPz4dIJFLYl1i/XrxoG5OX1YvWfQAICwuDvb09OnToAIFAgFGjRrGMqWVmZkJHR4dVd5SRmZmpUr+loaEBqVTKHPKqSh7VRz6xmj9/PgoLC1FZWYmff/4Z9+7dY9XD0aNHY9euXYiKisL8+fOxc+dO1j7K27dvQyaTYenSpfj111+xf/9+FBQUoH///syLCS0tLaxatQrh4eE4evQoevXqhWHDhilMnuS/Hw29QHhdcDv1W5DH/+RjzaEdAIDpXp+jlZFpC2v0ahCJRJxFHo7/HCmLa9dwS9Sefy2a0rstJvaygYDP/toRt9ADCQkJUPv/r7IcHBzQqYsWvv5IBn69LyMX534IAFAXPo93hLNls/VTdWLi4+ODb7/9Fvv27cPEiROxd+9e8Pn8Zu1BAIDy8nKFN+kAsG3bNnh6ejLnUX300UeYNGkSzpw5g379+jXrGaqmafjw4Rg+fHiz4n5b2bt3L9q1a8fct27dulnh09LS/rUOa9asweTJk+Ho6Agej4e2bdtiwoQJrC+ZMpkM77//PpYuXQqg1tx+cnIyNm3aBF9f33+tQ1McOXIEWlpaqKqqgkwmw+jRoxV+ty5cuAANDQ1cuXIFS5cuxaZNm1j+O3bs+Nd6TJ48GVlZWfj4449RVVUFHR0dTJ8+HQEBAUqt0u7evRuLFi3CoUOHFCaNUqkUiYmJKC4uxv79++Hr64tz587968mTtrY24uPjUVVVhePHjyM0NBRLliwBoHobBICvvvoKX3311b/S5W1g27ZtrMnE2LFj0adPH6xduxba2togIpW+gKuat927d//X7VZNTQ0HDhzApEmTYGBgAIFAAA8PDwwaNIilR939g05OTjAzM0O/fv2QlZWFtm3bQiaToaqqCr/99hsGDBgAANizZw9MTU0RFRXF9P0zZ85k4unWrRtyc3OxYsUKDB06lHGXv0xszAjG6+CFvjhduHABY8eOhaurK2PtY+fOnQqf7zga51H+fWxuOwCb2w7Ao/z7TQfg4OB4a9AQCaEhErJ+EEVCPjREQoj//6RHvslWQyRET5duUBeL8f7770NHRwdqglpZdTWB0nj5dSZfaoLmd+X29vbg8XhN/sDq6OhgxIgRjJGI4OBgjBw5ElpaWs16nqGhIbP0TE5NTQ22b9+Oo0ePQigUQigUQkNDAwUFBaxBtY6OjtIN5UVFRRAIBMwSPAcHh5cy0FcFU9PaF131LVM9ePCA8ZObj69LdXU1CgoKWDLK4qj7jIZk5P6N0bp1a9jZ2TFX/fPAmoOhoSEEAkGzdTEyMsLBgwdRWlqKO3fuIC0tDVpaWrC1tWVkzMzMFAb07dq1Y6y1vaz8bgi5UYDMzEyUl5dj+/btCktYbWxsIJVK4evriy+++KLJlwcNlZuOjk6DX5R5PB5+/vlnlJSU4M6dO8jPz0f37t0BgJVfQO2XjC+++AL79u1TWGYGgLEQ6OzsjGXLlqFTp05Ys2ZNozqrAp/Ph52dHdq1a4eZM2eiR48ejFEVIyMj6OnpvdZ2WFlZqWARsH69eNE2Ji+rF637KSkpuHLlCubMmcP0cT169GAM4gC1/VZxcXGDXxTlvGj/pkoeKcPZ2RmJiYkoKipCXl4eTpw4gcePHyvUw7q4uLgAqF3RAID5ila3bRsZGcHQ0FDBEmP9eORxyCkoKGDCtyTN/rX9448/4OnpCYlEgoSEBGYdZHFxMfOmiEM1BCIxnCoy4FSRAYHoxX/MODg43j7i4uKQmpqK1NRUxq1jx46v7fkGBgbw9PTE+vXrFawkAWD9yE6aNAkXL17EkSNHcPnyZaWW8ZqiS5cuSEtLY72tPHbsGJ4+fYqEhAQkJiYy1549e3DgwAFGB6lUips3bzK/N3Li4+NhY2PDLEkZPXo0Tp06pXQ5WlVVldJ0vig2NjYwNTXF6dOnGbcnT54gJiaG2Ufi6uqKoqIixMXFMTJnzpyBTCZjBhiurq44f/48a/9nZGQkpFIpY1HO1dWV9Ry5zMver9IUIpEIzs7OLF1kMhlOnz6tki7q6uqwsLBAdXU1/vjjD9ZejZ49eyI9PZ0ln5GRASsrKwAvL78bQlNTE3Z2dmjTpo1Kx2b4+fkhOTkZERERDcr8m3ITCASwsLCASCTCnj174Orqyhow7tmzBxMmTMCePXswePDgJuMDasuqfht6GcybNw979+5FfHw8+Hw+Ro0ahdDQUOTm5irIlpSUKJgB/zc4OztDTU2Nlc/p6em4e/cuq17cuHGDNamOjIyEjo4OM6BvqqxetO4HBQWhd+/euH79OquPmzlzJrNcb8SIERCJREqPIQCe98WjR49GRkYGDh06pCBDRA1aK1QljxpDV1cXRkZGyMzMRGxsrNI9VnISExMBPJ8w9ezZk3menIKCAjx69Ihp2w3FU3/pYnJyMtTU1NChQ4cmdX6lNNcqRefOnWn79u1ERKSlpUVZWVlERBQfH08mJibNje618yZZ1Ws2b6lVvaqqKjp27BgdO3ZMJWs6HBxvC41Z6WmIyspKunbtGutqKbKyssjU1JTat29P+/fvp4yMDEpJSaE1a9aQo6MjIyeTycjOzo709fVZ7nVJSEighIQEcnZ2ptGjR1NCQgLdvHmT8X/06BGpqanRjRs3GDcvLy/y9vZWiKumpoZMTU0ZS2yFhYVkbGxMI0eOpNjYWMrMzKSgoCDS1tamjRs3MuGePXtGH3zwAenr69O6desoMTGRsrKyaO/evdS1a1fGatOBAwdYVuuIiO7cuUMJCQm0aNEi0tLSYtLz9OlTRkYqldKBAweY+8DAQNLT06NDhw5RUlISeXl5kY2NDas+DBw4kLp06UIxMTF08eJFsre3Jx8fH8a/qKiITExMaNy4cZScnExhYWGkoaFBv//+OyNz6dIlEgqFtHLlSkpNTSV/f3+FvHwRq3qZmZmUkJBAX375JTk4ODBprmvhr36aw8LCSCwWU0hICKWkpNCUKVNIT0+PZbFs3LhxNG/ePOb+ypUr9Mcff1BWVhadP3+e+vbtSzY2NiwrblevXiWhUEhLliyhzMxMCg0NJQ0NDdq1a9dLzW8i1azq1aUhq3Nz5swhJycnxtpd/XTfvn2bNDQ0aPbs2ZSamkrr168ngUBAJ06cYGTWrl1Lffv2Ze7/+ecf2rhxI6WmplJCQgJ98803pK6uTjExMYxMaGgoCYVCWr9+PeXl5TFXXatq8+bNo3PnzlF2djYlJSXRvHnziMfj0V9//cXI5OXlUUJCAm3ZsoUA0Pnz5ykhIYEeP35u5VcVq3pEbMuPjx8/JkdHR7K0tKTt27fTzZs3KSMjg4KCgsjOzo7Jx/ppJ2q6Tt67d4+kUikrP6ZOnUpt2rShM2fOUGxsLLm6upKrqyvjX11dTe+99x4NGDCAEhMT6cSJE2RkZETz589vVlmpUvfrWtWrrKwkIyMjVh8lR24BUm61bv369cTj8WjixIl09uxZysnJoYsXL9KUKVNo5syZRFTbD3t7e5NEIqElS5bQtWvXKCcnh/7880/q27cvRUREEBFRTEwMSaVSunfvnsp5RKTY1vft20dRUVGUlZVFBw8eJCsrK/rkk08Y/1u3btHixYspNjaWsrOz6dChQ2Rra0u9e/dmxevl5UUdOnSgS5cu0Y0bN+jjjz+m9u3bU2VlJRHVWlXdvXs3paamUmpqKi1ZsoT4fD5t27aNFY+/v79CfWkOL8uqXrMnThKJhLKzs4mIPXHKysoisVjc3OheO9zE6fVTUVFB/v7+5O/vz/pB5uB422nuxCk3N1dh0lR3YN4S5Obmkp+fH1lZWZFIJCILCwsaOnQoa2BJRLR06VICQMuXL1caDwCFy8rKiiUzcuRIZmCZn59PQqGQ9u3bpzS+adOmUZcuXZj79PR0Gj58OJmbm5OmpiZ16tSJtmzZomCi+dmzZ7Rs2TJycnIidXV1MjAwoJ49e1JISAjz4iY4OJjqvzf09fVVmoa6+QCAgoODmXuZTEYLFy4kExMTEovF1K9fP0pPT2fF+/jxY/Lx8SEtLS3S0dGhCRMmKJT59evXqVevXiQWi8nCwoICAwMV8mPfvn3k4OBAIpGIOnToQEePHmX5v8jEqU+fPkrTLP+NV5ZmotoBb5s2bUgkElH37t3pypUrCvHWNcl89uxZateuHYnFYmrVqhWNGzdOqanqP//8k9577z0Si8Xk6OhImzdvZvm/rPx+WROnu3fvklAopL179ypNtzxs586dSSQSka2trUJe+vv7s9rJP//8Qz169CBNTU3S0NCgfv36Kc1fZeVW99kTJ05k2rSRkRH169ePNWmSP1tZPHV1VHXiFB0dTQCYCU1RURHNmzeP7O3tSSQSkYmJCXl4eFBERATTZuunvbG0yeukvF7XLb/y8nL63//+R/r6+qShoUHDhw+nvLw8Vrw5OTk0aNAgkkgkZGhoSN99953Ci9ymyoqo6bpfd+K0f/9+BTPodWnXrh19++23zH1kZCR5enqSvr4+qaurk6OjI82aNYtyc3MZmZqaGtq4cSN169aNNDQ0SEdHh5ydnWnNmjVUVlbGpKN+O1Ylj+qX/Zo1a8jS0pLU1NSoTZs29MMPP7DGcHfv3qXevXuTgYEBicVisrOzo9mzZyuMr4uLi2nixImkp6dHBgYGNHz4cNaREyEhIdSuXTsmPd27d2eZTpcjlUqZowdehJc1ceIRNc90ka2tLTZv3gwPDw9oa2vj+vXrsLW1xY4dOxAYGIiUlJTmRPfaefLkCXR1dVFcXAwdHZ2WVqd5lJYC8n0FJSWAEhPCbyLV1dU4d+4cAKBPnz4qLYPg4HgbePbsGbKzs2FjY6PU8EFdkpOTWQd1AmCZTf4vkJSUhP79+yMrK6vZe6Q4GickJAQhISE4e/ZsS6vyxmNtbY2QkBC4u7u3tCpvPOPHj4e1tTVn4ElFAgICkJOT88JHKnAo5/jx4/juu++QlJT0wmPIxn6vmzM3aPbTJ0+ejOnTp2Pbtm3g8XjIzc1FdHQ0Zs2ahYULFzY3uv80mSnX4ZNbaxRij7kF7Nt3amGNXg1CobDZ1rE4ON4lrl+/ztrDoqamhk6d3s323hgdO3bEzz//jOzsbDg5ObW0OhwcHBwcbwGlpaUIDg5+I168N1uDefPmQSaToV+/figrK0Pv3r0hFosxa9YsfP31169Cx3cWWU017gnMmf85ODjeTWxsbJCRkQGg1iJQY5ti33XqHhDKwcHBwcHRFCNGjGhpFRiaPXHi8XhYsGABZs+ejVu3bqGkpATt27fnll28ACaWNphzaXPt/52nNCH99kJEzNt2NTU1lc4r4OB428nNzUWrVq0gFouho6MDY2NjGBkZvfKDbTn+m3Tu3JmblKrIjBkzYG1t3dJqvBUMGzZM4eBUjoZxd3dXMPvN8W7R7D1ObzvcHqfXT2VlJWOq/vvvv4dIJGphjTg4Xg4NrZlOSkpiTkX/r+1j4uDg4ODgeNNosT1OH374YaNfDM6cOdPcKDk4ODjeGWJjY1n3Dx8+hLGxcQtpw8HBwcHBwfGyaPbEqXPnzqz7qqoqJCYmIjk5Gb6+vi9Lr/8Ej//Jx8YD2wEA0z7xRSujpk+BfxtRU1PD999/z/zPwfEuUl5ejps3b7LcTE1NuUkTBwcHBwfHO0KzJ06rV69W6h4QEICSkpJ/rdB/iUf597HOwRMA8Fn+/Xd24sTj8bjleRzvNPfv30dhYSHLzcnJCWKxuIU04uDg4ODg4HjZ8F9WRGPHjsW2bdteVnT/CQQiMRyqsuFQlQ2BiBtgcXC8jVRVVbEmTTweD++//z43aeLg4ODg4HjHeGkG0aOjo5s8AJKDjZ30PZyXvtfSarxyuANwOd5l1NTUGEMQ6urqeO+9d79Nc3BwcHBw/Bdp9gj2k08+Yd0TEfLy8hAbG8sdgMuhFJlMhgsXLgAAPvjggxbWhoPj3xMfH4+pU6fi/PnzAID33nsPFRUV0NXVbWHNODg4ODg4OF4VzV6qp6ury7oMDAzg7u6OY8eOwd/f/1XoyPGWw+fz4eLiAhcXF/D5L211KAdHizBu3Dg4Ozvj2rVrmDNnDuPOTZqa5vHjxzA2NkZOTk5Lq/LOERAQwJ3jpCI8Ho+rgyri7u6OkJCQllbjrWH8+PEICAhoaTXeOUaNGoVVq1a1tBoAmjlxqqmpwYQJE/DLL78gODgYwcHBCAoKQmBgIAYMGPCqdHxnuZWeDLfIw3CLPIxb6cktrc4rQygUYtCgQRg0aBC3TI/jrcbQ0BC7du1i7k+fPt2C2rwc8vPz8fXXX8PW1hZisRitW7fGkCFDcPLkSRgaGiIwMFBpuJ9++gkmJiaoqqrCgQMH0L9/fxgZGUFHRweurq44efKkQpglS5bAy8tL6eGjnp6eEAgEuHbtmoKfu7s7ZsyYoeAeEhKicDjnkydPsGDBAjg6OkJdXR2mpqbw8PDAgQMH0Nixhd988w2cnZ0hFosVrMc2xLNnz+Dn54dWrVpBS0sLn376KR48eMCSuXv3LgYPHgwNDQ0YGxtj9uzZqK6uZsmcPXsWXbt2hVgshp2dndKB6vr162FtbQ11dXW4uLjg6tWrjep29uxZ8Hg8heuHH35gdB8/fjycnJwgFAoxbNgwldJMRPjxxx9hZmYGiUQCDw8PZGZmNhrm6dOnmDFjBqysrCCRSODm5qa0nFNTUzF06FDo6upCU1MT3bp1w927dxn/l5Xf9XF3d2fyR11dHe3bt8eGDRsY/5CQEMafz+fDzMwM3t7eLN0aIjw8nKmLTk5OOHbsWJNh1q9fj3bt2kEikUAqlWLHjh0s/y1btuCDDz6Avr4+9PX14eHhoVAfAgIC4OjoCE1NTUYmJiaGJbNkyRK4ublBQ0ND5UNuAwICmLwQCARo3bo1pkyZgoKCApZcQ/1KY33mi9bJgoICjBkzBjo6OtDT08OkSZMUjJUlJSXhgw8+gLq6Olq3bo3ly5crxNNUWb1I3ZdTXl4OAwMDGBoaoqKiQqnMH3/8AXd3d+jq6kJLSwsdO3bE4sWLWXlbWVmJ5cuXo1OnTtDQ0IChoSF69uyJ4OBgVFVV/as8qk9WVhaGDx/O9OsjR45UaG/W1tYKfUz93wwiwsqVK+Hg4ACxWAwLCwssWbKE8W+or8rPz2dkfvjhByxZsgTFxcWN6vw6aNbESSAQYMCAAdypyC+JmsoK3Ba2wW1hG9RUKm9IHBwcbwY8Hg+PHz9m7jt37oy4uLgW1Ojfk5OTA2dnZ5w5cwYrVqzAjRs3cOLECXz44YeYPn06xo4di+DgYIVwRISQkBB8/vnnUFNTw/nz59G/f38cO3YMcXFx+PDDDzFkyBAkJCQwYcrKyhAUFIRJkyYpxHf37l1cvnwZX3311b8yMlRUVAQ3Nzfs2LED8+fPR3x8PM6fPw9vb2/MmTOnyR/diRMnwtvbW+Xnffvtt/jzzz8RHh6Oc+fOITc3l7WcvaamBoMHD0ZlZSUuX76M7du3IyQkBD/++CMjk52djcGDB+PDDz9EYmIiZsyYgS+++II18dy7dy9mzpwJf39/xMfHo1OnTvD09MTDhw+b1DE9PR15eXnMNW/ePEY3iUSCb775Bh4eHiqnefny5fjtt9+wadMmxMTEQFNTE56ennj27FmDYb744gtERkZi586duHHjBgYMGAAPDw/cv3+fkcnKykKvXr3g6OiIs2fPIikpCQsXLmTtnX4Z+d0QkydPRl5eHlJSUjBy5Ej4+flhz549jL+Ojg7y8vJw//59/PHHH0hPT8dnn33WaJyXL1+Gj48PJk2ahISEBAwbNgzDhg1DcnLDL0o3btyI+fPnIyAgADdv3sSiRYvg5+eHP//8k5E5e/YsfHx8EBUVhejoaLRu3RoDBgxg5aeDgwPWrVuHGzdu4OLFi7C2tsaAAQPwzz//MDKVlZX47LPPMG3atCbzpy4dOnRAXl4e7t69i+DgYJw4cYIVR2P9ip+fX4PxvmidHDNmDG7evInIyEgcOXIE58+fx5QpUxj/J0+eYMCAAbCyskJcXBxWrFiBgIAAbN68mZFRpaxepO7L+eOPP9ChQwc4Ojri4MGDCv4LFiyAt7c3unXrhuPHjyM5ORmrVq3C9evXsXPnTgC15eXp6YnAwEBMmTIFly9fxtWrV+Hn54e1a9cqHIfRnDyqT2lpKQYMGAAej4czZ87g0qVLqKysxJAhQyCTyViyixcvZvUxX3/9Nct/+vTp2Lp1K1auXIm0tDQcPnwY3bt3V3hm/b6q7lEe7733Htq2bct6cdliUDNxdnamU6dONTfYG0NxcTEBoOLi4pZWhR49zKMlG5fRko3L6NHDvKYDlJQQAbVXScmrV5CDg4NOnz5NAFjXunXriIiovLycUlJSqLy8XDFgRUntJZM9d6uqqHWreqZctqbmuVt1Za1bZblqsi/AoEGDyMLCgkqU9CeFhYWUlJREAOjChQssv6ioKAJAqampDcbdvn17WrRoEXMfHh5ORkZGSmUDAgJo1KhRlJqaSrq6ulRWVsby79OnD02fPl0hXHBwMOnq6jL306ZNI01NTbp//76C7NOnT6mqqqpBfeX4+/tTp06dmpQrKioiNTU1Cg8PZ9xSU1MJAEVHRxMR0bFjx4jP51N+fj4js3HjRtLR0aGKigoiIpozZw516NCBFbe3tzd5enoy9927dyc/Pz/mvqamhszNzWnZsmUsvX19fZl7eRkVFhY2mRZfX1/y8vJqUk4mk5GpqSmtWLGCcSsqKiKxWEx79uxRGqasrIwEAgEdOXKE5d61a1dasGABc+/t7U1jx45t8NkvK7+JiABQdnY2c6+sftnb29OoUaOISLGeERH99ttvTY4lRo4cSYMHD2a5ubi40JdfftlgGFdXV5o1axbLbebMmdSzZ88Gw1RXV5O2tjZt3769QRn52EfZ+E1Z+uT06dOHgoODmXtl7WPmzJmkr6/P3DfVr6iCqnUyJSWFANC1a9cYt+PHjxOPx2P6gQ0bNpC+vj6rDsydO5ekUilz31RZqVr3fX19yd/fX0FPd3d32rRpE23cuJH69+/P8ouJiSEA9OuvvypNozzPfv75Z+Lz+RQfH68gU1lZqTS/iVTLo/qcPHmS+Hw+q34XFRURj8ejyMhIxs3KyopWr16tNA75s4VCIaWlpTUoo2pftWjRIurVq1ejMo3R2O91c+YGzd5w8n//93+YNWsWjhw5gry8PDx58oR1cahOKyNTfD91Hr6fOu+dPcMJqH1LEhAQgICAAMb6GAfH20K/fv1Y96WlpY2+NWVYal57lT3/SoXLa2rdjs1iy66wq3Uv/vu529UttW6Hv2LL/upU6/4o/blbYqiKqXlOQUEBTpw4AT8/P2hqair46+npwcnJCd26dVP4ChQcHAw3Nzc4OjoqjVsmk+Hp06cwMDBg3C5cuABnZ2cFWSJCcHAwxo4dC0dHR9jZ2WH//v3NTo9MJkNYWBjGjBkDc3NzBX8tLS1mqXBAQIDS5YLNIS4uDlVVVaw3446OjmjTpg2io6MB1FqbdXJygomJCSPj6emJJ0+eMG+Ho6OjFd6ue3p6MnFUVlYiLi6OJcPn8+Hh4cHIvCpycnLA4/Fw9uxZALVfx/Lz81m66OrqwsXFpUFdqqurUVNTo2B1VyKR4OLFiwBqy+7o0aNwcHCAp6cnjI2N4eLiwnoz/7LyW1UkEkmDv1cPHz5EREQEBAIBBAIB425tbc3a39JU2SqjoqJCaV5dvXq1waVYZWVlqKqqYrW3ulRWVmLz5s3Q1dVFp06dGnz2i5CTk4OTJ08yZzWq0q/IGT9+PNzd3f/V86Ojo6Gnp4f333+fcfPw8ACfz2eWJkZHR6N3796s8yQ9PT2Rnp7OHCXRVFm9SN2Xk5WVhejoaIwcORIjR47EhQsXcOfOHcY/NDQUWlpa+N///qc0vDzPQkND4eHhgS5duijIqKmpMfktX1ranDyqT0VFBXg8HutYDXV1dfD5fKbdygkMDESrVq3QpUsXrFixgrU09s8//4StrS2OHDkCGxsbWFtb44svvlBY2gnUruIwMzND//79cenSJQX/7t274+rVqw0udXxdqDxxWrx4MUpLS/HRRx/h+vXrGDp0KCwtLZk1tnp6etDX13+VunJwcHC8duQbUtXV1UFE0NDQaGGNXg63bt0CETU4+ZEzadIkhIeHM+vhnz59iv3792PixIkNhlm5ciVKSkowcuRIxu3OnTtKJzSnTp1CWVkZPD1rDwMfO3YsgoKCmp2eR48eobCwsMn0ALV71dq2bdvsZ9QlPz8fIpFIYW+IiYkJszY/Pz+fNYiX+8v9GpN58uQJysvL8ejRI9TU1CiVqbsHoCEsLS2hpaXFXHWXmzaFmpoapFIpU+flz2uOLtra2nB1dcVPP/2E3Nxc1NTUYNeuXYiOjkZeXh6A2olISUkJAgMDMXDgQPz1118YPnw4PvnkE+Yoi5eV300h1y8pKQl9+/Zl3IuLi6GlpQVNTU2YmJggKipKYXLQtm1bGBoaMvcN6dOYLp6enti6dSvi4uJARIiNjcXWrVtRVVWFR48eKQ0zd+5cmJubKwz8jxw5Ai0tLairq2P16tWIjIxk6fei3LhxA1paWpBIJLCxscHNmzcxd+5cAKr3KwBgZmaGNm3a/Ctd8vPzWUu6gNp91QYGBi+lHdb1rxtOmUxDbNu2DYMGDYK+vj4MDAzg6enJWgKdmZkJW1tbqKmpNRpPZmamSvmqq6sLqVTK3KuSR/Xp0aMHNDU1MXfuXJSVlaG0tBSzZs1CTU0N026B2r2hYWFhiIqKwpdffomlS5eyjCbdvn0bd+7cQXh4OHbs2IGQkBDExcVhxIgRjIyZmRk2bdqEP/74A3/88Qdat24Nd3d3xMfHs3QyNzdHZWWlym35VaHyTv1FixZh6tSpiIqKepX6/KcoKizA5r21a2yneE+Bnr7yt0VvO2pqapg9ezbzPwfHm0xZWRlatWqFx48fQ0NDAzNnzsTUqVObP2H6Prf2r1qdcG7TgR7/A/j1ut7Zt2r/CiXP3bpPBpx9AZ6ALTvjhqJs5zHN0w1o1FBCXXx8fPDtt99i3759mDhxIvbu3Qs+n9/gXqDdu3dj0aJFOHToEOvHury8XOlZf9u2bYO3tzfzNcjHxwezZ89GVlZWsyY3qqYHAL766it89dVXTQu+A1y4cAHa2trMfXNecFpYWCAtLe1f67Bz505MnDgRFhYWEAgE6Nq1K3x8fJg9gvI9E15eXvj2228B1L59vnz5MjZt2oQ+ffr8ax2aYsOGDdi6dSsqKyshEAjw7bffsvbtaGtrIz4+HlVVVTh+/DhCQ0NZG9yBl2MsZuHChcjPz0ePHj1ARDAxMYGvry+WL1+u1CptYGAgwsLCcPbsWYX2Jd839+jRI2zZsgUjR45ETEyMwiC6uUilUhw+fBjPnj3Drl27kJiYyOxraU47XLZs2b/S422gpqYG27dvx5o1axi3sWPHYtasWfjxxx/B5/NVzjNV5YYPH47hw4e/kL5yjIyMEB4ejmnTpuG3334Dn8+Hj48PunbtyqqHM2fOZP7v2LEjRCIRvvzySyxbtgxisRgymQwVFRXYsWMHHBwcAABBQUFwdnZGeno6pFIpc8lxc3NDVlYWVq9ezezvAmq/vAK1v9EticpfnOQF1qdPn0YvDtV5cC8bv0gH4hfpQDy4l93S6rwyeDweNDU1oampyfp8zMHxphEREQFNTU08e/aM9Sb5hb4yiTRrr7p1XiiqdROKlcvWHRgJ1Grd1NRVk20m9vb24PF4TQ6MdXR0MGLECOYNaXBwMEaOHAktLS0F2bCwMHzxxRfYt2+fwttvQ0NDZlmMnIKCAkRERGDDhg0QCoUQCoWwsLBAdXU1a3mgjo6OUsMORUVFjBl4IyMj6OnpvZSBviqYmpqisrJSwVjSgwcPYGpqysjUt0Ilv29KRkdHBxKJBIaGhhAIBEpl5HE0ho2NDezs7Jjr3xwJIX9ec3Vp27Ytzp07h5KSEvz999/MsjNbW1sAtXVDKBSiffv2rHDt2rVjLNe9rPxuiDFjxiAxMRHZ2dkoLS3FL7/8wsorPp8POzs7tGvXDjNnzkSPHj2aNKrQkD6N6SKRSLBt2zaUlZUhJycHd+/ehbW1NbS1tWFkZMSSXblyJQIDA/HXX3+hY8eOCnFpamrCzs4OPXr0QFBQEIRC4Qt9za2PSCSCnZ0d3nvvPQQGBkIgEGDRokUAVO9XXhampqYKRlKqq6tRUFDwUtphXf+64ZTJKOPkyZO4f/8+83JIKBRi1KhRuHPnDjPRdnBwwO3btxu1iieXe5F8VSWPlDFgwABkZWXh4cOHePToEXbu3In79+8z7VYZLi4uqK6uZsz9m5mZQSgUMpMmoLZdA2jUKmX37t1x69Ytlpt8eV/9dvC6aVYPyg16Xy4CkRhWNfdgVXMPApG46QAcHByvjIEDByoc8P0uI18ysn79epSWlir41x2gTpo0CRcvXsSRI0dw+fJlpZbx9uzZgwkTJmDPnj0YPHiwgn+XLl2QkpLCcgsNDYWlpSWuX7+OxMRE5lq1ahVCQkJQU1MDoPYNd/1lG0DtQcTyH2Q+n49Ro0YhNDQUubm5CrIlJSVNmqVuDs7OzlBTU2N9ZUhPT8fdu3fh6uoKAHB1dcWNGzdYg5bIyEjo6OgwkwRXV1eFLxWRkZFMHCKRCM7OziwZmUyG06dPMzKvCxsbG5iamrJ0efLkCWJiYlTSRVNTE2ZmZigsLMTJkyfh5eUFoDaN3bp1Q3p6Oks+IyMDVlZWAF5efjeErq4u7OzsYGFhodLkct68edi7d6/SeimnqbJtDDU1NVhaWkIgECAsLAwff/wxS6/ly5fjp59+wokTJ1h7VxpD/vb/ZfPDDz9g5cqVyM3NbVa/8jJwdXVFUVERy8LpmTNnIJPJ4OLiwsicP3+eNTGJjIyEVCplvsA2VVYvWveDgoIwatQoVv+WmJiIUaNGMZPY0aNHo6SkhGUCvy7yPBs9ejROnTrFslYqp6qqSml+q5pHjWFoaAg9PT2cOXMGDx8+xNChQxuUTUxMBJ/PZ75q9uzZE9XV1cjKymJkMjIyAIBp2w3FY2ZmxnJLTk6GpaXlS1lu+q9Q1RoFj8cjPT090tfXb/R603mTrOo1ikxWazmv7vXgwVtpVa+qqorOnTtH586dU8mqFQfH60ZLS4tlNU9TU1OlcI1a1XsLyMrKIlNTU2rfvj3t37+fMjIyKCUlhdasWUOOjo6MnEwmIzs7O9LX12e5ywkNDSWhUEjr16+nvLw85ioqKmJkkpKSSCgUUkFBAePWqVMnmjt3rkJ8RUVFJBKJGEtsWVlZpK6uTl9//TVdv36d0tLSaNWqVSQUCun48eNMuMePH5OjoyNZWlrS9u3b6ebNm5SRkUFBQUFkZ2fHWG1au3Yt9e3bl/XMzMxMSkhIoC+//JIcHBwoISGBEhISGEtc9+7dI6lUSjExMUyYqVOnUps2bejMmTMUGxtLrq6u5OrqyvhXV1fTe++9RwMGDKDExEQ6ceIEGRkZ0fz58xmZ27dvk4aGBs2ePZtSU1Np/fr1JBAI6MSJE4xMWFgYicViCgkJoZSUFJoyZQrp6emxrMe9iFW9mzdvUkJCAg0ZMoTc3d2ZNMtRlubAwEDS09OjQ4cOUVJSEnl5eZGNjQ2rDfTt25fWrl3L3J84cYKOHz9Ot2/fpr/++os6depELi4uVFn53BrkgQMHSE1NjTZv3kyZmZm0du1aEggELIuOLyO/iVSzqleXhqzO1bfEVj/dly5dIqFQSCtXrqTU1FTy9/cnNTU1unHjBiMzb948GjduHHOfnp5OO3fupIyMDIqJiSFvb28yMDBg6RsYGEgikYj279/Pam9Pnz4lIqKSkhKaP38+RUdHU05ODsXGxtKECRNILBZTcnIyE8+dO3coISGBFi1aRFpaWkz5y+OR501TVvWI2JYfVe1X6qedqOk6GRMTQ1KplO7du8e4DRw4kLp06UIxMTF08eJFsre3Jx8fH8a/qKiITExMaNy4cZScnExhYWGkoaFBv//+e7PKSpW6X9eq3sOHD0lNTY3VR8k5duwYicVievz4MRHVWtcUCAQ0e/Zsunz5MuXk5NCpU6doxIgRjLW9Z8+e0QcffED6+vq0bt06SkxMpKysLNq7dy917dqVyacDBw6wLAaqkkfK2vq2bdsoOjqabt26RTt37iQDAwOaOXMm43/58mVavXo1o8euXbvIyMiIPv/8c0ampqaGunbtSr1796b4+HiKjY0lFxcXlmXB1atX08GDBykzM5Nu3LhB06dPJz6fr2AB0tfXlyZOnKiQl6rysqzqNWvitGbNGgoJCWn0etN5KyZOMhmRm9vzSZKy6y2aOFVUVJC/vz/5+/uzzIFycLQ0paWlCqbG+/Xrp3L4t33iRESUm5tLfn5+ZGVlRSKRiCwsLGjo0KEUFRXFklu6dCkBoOXLlyvE0adPH4V8BMAayBPVDq42bdpERESxsbEEgK5evapUr0GDBtHw4cOZ+6tXr1L//v3JyMiIdHV1ycXFhSIiIhTCFRUV0bx588je3p5EIhGZmJiQh4cHRUREkOz/m4b39/cnKysrldIgH7BmZ2cTAFa+lJeX0//+9z/S19cnDQ0NGj58OOXlsY+WyMnJoUGDBpFEIiFDQ0P67rvvFF4gRUVFUefOnUkkEpGtrS1roCpn7dq11KZNGxKJRNS9e3e6cuUKy/9FJk5WVlZK0yxHWZplMhktXLiQTExMSCwWU79+/Sg9PV0h3rommffu3Uu2trYkEonI1NSU/Pz8WJNqOfIJrrq6OnXq1IkOHjzI8n9Z+f2yJk7R0dEEgBls1k83EdG+ffvIwcGBRCIRdejQgY4ePcry9/X1pT59+jD3KSkp1LlzZ5JIJKSjo0NeXl4KppwbKjf5s8vLy2n48OFkbm5OIpGIzMzMaOjQoQptzdfXV2k8dctb1YnTnj17SCwW0927d4lItX6lftobS5sceb2uW36PHz8mHx8f0tLSIh0dHZowYQJr8kdEdP36derVqxeJxWKysLCgwMBAhTQ0VVaq1P26E6eVK1eSnp4e6wWBnIqKCtLT06M1a9Ywbnv37qXevXuTtrY2aWpqUseOHWnx4sWsNvzs2TNatmwZOTk5kbq6OhkYGFDPnj0pJCSEqefBwcGsPFMlj5S19blz55KJiQmpqamRvb09rVq1iulDiYji4uLIxcWFdHV1SV1dndq1a0dLly6lZ8/Yx23cv3+fPvnkE9LS0iITExMaP348M2EkqjWz3rZtWyY97u7udObMGVYc5eXlpKuryxw98CK8rIkTj0i13WZ8Pl+pZY63jSdPnkBXVxfFxcXQ0dFpaXWUU1oKKNk/wNCzJ3DhAnvvxBtMdXU1cwL3Rx99xGwC5+BoaRwdHVnLgw4cONCsTbXPnj1DdnY2bGxslBo+4GBz9OhRzJ49G8nJyf9qrw2HIgEBAcjJyUFISEhLq/LGw+PxkJ2d/a9N0v8XcHd3x/jx4zF+/PiWVuWtYPz48Qpm6Tn+PRs3bkRERAT++uuvF46jsd/r5swNVB7BcvubXj630pMxJad20LbZWgo76XuKQg8eAPXPQtDQeGsmTUCt2cvG1sRycLQUaWlpTN9WWlr6zpgaf1MZPHgwMjMzcf/+fbRu3bql1eHg4ODgeAtQU1PD2rVrW1oNAM2YOKn4YYqjGdRUViBF1Jb5XymamooTJw4OjhdGIpHA2toaqampALi+7XUzY8aMllaBg4ODg+Mt4osvvmhpFRhUnjjJz1ngeHkYmlpgWvT22v/f821hbTg43m3KysoYE+NpaWkIDAzEvHnzWlgrDo6Xg7u7+0u3WPau4u/vr3CQLodyxo8fj86dO7e0Gm8Nw4YN4+rWO47Ke5zeFd66PU4lJW/9F6fKykqsWLECADB79myIRKIW1ojjv8b69esVDjw9ffo0+vbt+6/i5fY4cXBwcHBwvPm89j1OHBz/hqYOduPgeFV06tQJSUlJLLf/2PsiDg4ODg4OjpcAN3FqQYoKC7B97xYAgK/3ZOjpG7SwRq8GoVCI6dOnM/9zcLwuxGIxKisrmftWrVrh0aNHLagRBwcHBwcHx9sKZw+2BXlwLxvLpJ5YJvXEg3vZLa3OK4PP50NfXx/6+vqcCWKO10ZZWRlr0jR27Fhu0sTBwcHBwcHxwnCj2BaELxDCVPYAprIH4Au4LzEcHC8TDQ0NxhLPpUuXsHPnzhbWiIODg4ODg+NthhuttyD27TshsX2nllbjlVNTU4OrV68CALp37w6BQNDCGnG8q7Rr1w5paWnMHqYtW7Zgy5YtLawVBwcHBwcHx7sA98WJ45VTU1ODkydP4uTJk6ipqWlpdTjeUdTU1JCWlgYA3OT8DeXx48cwNjZGTk5OS6vyzhEQEIDx48e3tBpvBTwej6uDKuLu7o6QkJCWVuOtISQkBO7u7i2txjtHSkoKLC0tUVpa2tKqcBMnjlcPj8eDk5MTnJycwOPxWlodjneMu3fvgsfjobq6mnGbMmVKC2r0dpGfn4+vv/4atra2EIvFaN26NYYMGYKTJ0/C0NAQgYGBSsP99NNPMDExQVVVFS5evIiePXuiVatWkEgkcHR0xOrVqxXCLFmyBF5eXrC2tlbw8/T0hEAgwLVr1xT83N3dlR6cGxISonBmypMnT7BgwQI4OjpCXV0dpqam8PDwwIEDBxq1pnj37l0MHjwYGhoaMDY2xuzZs1l1Shnx8fHo378/9PT00KpVK0yZMgUlJSVKZR8/fgxLS0vweDzWeUvjx48Hj8dTuDp06MAKv379elhbW0NdXR0uLi7MV/yGOHv2rNJ4f/jhBwC1pnnHjx8PJycnCIVCDBs2rNH45BARfvzxR5iZmUEikcDDwwOZmZmNhqmpqcHChQthY2MDiUSCtm3b4qeffmKVR0lJCb766itYWlpCIpGgffv22LRpEyueZ8+ewc/PD61atYKWlhY+/fRTPHjwgCXzIuXo7u7O5I+6ujrat2+PDRs2MP4hISGMP5/Ph5mZGby9vXH37t0m8ys8PJypi05OTjh27FiTYUJDQ9GpUydoaGjAzMwMEydOxOPHj5XKhoWFgcfjKS2/1NRUDB06FLq6utDU1ES3bt0YnQsKCvD1119DKpVCIpGgTZs2+Oabb1BcXNyobgEBAUxeCAQCtG7dGlOmTEFBQQFLrqF+5fTp043Gn5SUhA8++ADq6upo3bo1li9f3qg8UHu0hJubG7S1tWFqaoq5c+eyylzVul5RUYEFCxbAysoKYrEY1tbW2LZtG+Nft57UvQYPHtykjkDjfRwAJCQk4LPPPoOJiQnU1dVhb2+PyZMnIyMjgyX3xx9/wN3dHbq6utDS0kLHjh2xePFihTKoy6tqt0Dj9QwAsrKyMHz4cBgZGUFHRwcjR45UaLcAcPToUbi4uEAikUBfX59VTu3bt0ePHj3wyy+/NKrza4H+YxQXFxMAKi4ubmlV6HZmKg08tocGHttDtzNTn3uUlBABtVdJScspyMHxhjN37lwCwLpSU1ObDviSKC8vp5SUFCovL39tz3yZZGdnk7m5ObVv3572799P6enplJycTKtWrSKpVErTp08nBwcHhXAymYxsbW1p1qxZREQUHx9Pu3fvpuTkZMrOzqadO3eShoYG/f7770yY0tJS0tHRoejoaIX47ty5Q1paWvTNN9/Q1KlTFfz79OlD06dPV3APDg4mXV1d5r6wsJA6dOhAlpaWFBISQjdv3qT09HTavHkztW3blgoLC5XmQ3V1Nb333nvk4eFBCQkJdOzYMTI0NKT58+c3mHf3798nfX19mjp1KqWlpdHVq1fJzc2NPv30U6XyXl5eNGjQIALA0qOoqIjy8vKY6++//yYDAwPy9/dnZMLCwkgkEtG2bdvo5s2bNHnyZNLT06MHDx4wMv7+/uTr68vcR0VFEQBKT09nxf/06VMiIiopKaGpU6fS5s2bydPTk7y8vBpMa10CAwNJV1eXDh48SNevX6ehQ4eSjY1No21gyZIl1KpVKzpy5AhlZ2dTeHg4aWlp0Zo1axiZyZMnU9u2bSkqKoqys7Pp999/J4FAQIcOHWJkpk6dSq1bt6bTp09TbGws9ejRg9zc3Bh/VcsRAGVnZzP3ffr0ocmTJ1NeXh5lZWWRv78/AaDdu3cTUW0909HRoby8PMrNzaVLly5Rp06dqHv37o3m1aVLl0ggENDy5cspJSWFfvjhB1JTU6MbN240GObixYvE5/NpzZo1dPv2bbpw4QJ16NCBhg8friCbnZ1NFhYW9MEHHyiU361bt8jAwIBmz55N8fHxdOvWLTp06BBTZ27cuEGffPIJHT58mG7dukWnT58me3t7hfrbp08fCg4OZu79/f2pQ4cOlJeXR/fu3aPIyEhq3bo1jRw5kqVXY/1KQxQXF5OJiQmNGTOGkpOTac+ePSSRSFj9SH0SExNJJBLRokWLKDMzk86ePUuOjo703XffMTKq1vWhQ4eSi4sLRUZGUnZ2Nl2+fJkuXrzI+D9+/JjVlpKTk0kgELDyJzg4mPr06aMQd1N93J9//kkikYiGDBlCkZGRdPv2bbpy5Qp99913rLz9/vvvSSAQ0KxZs+jSpUuUnZ1Nf/31F33yySf066+/NphPr6rdNlXPSkpKyNbWloYPH05JSUmUlJREXl5e1K1bN6qpqWHi2b9/P+nr69PGjRspPT2dbt68SXv37mXpc+TIETIzM6OqqqoGdW6Mxn6vmzM34CZOLUhaUiyZnEkgkzMJlJYU+9yDmzhxcDSJlZUVa8LE5/Nfuw6NdcSllaVUWllKMpmMcausrqTSylKqqK5QKlsje/5DUllTK/us+plKsi/CoEGDyMLCgkqU9DOFhYWUlJREAOjChQssP/mgvLFJ6vDhw2ns2LHMfXh4OBkZGSmVDQgIoFGjRlFqairp6upSWVkZy1/VidO0adNIU1OT7t+/ryD79OnTBn9wjx07Rnw+n/Lz8xm3jRs3ko6ODlVUVCgN8/vvv5OxsTHrx1+eX5mZmSzZDRs2UJ8+fej06dMKE6f6REREEI/Ho5ycHMate/fu5Ofnx9zX1NSQubk5LVu2jHFraOLU2LPk+Pr6qjRxkslkZGpqSitWrGDcioqKSCwW0549exoMN3jwYJo4cSLL7ZNPPqExY8Yw9x06dKDFixezZLp27UoLFixgnqOmpkbh4eGMf2pqKgFgJuOqlqOyiVP9+mVvb0+jRo0iIsV6RkT022+/NTmWGDlyJA0ePJjl5uLiQl9++WWDYVasWEG2trYKz7KwsGC5VVdXk5ubG23dulVp+Xl7e7Panyrs27ePRCIRq50omzh16tSJFW7mzJmkr6/P3DfVrzTEhg0bSF9fn1VWc+fObXSyNX/+fHr//fdZbocPHyZ1dXV68uSJgnxDdf348eOkq6tLjx8/bvBZ9Vm9ejVpa2uz0tnQxKmxPq60tJQMDQ1p2LBhSp8jz7OYmBgC0OAEqaG8fZXttql6dvLkSeLz+ax2UlRURDwejyIjI4mIqKqqiiwsLGjr1q0NxkNEVFFRQWKxmE6dOtWoXEO8rIkTt1SvBdE3MsOk7EhMyo6EvpFZS6vDwfFWcf/+feZ/KyurN27/nMtuF7jsdkFhRSHjFnwzGC67XbA0ZilL1n2fO1x2uyCvNI9xC0sLg8tuF/x46UeW7MA/BsJltwtuF91m3A7dOtRs/QoKCnDixAn4+flBU1NTwV9PTw9OTk7o1q0ba7kKAAQHB8PNzQ2Ojo5K405ISMDly5fRp08fxu3ChQtwdnZWkCUiBAcHY+zYsXB0dISdnR3279/f7PTIZDKEhYVhzJgxMDc3V/DX0tJizpELCAhgLReMjo6Gk5MTTExMGDdPT088efIEN2/eVPq8iooKiEQi1hELEokEAHDx4kXGLSUlBYsXL8aOHTtUOo4hKCgIHh4esLKyAgBUVlYiLi4OHh4ejAyfz4eHhweio6ObjO/fkJOTAx6Ph7NnzwIAsrOzkZ+fz9JFV1cXLi4ujeri5uaG06dPM0uOrl+/josXL2LQoEEsmcOHD+P+/fsgIkRFRSEjIwMDBgwAAMTFxaGqqor1bEdHR7Rp04Z59ouUY0NIJBLWcQZ1efjwISIiIiAQCFj7Ka2trREQEMDcR0dHs/SV69NYXrm6uuLvv//GsWPHQER48OAB9u/fj48++oglt3jxYhgbG2PSpEkKcchkMhw9ehQODg7w9PSEsbExXFxccPDgwUbTXFxcDB0dnWadt5iTk4OTJ09CJBIBUK1fkTN+/HjWfqDo6Gj07t2biQuoza/09HQUFhZCGRUVFVBXV2e5SSQSPHv2DHFxcSqn4/Dhw3j//fexfPlyWFhYwMHBAbNmzUJ5eXmDYYKCgjBq1Cil6axLU33cyZMn8ejRI8yZM0dpeHmehYaGQktLC//73/8alXtd7VaVelZRUQEejwexWMy4qaurg8/nM/1kfHw87t+/Dz6fjy5dusDMzAyDBg1CcnIySx+RSITOnTvjwoULDer8OuAmTi2Isak5lkycjSUTZ8PYVPGH/l2hsrISy5cvx/Llyxv8IeLgaC5VVVXg8XiYO3cut9H7Bbh16xaIqMHJj5xJkyYhPDyc2bvz9OlT7N+/HxMnTlSQtbS0hFgsxvvvvw8/Pz/GHDwA3LlzR+mE5tSpUygrK4OnpyeA2vO2goKCmp2eR48eobCwsMn0AIChoSHatm3L3Ofn57MG2wCY+/z8fKVx9O3bF/n5+VixYgUqKytRWFiIefPmAQDy8monwBUVFfDx8cGKFSvQpk2bJvXKzc3F8ePHWfn26NEj1NTUKNWvId3qYmlpCS0tLeZqaK+MMtTU1CCVSqGhoQHgeV40V5d58+Zh1KhRcHR0hJqaGrp06YIZM2ZgzJgxjMzatWvRvn17WFpaQiQSYeDAgVi/fj169+7NPFskEinsaav77Bcpx/rU1NRg165dSEpKQt++fRn34uJiaGlpQVNTEyYmJoiKilKYHLRt2xaGhobMfUP6NKZLz549ERoaCm9vb4hEIpiamkJXVxfr169nZC5evIigoKAGLYY+fPgQJSUlCAwMxMCBA/HXX39h+PDh+OSTT3Du3DmlYR49eoSffvpJpf2hN27cgJaWFiQSCWxsbHDz5k3MnTsXgOr9CgCYmZmx2sWLlJ+npycuX76MPXv2oKamBvfv38fixYsBPG+HqnD79m1cvHgRycnJiIiIwK+//or9+/c3OEm5evUqkpOTWW21IZrq4+R7jZrKs8zMTNja2kJNTa1RudfVblWpZz169ICmpibmzp2LsrIylJaWYtasWaipqWHK5/bt2peAAQEB+OGHH3DkyBHo6+vD3d1dYd+Wubk57ty502j6XzWcOXKO10JZWVlLq8DxljNt2jRs2rQJH330EY4ePQqg9o3Xm0rM6BgAgEQoYdwmdJiAse3GQshnd71nR54FAKgLn785HeU4Cp/afwoBn20h8MSnJxRkvey8mq0fNWIooS4+Pj749ttvsW/fPkycOBF79+4Fn8+Ht7e3guyFCxdQUlKCK1euYN68ebCzs4OPjw8AoLy8XOHNMABs27YN3t7ezFtuHx8fzJ49G1lZWazJzctKDwB89dVX+Oqrr1SWV0aHDh2wfft2zJw5E/Pnz4dAIMA333wDExMT5svS/Pnz0a5dO4wdO1alOLdv3w49PT2VDTWowoULF6Ctrc3c6+vrqxzWwsKCsVT5b9i3bx9CQ0Oxe/dudOjQAYmJiZgxYwbMzc3h6+sLoHbidOXKFRw+fBhWVlY4f/48/Pz8YG5urvDV5lWwYcMGbN26FZWVlRAIBPj2228xbdo0xl9bWxvx8fGoqqrC8ePHERoaiiVLlrDiaMrwgSqkpKRg+vTp+PHHH+Hp6Ym8vDzMnj0bU6dORVBQEJ4+fYpx48Zhy5YtrElaXeT9opeXF7799lsAQOfOnXH58mVs2rSJ9SUYqDWoMnjwYLRv3571xawhpFIpDh8+jGfPnmHXrl1ITEzE119/DaB57XDZsmUqyzbEgAEDsGLFCkydOhXjxo2DWCzGwoULceHCBZW+8MqRyWTg8XgIDQ2Frq4uAOCXX37BiBEjsGHDBuZrspygoCA4OTmhe/fuTcbdVB+nap6pKve62q0q9czIyAjh4eGYNm0afvvtN/D5fPj4+KBr165M+cjjWbBgAT799FMAtasaLC0tER4eji+//JLRSSKRtPh4kps4tSBFhQXYu792CYz3iInQ0zdoYY1eDUKhkHlr05wlABwccszNzZm3U6pYpXoT0FDTUHBTE6hBTaD4tlCpLF8NanzVZZuLvb09eDxekz+wOjo6GDFiBIKDgzFx4kQEBwdj5MiR0NLSUpC1sbEBADg5OeHBgwcICAhgJk6GhoYKy20KCgoQERGBqqoqbNy4kXGvqanBtm3bmIGpjo6OUmtfRUVFzCDHyMgIenp6LzRgMDU1VbBSJ7f6ZGpq2mC40aNHY/To0Xjw4AE0NTXB4/Hwyy+/wNbWFgBw5swZ3Lhxg1mWIx/4GBoaYsGCBVi0aBETFxFh27ZtGDduHGupkqGhIQQCgYIVqgcPHjSqmxwbGxuFrzQvivx5Dx48gJnZ8+XlDx48QOfOnRsMN3v2bObtNVBbP+7cuYNly5bB19cX5eXl+P777xEREcFYKOvYsSMSExOxcuVKeHh4wNTUFJWVlSgqKmKlp24+vGg5AsCYMWOwYMECSCQSmJmZKQy6+Xw+7OzsANSeF5eVlYVp06Y1erC2qalps8tt2bJl6NmzJ2bPns3kg6amJj744AP83//9Hx48eICcnBwMGTKECSMfeAqFQqSnp6N169YQCoVo3749K+527dqxlpECtV+QBw4cCG1tbURERDT5NQOoXTIlz4vAwEAMHjwYixYtwk8//aRyv6KMhvJL7tcQM2fOxLfffou8vDzo6+sjJycH8+fPZ9qhKpiZmcHCwoLpT4Da/CIi3Lt3D/b29ox7aWkpwsLCmC9bjaFKH+fg4AAASEtLg6ura4NxOTg44OLFi6iqqlKpnOS8qnZraGioUj0bMGAAsrKy8OjRIwiFQujp6cHU1JQpH7lOdeMRi8WwtbVVsFxZUFDQrBdqrwJuqV4L8uBeNvztPOBv54EH97JbWp1XBp/Ph7GxMYyNjZv1BoiDA6itP3WXXEil0hbU5t3BwMAAnp6eWL9+vdKzMeqazJ40aRIuXryII0eO4PLly0r3VdRHJpOhoqKCue/SpQtSUlJYMqGhobC0tMT169eRmJjIXKtWrUJISAizb00qlSI+Pl7hGfHx8cygg8/nY9SoUQgNDUVubq6CbElJSYNmqV1dXXHjxg08fPiQcYuMjISOjo7CoEAZJiYm0NLSwt69e6Guro7+/fsDqDUbXDdtW7duBVD7FcjPz48Vx7lz53Dr1i2FvBWJRHB2dmZ9zZDJZDh9+nSjg6xXgY2NDUxNTVm6PHnyBDExMY3qUlZWptD3CwQCZsBfVVWFqqqqRmWcnZ2hpqbGenZ6ejru3r3LPPvflKOuri7s7OxgYWGh0u/UvHnzsHfvXqX1Uo6rq6vCV6jIyMgXyisAzBK4GzdusNrL0KFD8eGHHyIxMRGtW7eGSCRCt27dkJ6ezoonIyOD2TsH1JbdgAEDIBKJcPjwYaVfhFXhhx9+wMqVK5Gbm9usfqU+rq6uOH/+PKqqqhi3yMhISKXSJr+U8ng8mJubQyKRYM+ePWjdujW6du2qchp69uyJ3Nxc1nECGRkZ4PP5sLS0ZMmGh4ejoqJCpS/JqvRxAwYMgKGhYYOm1+V5Nnr0aJSUlLBM5SuTq8+rareq1jM5hoaG0NPTw5kzZ/Dw4UMMHToUQG3bFovFrHiqqqqQk5OjEE9ycjK6dOnSoM6vhRcyTfEW8yZZ1cu4mUiOp8+Q4+kzlHEz8bkHZ1WPg4MuXbqkYGq8vtWtluZtN0eelZVFpqamjNngjIwMSklJoTVr1pCjoyMjJ5PJyM7OjvT19VnuctatW0eHDx+mjIwMysjIoK1bt5K2tjZjEY2o1uKcUCikgoICxq1Tp040d+5chfiKiopIJBLRkSNHGD3V1dXp66+/puvXr1NaWhqtWrWKhEIhHT9+nAn3+PFjcnR0JEtLS9q+fTvdvHmTMjIyKCgoiOzs7BirU2vXrqW+ffsy4eRmrAcMGECJiYl04sQJMjIyYpmxjomJIalUSvfu3WPc1q5dS3FxcZSenk7r1q0jiUTCMtVbn8Ys3Y0dO5ZcXFyUhgsLCyOxWEwhISGUkpJCU6ZMIT09PZb1uBexqnfz5k1KSEigIUOGkLu7OyUkJFBCQgLjf+/ePZJKpRQTE8O4BQYGkp6eHh06dIgxLVzfrHHfvn1p7dq1zL2vry9ZWFgwZo0PHDhAhoaGNGfOHEamT58+1KFDB4qKiqLbt29TcHAwqaur04YNGxiZqVOnUps2bejMmTMUGxtLrq6u5OrqyvirUo5EqlnVq4syq3pEilbz6qf70qVLJBQKaeXKlZSamkr+/v4K5sjnzZtH48aNYz1LKBTShg0bKCsriy5evEjvv/9+o6bPlVmKO3DgAKmpqdHmzZspMzOT1q5dSwKBgLGQWVxcTC4uLuTk5ES3bt1imdmurq5m5U1TVvWI2JYfVe1X6qe9qKiITExMaNy4cZScnExhYWEKxxocOHBAwcre8uXLKSkpiZKTk2nx4sWkpqZGERERLJmm6vrTp0/J0tKSRowYQTdv3qRz586Rvb09ffHFFwpp7dWrF3l7eyu4Eyla1VO1jzt48CCpqakx5sizs7Pp2rVrNHv2bNaz5syZQwKBgGbPnk2XL1+mnJwcOnXqFI0YMYKxtvc6221T9YyIaNu2bRQdHU23bt2inTt3koGBAc2cOZOVH9OnTycLCws6efIkpaWl0aRJk8jY2Jj1e5Gdna1gcbQ5cObIX5A3aeLUIO/YxKm6uppiY2MpNjaW1SFzcDTE1KlTFSZN//zzT0urpcDbPnEiIsrNzSU/Pz+ysrIikUhEFhYWNHToUIqKimLJLV26lADQ8uXLFeL47bffqEOHDqShoUE6OjrUpUsX2rBhA8tUN1Ht4GrTpk1ERBQbG0sA6OrVq0r1GjRoEOvsmqtXr1L//v3JyMiIdHV1ycXFRWFwRFQ7IJk3bx7Z29uTSCQiExMT8vDwoIiICMY0vL+/P1lZWbHC5eTk0KBBg0gikZChoSF99913LLPM8olI3QH3uHHjyMDAgEQiEXXs2JF27NihNC3146g/mSkqKiKJREKbN29uMOzatWupTZs2JBKJqHv37nTlyhWW/4tMnOqb9JdfcrKzswkAqy7IZDJauHAhmZiYkFgspn79+lF6erpCvHXPoXry5AlNnz6d2rRpQ+rq6mRra0sLFixgmZ3Oy8uj8ePHk7m5Oamrq5NUKqVVq1axzPmXl5fT//73P9LX1ycNDQ0aPnw45eXlsZ7dVDkSvbyJU3R0NAFgBqj1001Ua+LbwcGBRCIRdejQgY4ePcry9/X1VTBf/dtvv1H79u1JIpGQmZkZjRkzhjVhr09DJrblLwzU1dWpU6dOdPDgQcZPXj+UXfXzRpWJ0549e0gsFtPdu3eJSLV+RVnar1+/Tr169SKxWEwWFhYUGBjI8g8ODmbVUSKiDz/8kHR1dUldXZ1cXFzo2LFjCvo1VdeJas3be3h4kEQiIUtLS5o5c6bC0QhpaWkEgP766y+FZ8j1k6epuX3ctWvX6JNPPiEjIyMSi8VkZ2dHU6ZMUTjeYO/evdS7d2/S1tYmTU1N6tixIy1evJhp66+z3RI1Xs+Iak3Km5iYkJqaGtnb2yu0ayKiyspK+u6778jY2Ji0tbXJw8ODkpOTWTJLly4lT09PpXmpCi9r4sQjasZOvneAJ0+eQFdXlzG7+UZSWgrI9w+UlABNmLp806msrMTSpbXml7///nvW+n0ODmWEhYUxe2PU1NTeWGuMz549Q3Z2NmxsbF54mct/iaNHj2L27NlITk7mlu2+ZAICApCTk4OQkJCWVuWNh8fjITs7m2WSnkM57u7uGD9+PMaPH9/SqrwVhISEICQkhDEFzvFyqKyshL29PXbv3o2ePXu+UByN/V43Z27A7dTneOXweDxmXwqPx2thbTjeZMrKyqChoYFRo0Zhw4YNKC4uxvXr11taLY6XxODBg5GZmYn79++jdevWLa0OBwcHB8dbwN27d/H999+/8KTpZcJNnFqQ7FtpmJEeCwD4Vfo+bOyaPvfgbURNTY35esDBoYwzZ86gX79+AJ5bHjt//nxLqsTxipgxY0ZLq8DBwcHB8RZhZ2fHWHJsabiJUwtSWV6KGI33mP85OP6LDB8+nHXSeO/evblJEwdHM3F3d2/UYhnHc/z9/V+aifZ3nfHjxzdqspqDTefOnbllje843B6nFuRhfi5WnAwDAMz2HAVjU/Naj3dsjxMHR0Po6uriyZMnzL26ujrKy8tbUKPmwe1x4uDg4ODgePPh9ji9AxibmmOF78yWVuOVU1lZifXr1wMA/Pz8OOMQHCgrK4NmvRcCbm5uuHTpUgtpxMHBwcHBwcHROJxZI47XQnFxMYqLi1taDY43hPqTppCQEG7SxMHBwcHBwfFGw31xakFKnz7BoUO7AABeXmOhqf2Gmkf/lwiFQkyePJn5n4ODx+MxRiBKS0uhoaHRwhpxcHBwcHBwcDQO98WpBbmXk4mZFm6YaeGGezmZLa3OK4PP58PCwgIWFhbc2S3/YY4fP878L5PJ8N5774GIuEkTBwcHBwcHx1sBN4ptYTSoFBrEWdTjeHcpKysDj8fDRx99hFmzZjHuN27caEGtODg4ODg4ODiaBzdxakGkTs643bcnbvftCamTc0ur88qoqalBUlISkpKSUFNT09LqcLxGtm/fztrPtGrVqhbUhoODg4ODg4PjxeEmThyvnJqaGhw4cAAHDhzgJk7/IXr27KlwnkVpKfd19b/M48ePYWxsjJycnJZW5Z0jJCQE7u7uLa3GW4G1tTXOnj3b0mq8FYwfPx4BAQEtrcZbw9mzZ2Ftbd3SarxzPHr0CMbGxrh3715Lq8JNnDhePTweD7a2trC1tQWPx2tpdTheAxKJBJcvX2budXR0uP1Mbyj5+fn4+uuvYWtrC7FYjNatW2PIkCE4efIkDA0NERgYqDTcTz/9BBMTE1RVVbHcL126BKFQqPTQzCVLlsDLy0vpwMLT0xMCgQDXrl1T8HN3d8eMGTMU3ENCQhQOMn3y5AkWLFgAR0dHqKurw9TUFB4eHjhw4AAaO7bw7t27GDx4MDQ0NGBsbIzZs2ejurq6QXkAiI+PR//+/aGnp4dWrVphypQpKCkpYfwfP36MgQMHwtzcnMnbr776inV2GQCsX78e7dq1g0QigVQqxY4dOxp8ZlhYGHg8HoYNG9aobjk5OeDxeArX2LFjGZlvvvkGzs7OEIvFzTrkdP369bC2toa6ujpcXFxw9erVJsP8+uuvkEqlkEgkaN26Nb799ls8e/aM8a+pqcHChQthY2MDiUSCtm3b4qeffmKVGRHhxx9/hJmZGSQSCTw8PJCZyd4fXFBQgDFjxkBHRwd6enqYNGkSq0yUMX78eCZ/RCIR7OzssHjxYqb8z549y8pDIyMjfPTRRyotNz579iy6du0KsVgMOzs7hISENBnm5MmT6NGjB7S1tWFkZIRPP/20wZcNjbW3+/fvY+zYsWjVqhUkEgmcnJwQGxsLAKiqqsLcuXPh5OQETU1NmJub4/PPP0dubm6juoWEhDD5wOfzYWZmBm9vb9y9e5cl9ya1Q0C1uk5EWLlyJRwcHCAWi2FhYYElS5Yw/nXrSd2rQ4cOjeon58svv4RAIEB4eLhS/1u3bmHChAmwtLSEWCyGjY0NfHx8mDKTExUVhY8++gitWrWChoYG2rdvj++++w73799v9Pmvot0CjdczAHjw4AHGjx8Pc3NzaGhoYODAgQrtFgCio6PRt29faGpqQkdHB71792bOdTQ0NMTnn38Of3//JnV+5dB/jOLiYgJAxcXFLa0K3c3OoM8OB9Nnh4PpbnbGc4+SEiKg9iopaTkFOTheAKFQSACYa9iwYS2t0iujvLycUlJSqLy8vKVVeSGys7PJ3Nyc2rdvT/v376f09HRKTk6mVatWkVQqpenTp5ODg4NCOJlMRra2tjRr1iyWe2FhIdna2tKAAQOoU6dOLL/S0lLS0dGh6Ohohfju3LlDWlpa9M0339DUqVMV/Pv06UPTp09XcA8ODiZdXV3W8zt06ECWlpYUEhJCN2/epPT0dNq8eTO1bduWCgsLleZDdXU1vffee+Th4UEJCQl07NgxMjQ0pPnz5yuVJyK6f/8+6evr09SpUyktLY2uXr1Kbm5u9OmnnzIyBQUFtGHDBrp27Rrl5OTQqVOnSCqVko+PDyOzYcMG0tbWprCwMMrKyqI9e/aQlpYWHT58WOGZ2dnZZGFhQR988AF5eXkp5EWfPn1YsgDo1KlTlJeXx1xFRUWMzNdff03r1q2jcePGKZRXQ4SFhZFIJKJt27bRzZs3afLkyaSnp0cPHjxoMExoaCiJxWIKDQ2l7OxsOnnyJJmZmdG3337LyCxZsoRatWpFR44coezsbAoPDyctLS1as2YNIxMYGEi6urp08OBBun79Og0dOpRsbGxY7W/gwIHUqVMnunLlCl24cIHs7OxY+U1EZGVlRVFRUcy9r68vDRw4kPLy8ignJ4c2bNhAPB6Pli5dSkREUVFRBIDS09MpLy+P4uLiqG/fvtS6dWuqqKhoMN23b98mDQ0NmjlzJqWkpNDatWtJIBDQiRMnGg0jFotp/vz5dOvWLYqLi6PevXtTly5dFGQba28FBQVkZWVF48ePp5iYGLp9+zadPHmSbt26RURERUVF5OHhQXv37qW0tDSKjo6m7t27k7OzMyseX19f8vf3Z+6Dg4NJR0eH8vLyKDc3ly5dukSdOnWi7t27s/R6k9ohkWp1/euvvyapVEqHDh2i27dvU2xsLP3111+Mf1FREast/f3332RgYMDKn6ioKLKyslKIW97/zZs3jwYOHKjgf+3aNdLR0SE3Nzc6cuQI3bp1ixISEiggIIB69+7NyG3atIn4fD5NmDCBoqKiKDs7m86dO0eTJk1itaf6vKp221Q9k8lk1KNHD/rggw/o6tWrlJaWRlOmTKE2bdpQSZ3x7eXLl0lHR4eWLVtGycnJlJaWRnv37qVnz54xMsnJySQWi+nx48cN6twYjf1eN2duwE2cWpC0pFgyOZNAJmcSKC0p9rkHN3HieIuZO3cuM2k6ffp0S6vzSmmsI64pLaWa0lKSyWSMm6yiota93mCLka2peS5bWVnrXueHozHZF2HQoEFkYWHB+gGTU1hYSElJSQSALly4wPKTDyRTU1NZ7t7e3vTDDz+Qv7+/wuAkPDycjIyMlOoREBBAo0aNotTUVNLV1aWysjKWv6oTp2nTppGmpibdv39fQfbp06dUVVWl9PnHjh0jPp9P+fn5jNvGjRtJR0enwYHx77//TsbGxlRTpxzk+ZWZmak0DBHRmjVryNLSkrl3dXVVmIDOnDmTevbsyXKrrq4mNzc32rp1K/n6+qo8cUpISGhQFznKyqshunfvTn5+fsx9TU0NmZub07JlyxoM4+fnR3379mW51U/j4MGDaeLEiSyZTz75hMaMGUNEtQMwU1NTWrFiBeNfVFREYrGY9uzZQ0REKSkpBICuXbvGyBw/fpx4PB6rTiibONXPz/79+1OPHj2I6Hl9rzvgP3z4MAGg69evN5juOXPmUIcOHVhu3t7e5Onp2WCY8PBwEgqFrHp1+PBh4vF4VFmvnTfW3ubOnUu9evVq8DnKuHr1KgGgO3fuMG7KJk512xwR0W+//cYaV73J7bChup6SkkJCoZDS0tKUPkcZERERxOPxKCcnh3FraOIUEhJCPXr0oKKiItLQ0KC7d+8yfjKZjDp06EDOzs6sdMiR17u///6bRCIRzZgxQ6k+DU1IiV5du22qnqWnpxMASk5OZj3byMiItmzZwri5uLjQDz/80GA8cmxsbGjr1q1NyinjZU2cuKV6LYi2viG8c8/BO/cctPUNW1odDo4XZty4ccz/gYGBmDt3LogIffv2bUGtWpb0rs5I7+qMmsJCxu3xtm1I7+qMBz/9xJLN6NkL6V2dUZWbx7gV7t6N9K7OyFvwA0v2Vj8PpHd1RmVWFuNWFBHRbP0KCgpw4sQJ+Pn5KRxIDAB6enpwcnJCt27dsG3bNpZfcHAw3Nzc4OjoyHK7fft2g0spLly4AGdnRSM4RITg4GCMHTsWjo6OsLOzw/79+5udHplMhrCwMIwZMwbm5uYK/lpaWsw5cgEBAazlgtHR0XBycoKJiQnj5unpiSdPnuDmzZtKn1dRUQGRSMQ6YkEikQAALl68qDRMbm4uDhw4gD59+rDiUVdXZ8lJJBJcvXqVtQxy8eLFMDY2xqRJkxrKglcCj8djlpdVVlYiLi4OHh4ejD+fz4eHhweio6MbjMPNzQ1xcXHM0qDbt2/j2LFj+Oijj1gyp0+fRkZGBgDg+vXruHjxIgYNGgQAyM7ORn5+PuvZurq6cHFxYZ4dHR0NPT09vP/++4yMh4cH+Hw+YmJimpVuiUSCyspKpX7FxcUICwsDAIhEIsbd3d2dta8zOjqapS9QW68ayytnZ2fw+XwEBwejpqYGxcXF2LlzJzw8PKCmpsbINdXeDh8+jPfffx+fffYZjI2N0aVLF2zZsqXRNBcXF4PH4yksf22Mhw8fIiIiAgKBAAKB4K1oh8r4888/YWtriyNHjsDGxgbW1tb44osvUFBQ0GCYoKAgeHh4wMrKqsn4g4KCMHbsWOjq6mLQoEGsJZuJiYm4efMmvvvuO6VHtsjLIzw8HJWVlZgzZ47SZ9Qtt9fVbpuqZxUVFQDA6uP4fD7EYjFTPg8fPkRMTAyMjY3h5uYGExMT9OnTR2n5de/eHRcuXGhQ59cBN3FqQcwtrbBmzHSsGTMd5pZNN7y3lcrKSqxfvx7r169v8IeI4+1FLBZj165drP1rDe2L4XhzuHXrFoiINflRxqRJkxAeHs7sGXj69Cn279+PiRMnMjKZmZmYN28edu3a1eAh13fu3FE6kDp16hTKysrg6ekJABg7diyCgoKanZ5Hjx6hsLCwyfQAtevl27Zty9zn5+ezBmsAmPv8/HylcfTt2xf5+flYsWIFKisrUVhYiHnz5gEA8vLyWLI+Pj7Q0NCAhYUFdHR0sHXrVsbP09MTW7duRVxcHIgIsbGx2Lp1K6qqqvDo0SMAtQPAoKCgJge+ynBzc4OWlhZzJSQkNCu8VCqFrq4ugNo8rqmpUZpXDeUTAIwePRqLFy9Gr169oKamhrZt28Ld3R3ff/89IzNv3jyMGjUKjo6OUFNTQ5cuXTBjxgyMGTMGwPNyaOzZ+fn5MDY2ZvkLhUIYGBg0ql9diAinTp3CyZMnFV78WFpaQktLC3p6eti9ezeGDh3Kqm9t2rSBmZkZc99QvXry5Amzd6M+NjY2+Ouvv/D9999DLBZDT08P9+7dw759+xgZVdrb7du3sXHjRtjb2+PkyZOYNm0avvnmG2zfvl2p/LNnzzB37lz4+PhAR0en0TwqLi6GlpYWNDU1YWJigqioKOYFzJvcDhvj9u3buHPnDsLDw7Fjxw6EhIQgLi4OI0aMUCqfm5uL48eP44svvmgy7szMTFy5cgXe3t4Aavu44OBgZq+XfL9PU3mWmZkJHR0dVh1riNfVbpuqZ46OjmjTpg3mz5+PwsJCVFZW4ueff8a9e/eY8rl9+zaA2on05MmTceLECXTt2hX9+vVT2Atlbm6OO3fuNJn+Vwk3ceJ4Lfzzzz/4559/WloNjpfIo0ePwOPxWJPhsrKyFtTozUIaHwdpfBwE+vqMW6uJEyGNj4PJwoUsWYdLFyGNj4Oa+fMfRP3RoyGNj4PZkv9jydqdPgVpfBxEdQYcesOHN1s/amSDdl18fHxQU1PDDNz27t0LPp/PDAJqamowevRoLFq0CA4ODg3GU15ervBlBQC2bdsGb29vZgDo4+ODS5cuIavOF7WXmR4A+Oqrr3D69OlmxV+fDh06YPv27Vi1ahU0NDRgamoKGxsbmJiYKLw1Xr16NeLj43Ho0CFkZWVh5syZjN/ChQsxaNAg9OjRA2pqavDy8oKvry+A2jezT58+xbhx47BlyxYYGjZ/ZcLevXuRmJjIXO3bt29W+LS0NAx/gfpVl7Nnz2Lp0qXYsGED4uPjceDAARw9ehQ/1fnyum/fPoSGhmL37t2Ij4/H9u3bsXLlygYH+i+bI0eOQEtLC+rq6hg0aBC8vb0VrMlduHABcXFxCAkJgYODAzZt2sTy37FjB5YtW/av9MjPz8fkyZPh6+uLa9eu4dy5cxCJRBgxYgSISOX2JpPJ0LVrVyxduhRdunTBlClTMHnyZAWdgVpDESNHjgQRYePGjU3qqK2tjcTERMTGxmLVqlXo2rUrY0ThTW6HjSGTyVBRUYEdO3bggw8+gLu7O4KCghAVFYX09HQF+e3bt0NPT69JIy1AbR/n6enJtN+PPvoIxcXFOHPmDADV84yIVDaw9brabVP1TE1NDQcOHEBGRgYMDAygoaGBqKgoDBo0iCkfmUwGoNZ4xoQJE9ClSxesXr0aUqlUYbWDRCJp8XGG8lcVHK+F0qdPcDbyTwCAe/8h0NRu/C3P24pQKGQGAg29HeN4u/jpp5/w448/stzi4uI4q3l14CvJC55IBF6dpT2NyqqpgVdnaU5Tss3F3t4ePB4PaWlpjcrp6OhgxIgRCA4OxsSJExEcHIyRI0dCS0sLQO0XqNjYWCQkJOCrr74CUPtDSEQQCoX466+/0LdvXxgaGqKwzrJFoHa5YEREBKqqqlgDtpqaGmzbto0ZjOno6KC4uFhBt6KiIuatqpGREfT09JpMjzJMTU0VLEw9ePCA8WuI0aNHY/To0Xjw4AE0NTXB4/Hwyy+/wNbWViF+U1NTODo6wsDAAB988AEWLlzIWIfbtm0bfv/9dzx48ABmZmbYvHkzY1EtKSkJOTk5GDJkCBOffKAhFAqRnp7Oemtfn9atW8POzq7ZeaIMQ0NDCAQCJm/kPHjwoNF8WrhwIcaNG8e8nXdyckJpaSmmTJmCBQsWgM/nY/bs2cxXJ7nMnTt3sGzZMvj6+jLxy/Oo7rPlVtJMTU3x8OFD1rOrq6tRUFDQqH4A8OGHH2Ljxo0QiUQwNzdX+ltlY2MDPT09SKVSPHz4EN7e3jh//nyDcZqamirNKx0dHWY5WX3Wr18PXV1dLF++nHHbtWsXWrdujZiYGDg6OqrU3szMzBQmye3atcMff/zBcpNPmu7cuYMzZ840+bUJqJ3Qy+tUu3btkJWVhWnTpmHnzp1vdDtsDDMzMwiFQtZktF27dgBqLf1JpVLGnYiwbds2jBs3jrVUUxk1NTXYvn078vPzWXVK3sf169ePeWZaWhq6dOnSYFwODg4oLi5GXl6eSl+d5LzKdqtKPXN2dkZiYiKKi4tRWVkJIyMjuLi4MEtq5WlRFk99a40FBQUwMjJSOe2vAu6LUwtyLycTk/Q7YJJ+B9zLUTTN+K7A5/NhY2MDGxubZr0B4ngzcXR0ZE2aeDweiAhdu3ZtQa04mouBgQE8PT2xfv16pedrFRUVMf9PmjQJFy9exJEjR3D58mXWPhsdHR3cuHGD9VVj6tSpkEqlSExMhIuLCwCgS5cuSElJYT0jNDQUlpaWuH79Oiv8qlWrEBISwpz7JpVKER8fr6BjfHw8M+jg8/kYNWoUQkNDlZpULikpadCssaurK27cuMEadEdGRkJHR0elLzQmJibQ0tLC3r17oa6ujv79+zcoK5/0yNf+y1FTU4OlpSUEAgHCwsLw8ccfg8/nw9HRUSF/hw4dig8//BCJiYlo3bp1k/q9LEQiEZydnVlfCWQyGU6fPg1XV9cGw5WVlSn0/QKBAMDzt+0Nycjzy8bGBqampqxnP3nyBDExMcyzXV1dUVRUhLi4OEbmzJkzkMlkTD1sCE1NTdjZ2aFNmzYqveDz8/NDcnIyIhrZX+jq6qrwRSUyMvKF80omk6nc3nr27KnwpSQjI4O1H0c+acrMzMSpU6fQqlWrJtOtjHnz5mHv3r2Ij49/a9phfXr27Inq6mrWl275frv6e5jOnTuHW7duqbTf8NixY3j69CkSEhJYZbZnzx4cOHAARUVF6Ny5M9q3b49Vq1Yx9b0u8r54xIgREIlErEm1Mrn6vMp2q0o9k6OrqwsjIyNkZmYiNjYWXl5eAGrPVTM3N1cpnuTk5EYnl6+FFzJN8RbzplnVMz99jcxPX+Os6nG8FXz33XcsU+NmZmYtrVKL8rabI8/KyiJTU1PGHHlGRgalpKTQmjVryNHRkZGTyWRkZ2dH+vr6LPeGUGa5KikpiYRCIRUUFDBunTp1orlz5yqELyoqIpFIREeOHGH0VFdXp6+//pquX79OaWlptGrVKhIKhXT8+HEm3OPHj8nR0ZEsLS1p+/btdPPmTcrIyKCgoCCys7NjrE6tXbuWZS1KbgZ5wIABlJiYSCdOnCAjIyOWGeSYmBiSSqV07949xm3t2rUUFxdH6enptG7dOpJIJCzz2UePHqVt27bRjRs3KDs7m44cOULt2rVjWaVKT0+nnTt3UkZGBsXExJC3tzcZGBhQdnZ2g/n7sqzqZWZmUkJCAn355Zfk4OBACQkJlJCQwLJgJpVK6cCBA8x9WFgYicViCgkJoZSUFJoyZQrp6emxLKGNGzeO5s2bx9z7+/uTtrY27dmzh27fvk1//fUXtW3blkaOHMlKk4WFBWOO/MCBA2RoaEhz5sxhZAIDA0lPT48OHTpESUlJ5OXlpdQceZcuXSgmJoYuXrxI9vb2Kpkjr5+fdVFmVY+o1mqek5MTYzmzfrrl5shnz55NqamptH79egVz5PXr4unTp4nH49GiRYsoIyOD4uLiyNPTk6ysrBSsTdbN3/rt7erVqyQUCmnJkiWUmZlJoaGhpKGhQbt27SIiosrKSho6dChZWlpSYmIiy8x23fJXxaoeEdHIkSNp8ODBRPTmtUOiput6TU0Nde3alXr37k3x8fEUGxtLLi4u1L9/f4W0jh07llxcXJSWRX2rel5eXuTt7a0gV1NTQ6amprRu3TomXdra2uTm5kZHjx6lrKwsun79Ov3f//0fyxz5+vXricfj0cSJE+ns2bOUk5NDFy9epClTptDMmTMZudfVbpuqZ0RE+/bto6ioKMrKyqKDBw+SlZUVffLJJ6z8WL16Neno6FB4eDhlZmbSDz/8QOrq6oxZc6Jak+4SiYTOnz+vNO+bgjNH/oK8SROnBnnHJk7V1dWUkpJCKSkpVF1d3dLqcPxL5JMmZeft/Nd42ydORES5ubnk5+dHVlZWJBKJyMLCgoYOHcoaWBIRLV26lADQ8uXLm4yzIZO/3bt3p02bNhERUWxsLAGgq1evKo1j0KBBNHz4cOb+6tWr1L9/fzIyMiJdXV1ycXGhiIgIhXBFRUU0b948sre3J5FIRCYmJuTh4UERERHMANff31/BZHBOTg4NGjSIJBIJGRoa0nfffccymywfPNed0IwbN44MDAxIJBJRx44daceOHaw4z5w5Q66urqSrq0vq6upkb29Pc+fOZQ3AU1JSqHPnziSRSEhHR4e8vLyaNIn8siZOffr0Yb0IkV910wiAgoODWeHWrl1Lbdq0IZFIRN27d6crV64oxOvr68vcV1VVUUBAALVt25bU1dWpdevW9L///Y+VD0+ePKHp06dTmzZtSF1dnWxtbWnBggWsQbxMJqOFCxeSiYkJicVi6tevH6Wnp7Oe/fjxY/Lx8SEtLS3S0dGhCRMm0NOnT1kyL2vidPfuXRIKhbR3716l6ZaH7dy5M4lEIrK1tVXIS2V1cc+ePdSlSxfS1NQkIyMjGjp0qILp//pxKGtvf/75J7333nskFovJ0dGRNm/ezPjJ64eyq37eqDJxio6OJgAUExNDRG9WOyRSra7fv3+fPvnkE9LS0iITExMaP368wplBRUVFJJFIWHlZl7oTp/z8fBIKhbRv3z6lstOmTWOdz5Wenk6ff/45mZubk0gkIisrK/Lx8aH4+HhWuMjISPL09CR9fX1SV1cnR0dHmjVrFuXm5jIyr6vdEjVez4ieH8GgpqZGbdq0oR9++EGpeflly5aRpaUlaWhokKurq8IxGLt37yapVKo0L1XhZU2ceETN2Mn3DvDkyRPo6uqiuLhYpbW8LUJpKfD/9w+gpARQYir4baKyshJLly4FAHz//fdNrgnmeLOYN28eYmNjcerUKQC1n+/v3r2rktWkd51nz54hOzsbNjY2Sg0fcLA5evQoZs+ejeTkZG7Z7ksmJCQEISEhOHv2bEur8sZjbW2NkJAQuLu7t7Qqbzzjx4+HtbW1gqEMDuWcPXsW48ePR05OTkur8s7Ro0cPfPPNNxg9evQLhW/s97o5cwNupz7HK4fH4zHr8FW1CMPxZmBtbc2Y/ty+fTt8fX2hoaHBTZo4XojBgwcjMzMT9+/ff617czg4ODg43l4ePXqETz75BD4+Pi2tCjdxakn+zsnE9wnnAABLu/RBa2v7Ftbo1aCmpvbaD23k+PfU3ZgN1Fp7kltH5OB4UWbMmNHSKnBwcHBwvEUYGho2ePDv64ZbK9GClD19gki99xGp9z7Knj5paXU4OADUmkTl8XisSdPcuXMVzMRycHC8OXTu3Bnjx49vaTXeCmbMmAFra+uWVuOtYNiwYdySxmZgbW3NvRx6x+H2OLUguffu4KdLRwAAC3t+DHPL/2928R3b48Tx9jBt2jSFAxLv3LmDNm3atJBGbzbcHicODg4ODo43H26P0zuAuaUVNnr7tbQar5yqqioEBwcDACZMmAC1Fzisk+P1UHfSJBQKUVVV1YLacHBwcHBwcHC8OXBL9TheOUSE3Nxc5Obm4j/2gfOtY9iwYQBqDxzlJk0cHBwcHBwcHM/hvji1IM/Ky5EUHw0A6NjVFeoSSQtr9GoQCASM+Uj5qdMcbwaXL1/Ghx9+iIqKCgBAREREC2vEwcHBwcHBwfFmwn1xakHu3ErB0EoDDK00wJ1bKS2tzitDIBDAwcEBDg4O3MTpDWLcuHHo2bMnKisrueWTHBwcHBwcHBxNwH1x4uD4D9KqVSsUFBQw99xhpBwcHBwcHBwcjcONlloQK7v2SGpnjKR2xrCya9/S6rwyZDIZsrKykJWVxTJxzdEy8Hg81qSpc+fOzFI9Do5XyePHj2FsbIycnJyWVuWd4+zZs5yJbRVxd3dHSEhIS6vxVhAQEMCZuW8GISEhnPn2V8CmTZswZMiQllYDADdxalHUJRIYm5rD2NT8nd3fBADV1dXYuXMndu7cierq6pZW5z/LmTNnwOPxWG7r1q1DQkJCC2nE8SaQn5+Pr7/+Gra2thCLxWjdujWGDBmCkydPwtDQEIGBgUrD/fTTTzAxMUFVVRXOnj0LHo+ncOXn57PCLFmyBF5eXkoH+J6enhAIBLh27ZqCn7u7u9KzUUJCQqCnp8dye/LkCRYsWABHR0eoq6vD1NQUHh4eOHDgQKPGae7evYvBgwdDQ0MDxsbGmD17dpP9VXx8PPr37w89PT20atUKU6ZMQUlJiVI9O3bsCHV1dRgbG8PP77k11bNnz8LLywtmZmbQ1NRE586dERoaygp/8+ZNfPrpp7C2tgaPx8Ovv/7aqF5ylJVJr169GP8lS5bAzc0NGhoaCvnYGOHh4Uz+Ojk54dixY02GWb9+Pdq1aweJRAKpVIodO3aw/Lds2YIPPvgA+vr60NfXh4eHh8LZcQEBAXB0dISmpiYjExMTw5J5kTQFBAQw+SMUCmFtbY1vv/2WKcucnBxWHhoYGKBPnz64cOFCk3EnJSXhgw8+gLq6Olq3bo3ly5c3GebatWvo168f9PT0oK+vD09PT1y/fp3xr6+P/Lpy5Qoj4+7urlRm8ODBjMyBAwcwYMAAtGrVCjweD4mJiU3qVr+tGxkZ4aOPPsKNGzdYcpWVlVi+fDk6deoEDQ0NGBoaomfPnggODm7U8NCLlB8R4ccff4SZmRkkEgk8PDyQmZnJkikoKMCYMWOgo6MDPT09TJo0SaGtqlJWL1L35TTWxwFAQkICPvvsM5iYmEBdXR329vaYPHkyMjIyWHJ//PEH3N3doaurCy0tLXTs2BGLFy9mvRCtjyp5VJ+nT59ixowZsLKygkQigZubm4LuqtShL7/8Em3btoVEIoGRkRG8vLyQlpbGkmmq/504cSLi4+NVanOvGm7ixPHK4fF4MDExgYmJicLAneP1MWjQINZ9aWkpawDH8d8jJycHzs7OOHPmDFasWIEbN27gxIkT+PDDDzF9+nSMHTuWOUqgLkSEkJAQfP7556z9cenp6cjLy2MuY2Njxq+srAxBQUGYNGmSQnx3797F5cuX8dVXX2Hbtm0vnJ6ioiK4ublhx44dmD9/PuLj43H+/Hl4e3tjzpw5KC4uVhqupqYGgwcPRmVlJS5fvozt27cjJCQEP/74Y4PPys3NhYeHB+zs7BATE4MTJ07g5s2bCm/nf/nlFyxYsADz5s3DzZs3cerUKXh6ejL+ly9fRseOHfHHH38gKSkJEyZMwOeff44jR44wMmVlZbC1tUVgYCBMTU2blSfBwcGsMjl8+DDjV1lZic8++wzTpk1TOb7Lly/Dx8cHkyZNQkJCAoYNG4Zhw4YhOTm5wTAbN27E/PnzERAQgJs3b2LRokXw8/PDn3/+ycicPXsWPj4+iIqKQnR0NFq3bo0BAwbg/v37jIyDgwPWrVuHGzdu4OLFi7C2tsaAAQPwzz///Ks0AUCHDh2Ql5eHnJwc/Pzzz9i8eTO+++47lsypU6eQl5eH8+fPw9zcHB9//DEePHjQYJxPnjzBgAEDYGVlhbi4OKxYsQIBAQHYvHlzg2FKSkowcOBAtGnTBjExMbh48SK0tbXh6empMOGQ6yO/nJ2dGb8DBw6w/JKTkyEQCPDZZ58xMqWlpejVqxd+/vnnZuUV8Lytnzx5EhUVFUz7AWrLwNPTE4GBgZgyZQouX76Mq1evws/PD2vXrsXNmzcbjPdFym/58uX47bffsGnTJsTExEBTUxOenp549uwZIzNmzBjcvHkTkZGROHLkCM6fP48pU6Yw/qqU1YvUfTlN9XFHjhxBjx49UFFRgdDQUKSmpmLXrl3Q1dXFwoULGbkFCxbA29sb3bp1w/Hjx5GcnIxVq1bh+vXr2Llz57/Ko/p88cUXiIyMxM6dO3Hjxg0MGDAAHh4erDapSh1ydnZGcHAwUlNTcfLkSRARBgwYgJqaGgCq9b8ikQijR4/Gb7/91uBzXhv0H6O4uJgAUHFxcUurQvf/zqGJ+zfSxP0b6f7fOc89SkqIgNqrpKTlFOR45wBA6urqLa3GO0N5eTmlpKRQeXm5gl/ls2qqfFZNMpmMcauuqqHKZ9VUXVmjXLamjmx1rWxVZbVKsi/CoEGDyMLCgkqU9DOFhYWUlJREAOjChQssv6ioKAJAqamprPvCwsIGnxUeHk5GRkZK/QICAmjUqFGUmppKurq6VFZWxvLv06cPYFthXgAA5OFJREFUTZ8+XSFccHAw6erqMvfTpk0jTU1Nun//voLs06dPqaqqSunzjx07Rnw+n/Lz8xm3jRs3ko6ODlVUVCgN8/vvv5OxsTHV1DzPe3l+ZWZmEhFRQUEBSSQSOnXqlNI4GuKjjz6iCRMmKPWzsrKi1atXK7hHRUWRlZUVyw0ARURENPm8+vnYGCNHjqTBgwez3FxcXOjLL79sMIyrqyvNmjWL5TZz5kzq2bNng2Gqq6tJW1ubtm/f3qCM/PdcWf42lqY+ffpQcHAwc+/v70+dOnViyUyePJlMTU2JiCg7O5sAUEJCAuMvL+tDhw41qN+GDRtIX1+fVYfmzp1LUqm0wTDXrl0jAHT37l2FZ8nrlTJ9mmL16tWkra2ttK03Fp+/vz/5+voy98ra+uHDhwkAXb9+nYiIfv75Z+Lz+RQfH68QX2VlpVId6qNqnZTJZGRqakorVqxg3IqKikgsFtOePXuIiCglJYUA0LVr1xiZ48ePE4/HY/oKVcpKlbofHBxMffr0UdCzsT6utLSUDA0NadiwYUrTKM/rmJgYAkC//vpro3L1USWP6lNWVkYCgYCOHDnCcu/atSstWLBAQb45dfL69esEgG7dukVEqve/586dI5FIpPD7oCqN/V43Z27AfXFqQZ4WPsJRgx44atADTwsftbQ6HO8YZWVl4PF4GDduHONGRCgvL29Brf47bJ5+Dpunn8OzkudviRP+uovN08/hfFg6S3bb7AvYPP0cnhY8f/uXfPY+Nk8/hzM72Esadiy4jM3Tz6Egv5RxS7uc12z9CgoKcOLECfj5+UFTU1PBX09PD05OTujWrZvCG9Lg4GC4ubnB0dGR5d65c2eYmZmhf//+uHTpEsvvwoULrDficogIwcHBGDt2LBwdHWFnZ4f9+/c3Oz0ymQxhYWEYM2YMzM3NFfy1tLQgFNbaQwoICGAtF4yOjoaTkxNMTEwYN09PTzx58qTBt+MVFRUQiUQswyqS/7/k+uLFiwCAyMhIyGQy3L9/H+3atYOlpSVGjhyJv//+u9G0FBcXw8DAQLWEv2Ksra0REBDA3EdHR8PDw4Ml4+npiejo6AbjqKiogLq6OstNIpHg6tWrDS7bKisrQ1VVVYP5UFlZic2bN0NXVxedOnVSMTWqI5FImC8o9SkvL2eWGopEIsZ9/PjxrP0t0dHR6N27N0vG09MT6enpKCwsVBq3VCpFq1atEBQUhMrKSpSXlyMoKAjt2rVTWOI6dOhQGBsbo1evXqwvicoICgrCqFGjlLb1f0NxcTHCwsIAPM+L0NBQeHh4oEuXLgryampqjA4hISH/ehVKdnY28vPzWXVSV1cXLi4uTJ2Mjo6Gnp4e3n//fUbGw8MDfD6fWeqpSlm9SN0Hmu7jTp48iUePHmHOnDlKw8uXLIaGhkJLSwv/+9//GpWTL+U8e/asynlUn+rqatTU1Chtt/L+7UUoLS1FcHAwbGxs0Lp1awCq97/vv/8+qqurFZbnvm64iVMLoq1viMEFVzC44Aq09Q1bWh2Od4iIiAjmx2nXrl2Ij49vYY043jRu3boFIlKY/NRn0qRJCA8PZ/YDPH36FPv378fEiRMZGTMzM2zatAl//PEH/vjjD7Ru3Rru7u6senfnzh2lE5pTp06hrKyMWb42duxYBAUFNTs9jx49QmFhYZPpAQBDQ0O0bduWuc/Pz2f9aANg7uvv05LTt29f5OfnY8WKFaisrERhYSHmzZsHAMjLq53I3r59GzKZDEuXLsWvv/6K/fv3o6CgAP37929wUL5v3z5cu3YNEyZMaDrRKuDj4wMtLS3mOnjwYLPCt23bFoaGz3+fGsqrhvIJqB0Ebd26FXFxcSAixMbGYuvWraiqqsKjR8pfGs6dOxfm5uYKA9UjR45AS0sL6urqWL16NSIjI1n6vQzi4uKwe/du9O3bl+Xu5uYGLS0taGpqYuXKlXB2dka/fv0YfzMzM7Rp04a5f5F6pa2tjbNnz2LXrl2QSCTQ0tLCiRMncPz4cWbir6WlhVWrViE8PBxHjx5Fr169MGzYsAYnT1evXkVycjK++OKL5mdGA1haWkJLSwt6enrYvXs3hg4dyrS9zMxMldqhrq4upFLpv9JDno+N1cn8/HzWsmEAEAqFMDAwYMk0VVYvUveBpvs4+V6jpvIsMzMTtra2TR4foqamBqlUCg0NDZb+zdFdW1sbrq6u+Omnn5Cbm4uamhrs2rUL0dHRTP/WHDZs2MD0QcePH0dkZCQzSVW1nWhoaEBXVxd37txp9vNfJpw58hbE3NIKQZZTW1qNV05VVRV27doFoLbD4M4MerV4eHjg9OnTLDdVfsQ4Xi5T1vQBAAhFz99PdRnQBp36tQafz37LOnHFB7Wyas9l33O3QPte5uDVe731+RI3BVlHN7Nm60eNGEqoi4+PD7799lvs27cPEydOxN69e8Hn8+Ht7c3ISKVS1gDIzc0NWVlZWL16NbPuvry8XOHtJQBs27YN3t7ezKDQx8cHs2fPRlZWFmty87LSAwBfffUVvvrqK5XlldGhQwds374dM2fOxPz58yEQCPDNN9/AxMSE+Qolk8lQVVWF3377DQMGDAAA7NmzB6ampoiKimLtdQKAqKgoTJgwAVu2bEGHDh3+lX5yVq9ezZp8mJk1r67U70tehIULFyI/Px89evQAEcHExAS+vr5Yvny50qMQAgMDERYWhrNnzyrUmQ8//BCJiYl49OgRtmzZgpEjRyImJkZhYNxcbty4AS0tLdTU1KCyshKDBw/GunXrWDJ79+6Fo6MjkpOTMWfOHISEhLB+z5YtW/avdABq28mkSZPQs2dP7NmzBzU1NVi5ciUGDx6Ma9euQSKRwNDQEDNnzmTCdOvWDbm5uVixYgWGDh2qEGdQUBCcnJzQvXv3f62fnAsXLkBDQwNXrlzB0qVLsWnTJsZP1bY4fPhwDB8+/KXp9KbSVB+nan6pKmdhYaFgfOFF2LlzJyZOnAgLCwsIBAJ07doVPj4+iIuLa3ZcY8aMQf/+/ZGXl4eVK1di5MiRuHTpktLfhMaQSCQoKytr9vNfJtwXJ45XDhHhzp07uHPnTrMGNxzNR0tLizXQ0dTUBBExb544Xh9qYgHUxALWUhSBkA81sQACNb5y2ToTKoGgVlaoJlBJtrnY29uDx+M1+QOro6ODESNGMEYigoODMXLkSGhpaTUarnv37rh16xZzb2hoqLA8qaCgABEREdiwYQOEQiGEQiEsLCxQXV3NWh6oo6Oj1LBDUVERdHV1AQBGRkbQ09N7oQGDqampwiZ/+X1jxhhGjx6N/Px83L9/H48fP0ZAQAD++ecf2NraAng+SWnf/vlxE0ZGRjA0NMTdu3dZcZ07dw5DhgzB6tWr8fnnnzc7DQ1hamoKOzs75vq3S7UayqvG8kkikWDbtm0oKytDTk4O7t69C2tra2hra8PIyIglu3LlSgQGBuKvv/5Cx44dFeLS1NSEnZ0devTogaCgIAiFwhf6QlkfqVSKxMREpKamory8HIcPH1Z4C966dWvY29tj+PDhWLp0KYYPH97oUQ4vUq92796NnJwcBAcHo1u3bujRowd2796N7OxsHDp0qMFnubi4sNqbnNLSUoSFhSk1yvJvsLGxgVQqha+vL7744gvWixQHB4eXMnBXBXk+NlYnTU1N8fDhQ5Z/dXU1CgoKWDJNldWL1H1V+jgHBwcAaDLPHBwccPv27UatEipDlTxSRtu2bXHu3DmUlJTg77//ZpbWyvu35qCrqwt7e3v07t0b+/fvR1paGiIiIhj9VG0nBQUFCn3G64abOLUgz8rL8TA/Fw/zc/HsHd53Irfk89lnn0EgEDQdgKPZyPczlZY+3/fSr18/paaROTgAwMDAAJ6enli/fj2r3sgpKipi/p80aRIuXryII0eO4PLlyyoNwhITE1lfN7p06YKUlBSWTGhoKCwtLXH9+nUkJiYy16pVqxASEsJYXZJKpUqXm8bHxzODDj6fj1GjRiE0NBS5ubkKsiUlJQ2aF3d1dcWNGzdYg6vIyEjo6OiwJj0NYWJiAi0tLezduxfq6uro378/AKBnz54Aai2QySkoKMCjR49gZWXFuJ09exaDBw/Gzz//zLL09Sbi6uqq8BUqMjISrq6uTYZVU1ODpaUlBAIBwsLC8PHHH7O+OC1fvhw//fQTTpw4wdqP0hgymeylnEMnEolgZ2cHa2tr1j6XhhgxYgSEQiE2bNjQoIyrqyvOnz/PGuhGRkZCKpVCX19faZiysjLw+XzWCxf5fWPnINZvb3LCw8NRUVGBsWPHNpmmF8XPzw/JycnMQHj06NE4deqU0qMuqqqqlPY3L4qNjQ1MTU1ZdfLJkyeIiYlh6qSrqyuKiopYX0rOnDkDmUwGFxcXRqapsnqRuq9KHzdgwAAYGho2aKpe3hePHj0aJSUlDda5un12c/OoMTQ1NWFmZobCwkKcPHkSXl5eTYZpDCICETHtVtX+NysrC8+ePVO6d+618kKmKd5i3iSremlJsWRyJoFMziRQWlLscw/Oqh5HMwkJCSEAzHXgwIGWVuk/QWNWet4GsrKyyNTUlNq3b0/79++njIwMSklJoTVr1pCjoyMjJ5PJyM7OjvT19VnuclavXk0HDx6kzMxMunHjBk2fPp34fD7L2llSUhIJhUIqKChg3Dp16kRz585ViK+oqIhEIhFj0SkrK4vU1dXp66+/puvXr1NaWhqtWrWKhEIhHT9+nAn3+PFjcnR0JEtLS9q+fTvdvHmTMjIyKCgoiOzs7BirU2vXrqW+ffsy4aqrq+m9996jAQMGUGJiIp04cYKMjIxo/vz5jExMTAxJpVK6d+8e47Z27VqKi4uj9PR0WrduHUkkElqzZg0rLV5eXtShQwe6dOkS3bhxgz7++GNq3749VVZWEhHRmTNnSENDg+bPn095eXnM9fjxYyaOiooKSkhIoISEBDIzM6NZs2ZRQkICY2WN6MWs6t25c4cSEhJo0aJFpKWlxTzj6dOnjEzfvn1p7dq1zP2lS5dIKBTSypUrKTU1lfz9/UlNTY1u3LjByMybN4/GjRvH3Kenp9POnTspIyODYmJiyNvbmwwMDCg7O5uRCQwMJJFIRPv372flg1yXkpISmj9/PkVHR1NOTg7FxsbShAkTSCwWU3JycrPSpIpVvbo0ZDFsw4YNZGxsTKWlpUrTXVRURCYmJjRu3DhKTk6msLAw0tDQoN9//52ROXDgAMtyW2pqKonFYpo2bRqlpKRQcnIyjR07lnR1dSk3N5eIavv73bt3U2pqKqWmptKSJUuIz+fTtm3bFHTv1asXeXt7K03X48ePKSEhgY4ePUoAKCwsjBISEigvL4+VN01Z1SMimjNnDjk5OZFMJqNnz57RBx98QPr6+rRu3TpKTEykrKws2rt3L3Xt2pXJx/ppJ1Kt/KRSKes3LjAwkPT09OjQoUOUlJREXl5eZGNjw+qXBw4cSF26dKGYmBi6ePEi2dvbk4+PT7PKSpW6X9+qnqp93MGDB0lNTY2GDBlCkZGRlJ2dTdeuXaPZs2ezym/OnDkkEAho9uzZdPnyZcrJyaFTp07RiBEjGGt79+7dI6lUSjExMc3Ko/pt/cSJE3T8+HG6ffs2/fXXX9SpUydycXFh+i6iputQVlYWLV26lGJjY+nOnTt06dIlGjJkCBkYGNCDBw+ISLX+V563tra2CnmpKi/Lqh43cWpBuIkTx8ukY8eOBID5Eed49bztEyciotzcXPLz8yMrKysSiURkYWFBQ4cOpaioKJbc0qVLCQAtX75cIY6ff/6Z2rZtS+rq6mRgYEDu7u505swZBbnu3bvTpk2biIgoNjaWANDVq1eV6jVo0CAaPnw4c3/16lXq378/GRkZka6uLrm4uCidFBQVFdG8efPI3t6eRCIRmZiYkIeHB0VERDCm4f39/RUmGTk5OTRo0CCS/D/2zjssiqv749+l7LJ0kA5KkaYGwYpAoryKYIkixmiwvBKNLRg1dpMoat5YY4pdo4AtilgTG8GCDVREUJoICBIVNEqRXnbP7w9+O2HYXViwbDTzeZ59YO49c+fMmXvvzJm591yhkIyMjGjOnDms8OWSB8aGD/vjxo0jQ0ND4vP51LlzZ9q9e7eUPiUlJTRhwgTS19cnQ0NDCggIYIWaHj9+POulh+TX8OFL8uDelExrHCd5x2547a2trSkkJIS138GDB8nR0ZH4fD516tSJTp48KVVuQ93S0tLIzc2NhEIh6erqkr+/P929e5e1j7W1tUxdJMeurKykgIAAsrCwID6fT+bm5jR06FCp+qPIOb0qx6m8vJwMDAxo9erVMs+bqD708vvvv08CgYAsLS1p1apVrPywsDBq/A77jz/+IC8vL9LT0yMDAwPq27cvxcXFMfnh4eHUoUMH0tTUJF1dXerZsydFRkZK6X337l0CQH/88YfM85IcW57NJbZRxHHKy8sjNTU1ioiIICKiqqoqWrlyJbm4uDD9gpeXF4WHhzPtSta5K3L9ALCun1gspsWLF5OpqSkJBALq168fZWRksMp9/vw5BQYGkra2Nunq6tKnn37KcsaImr9WRM3X/YaOU0v7uPj4eBo+fDgZGxuTQCAge3t7mjx5MusFCRFRREQE9e7dm3R0dEhLS4s6d+5My5cvZ66JpL42tJkiNmrc1iMiIsjOzo74fD6ZmZlRcHAwFRcXS51vU3Xo0aNHNHDgQDIxMSF1dXWysrKi0aNHS7X/5vpfIiJfX19auXKlTFsqwqtynHhEyp90smnTJqxduxYFBQVwdXXFhg0b5E5i/OWXX7B7925mwbFu3bphxYoVCk96fPHiBfT09FBSUgJdXd1Xdg6toaqyEndu1YeC7NzVAxr/H8oW5eWAZP5AWRnwisOHvmnEYjEePnwIoD4Sj6zJwBwtRygUoqqqips3pkSqqqqQk5MDW1vbFk9y/Tdy8uRJzJs3DykpKVw/8IqJiYlBUFAQcnNzla3KPx5vb28EBQVJLVbMIc3SpUuRm5uL8PBwZavyVhAeHo7w8HAmFDjHqyE1NRV9+/bFvXv3mHmtLaWp+3VLfAOl37kiIiIwe/ZshISE4NatW3B1dYWfn5/URD4Jiqwu/ragIRSip1df9PTq+7fT9A4imQQZGhoqd44Bh+JI5jNJVvxu06aNkjXi4FCMwYMHY/LkyW9lf83BwcHBoRzy8/Oxe/fuVjtNrxKlO04//PADJk2ahE8//RQdO3bE1q1boampKbXgooR9+/bh888/h5ubG5ydnbFjxw6IxeJXEjKV4/VhaGj4j1nQ8W1m06ZNUlGxIiMjlaQNB0fLmTVrFrPwIQcHBwcHR3P4+PhILd+gLJS6jlNNTQ0SEhKwaNEiJk1FRQU+Pj7NrsQsobnVxaurq1kRd168ePFySr9CHj98gG+vngAALPb6EBZW1s3s8XbC5/MxY8YMZavx1uPq6oo7d+4w281FWeLg4Pj3YGNjg1mzZilbjbeCoKAguLm5KVuNtwJvb2+50do4pHFzc+OGgL7jKNVxevbsGUQikcwVgxVdA0De6uISVq5ciWXLlr20rq+D0qJnOGpSH652VtEz4B11nDheHj6fzwqTamxsLHc4KwcHx78PznFSHO7BVnG8vb2VrcJbhZubG+eUv+MofajeyyBZXfzo0aNyJ2YvWrQIJSUlzO/PP/98w1rKR1NHF/2Lb6J/8U1o6ig3UAXHP5uGTlNQUBDnNHFwcHBwcHBwvGGU+sXJyMgIqqqqLV7NGPh7dfGzZ8/KXF1cgkAggEAgeCX6vmra2jhgj42DstV47dTW1uLgwYMAgJEjR0JdXV3JGr19lJeXQ1tbGzdv3kTXrl2VrQ4HBwcHBwcHx78OpX5x4vP56NatGyuwgyTQQ1OrGbdmdXEO5UFEyMzMRGZmJhc6W0GcnZ3B4/GwatUqAICmpibEYjHnNHFwcHBwcHBwKAmlfnECgNmzZ2P8+PHo3r07evbsiZ9++gnl5eX49NNPAQD//e9/YWlpiZUrVwIAVq9ejSVLluDXX3+FjY0NCgoKAADa2trQlqx9xPGPQlVVFf7+/sz/HE2jrq7OhG1ftGgRFi5cqGSNODg4ODg4ODg4lO44jRo1Cn/99ReWLFmCgoICuLm54cyZM0zAiLy8PNZCiVu2bEFNTQ1GjBjBKickJARLly59k6q/NBnJCej7V/0XmPPGPDi5dFOyRq8HVVVVdOnSRdlq/OPJy8uDtTU7QMjUqVOVpA0HBwcHBwcHB0dDlO44AcD06dMxffp0mXmNV19+11ZFF/Ekl0CkVD04lMvChQuxevVqVlp6ejqcnZ2VpBEHBwcHBwcHB0dD3uqoem87VjYO2FmUip1FqbB6h4NEiMVi5OfnIz8/n1t3SAadOnViOU0qKiogIs5p4njnqKmpgb29PWJjY5WtyjtHeHg4FzpaQWxsbKReynLIZunSpVz49hbAtcPXw9atWzFkyBBlqwGAc5yUipaOLgYPH4PBw8dA6x0OR15XV4dt27Zh27ZtzNwdjr/x9PRk/rexsYFIxH195HhzFBQU4IsvvoCdnR0EAgHatm2LIUOGICoqCkZGRkyAksZ8++23MDU1ZULlV1dX4+uvv4a1tTUEAgFsbGwQGhrK2mfr1q2wtbVl1XkJU6ZMgaqqKiIjI6XygoKCMGzYMKn0mJgY8Hg81gKdNTU1WLNmDVxdXaGpqQkjIyN4eXkhLCyMFda/MYWFhRgzZgx0dXWhr6+PiRMnoqysTK48AGRnZyMgIADGxsbQ1dXFyJEjpaLE2tjYgMfjsX7ybJqVlQUdHR3o6+uz0sPDw6XKkLcEh4Tc3FypfXg8HsaOHcvIzJgxA926dYNAIGjR2jObNm2CjY0NNDQ04O7ujhs3bjQpX1tbi+XLl6N9+/bQ0NCAq6srzpw5w5JZunSplK6yXh7FxcWhb9++0NLSgq6uLnr37o3KykomvyX2lhAUFMTI8vl82NvbY/ny5cz9SlLPJD9jY2MMGjQIycnJzdoqJiYGXbt2hUAggL29PcLDw5vd5+DBg3Bzc4Ompiasra2xdu1aVn5+fj5Gjx4NR0dHqKioyF2/q7i4GMHBwTA3N4dAIICjoyNOnTrF5MuyFY/HQ3BwcJPno4gtWtsOv/vuO3h6ekJTU1OqHciDiLBkyRKYm5tDKBTCx8cHmZmZLBlF2vedO3fwwQcfQENDA23btsWaNWukjhUZGQlnZ2doaGjAxcWFZc/m8PPzg6qqKuLj42XmJyYm4uOPP4apqSk0NDTg4OCASZMm4d69eyy5w4cPw9vbG3p6etDW1kbnzp2xfPlyFBYWyj22IjZqTGlpKWbNmgVra2sIhUJ4enpK6d6w7Uh+AwYMkCrr5MmTcHd3h1AohIGBgVR/LqseHjhwgMmfMGECbt26hcuXLzep85uAc5w43gg6OjrQ0dFRthr/GJ49e8b8/8svv8De3h4LFixATk6OErXi+LeRm5uLbt264fz581i7di2Sk5Nx5swZ/Oc//8HMmTMxduxYhIWFSe1HRAgPD8d///tfZnmBkSNH4ty5c9i5cycyMjKwf/9+ODk5sfbZuHEjJk6cKFVeRUUFDhw4gPnz50s5Wy2hpqYGfn5+WLVqFSZPnozY2FjcuHEDwcHB2LBhA1JTU+XuO2bMGKSmpiI6OhonTpzApUuXMHnyZLny5eXl8PX1BY/Hw/nz53H16lXU1NRgyJAhUl/Wly9fznx1z8/PxxdffCFVXm1tLQIDA/HBBx/IPJ6uri6rjAcPHihkk7Nnz7L227RpEyt/woQJGDVqlEJlAUBERARmz56NkJAQ3Lp1C66urvDz82tybblvvvkG27Ztw4YNG5CWloapU6ciICAAiYmJLLlOnTqxdL1y5QorPy4uDgMGDICvry9u3LiB+Ph4TJ8+nTUPGlDM3o0ZMGAA8vPzkZmZiTlz5mDp0qVSDktGRgby8/MRFRWF6upqDB48GDU1NXLLzMnJweDBg/Gf//wHSUlJmDVrFj777DNERUXJ3ef06dMYM2YMpk6dipSUFGzevBk//vgjNm7cyMhUV1fD2NgY33zzDVxdXWWWU1NTg/79+yM3NxeHDh1CRkYGfvnlF1haWjIy8fHxLDtFR0cDAD7++ONm7dWULV6mHdbU1ODjjz/GtGnTmtVBwpo1a7B+/Xps3boV169fh5aWFvz8/FBVVcXINNe+X7x4AV9fX1hbWyMhIQFr167F0qVLsX37dkYmNjYWgYGBmDhxIhITEzFs2DAMGzYMKSkpzeqYl5eH2NhYTJ8+XWYfd+LECfTq1QvV1dXYt28f0tPTsXfvXujp6WHx4sWM3Ndff41Ro0ahR48eOH36NFJSUrBu3Trcvn0be/bseSkbNeazzz5DdHQ09uzZg+TkZPj6+sLHxwePHj1iyUnajuS3f/9+Vv7hw4cxbtw4fPrpp7h9+zauXr2K0aNHSx0vLCyMVU5D54rP52P06NFYv369XH3fGPQvo6SkhABQSUmJslWhR3/m0oy9P9GMvT/Roz9z/84oKyMC6n9lZcpTkOO1MHXqVAJAPB5P2apwvCSVlZWUlpZGlZWVUnk1lZVUU1lJYrGYSaurraGaykqqramRLSsSNZCtrZetrlZItjUMHDiQLC0tqUxGP1NUVER37twhAHT58mVW3oULFwgApaenExHR6dOnSU9Pj54/fy73WPHx8aSiokIvXryQygsPD6devXpRcXExaWpqUl5eHit//Pjx5O/vL7WfRI+ioiIiIlq9ejWpqKjQrVu3pGRrampknicRUVpaGgGg+Ph4Ju306dPE4/Ho0aNHMveJiooiFRUV1r2kuLiYeDweRUdHM2nW1tb0448/yiyjIfPnz6exY8dSWFgY6enpsfJkpTUmLCyM+vTpw2zn5OQQAEpMTGz22CEhIeTq6tqsHBFRz549KTg4mNkWiURkYWFBK1eulLuPubk5bdy4kZU2fPhwGjNmTIt0cHd3p2+++aZJGUXsbW1tTRcuXGC2ZdWv/v37U69evYhIup4REf32228EgG7fvi33OPPnz6dOnTqx0kaNGkV+fn5y9wkMDKQRI0aw0tavX09WVlasvkRCnz59aObMmVLpW7ZsITs7O6pp1Nc0xcyZM6l9+/as44SEhND48eOZbUVs0dp22BBF6jwRkVgsJjMzM1q7di2TVlxcTAKBgPbv309EirXvzZs3k4GBAVU36G8XLFhATk5OzPbIkSNp8ODBrOO7u7vTlClTWHo3bIcSli5dSp988gmlp6eTnp4eVVRUMHnl5eVkZGREw4YNk3mOEltfv36dANBPP/3UpFxjFLFRYyoqKkhVVZVOnDjBSu/atSt9/fXXzLa8vllCbW0tWVpa0o4dO+TKEBEBoKNHjzYpc/HiReLz+SzbtYSm7tct8Q24L05KpLToGSIs+iDCog9Ki541vwPHW4+FhQW2bt0KoP4N/K5du5SsEcfrYv34EVg/fgQqS18wafG/HcH68SNwPnQLS3bz5DFYP34EXjz7i0lLijqJ9eNHIGrrzyzZX6ZPwPrxI/D80Z9MWurFsy3Wr7CwEGfOnEFwcDC0tLSk8vX19eHi4oIePXpIvSENCwuDp6cnM5Tqt99+Q/fu3bFmzRpYWlrC0dERc+fOZQ2hunz5MhwdHWV+ed65cyfGjh0LPT09DBw4UKHhTLLYt28ffHx8ZEbxVFdXZ85TMvRNQlxcHPT19VnrAvr4+EBFRQXXr1+Xeazq6mrweDzWAusaGhpQUVGR+lKyatUqtGnTBl26dMHatWulhiyfP38ekZGRUl+DGlJWVgZra2u0bdsW/v7+Tb61f5XweDzmetTU1CAhIQE+Pj5MvoqKCnx8fBAXFye3jOrqaqmhhUKhUMpOmZmZsLCwgJ2dHcaMGYO8vDwm7+nTp7h+/TpMTEzg6ekJU1NT9OnTR6oMoHl7K4JQKJT7NamkpIQZRsTn85l0b29v1nyguLg4lq2A+uFarbHVw4cPFf7KCNS3SQ8PDwQHB8PU1BTvvfceVqxYIXcoeE1NDfbu3YsJEyaw2kZzyLJFa9tha8jJyUFBQQHLznp6enB3d2fsrEj7jouLQ+/evVnX08/PDxkZGSgqKmJkWno9gfp7fVhYGMaOHQtnZ2fY29vj0KFDTH5UVBSePXuG+fPny9xfMmRx37590NbWxueff96knGSYrmQenyI2akxdXR1EIpFC7TYmJgYmJiZwcnLCtGnT8Pz5cybv1q1bePToEVRUVNClSxeYm5tj4MCBMr/SBQcHw8jICD179kRoaKjUup/du3dHXV2d3D75TcE5TkpEU0cXvcuS0LssCZrv8BwnjnpUVFSQn5/PbDs7O2P8+PFK1Ijj30xWVpZCQUgmTpyIyMhIZj5AaWkpDh06hAkTJjAy9+/fx5UrV5CSkoKjR4/ip59+wqFDh1g3+AcPHsDCwkKq/MzMTFy7do0ZLiYZHtj4pqkImZmZCgVV0dPTYw0jLCgogImJCUtGTU0NhoaGzFqBjenVqxe0tLSwYMECVFRUoLy8HHPnzoVIJGK18xkzZuDAgQO4cOECpkyZghUrVrAekJ4/f46goCCEh4dDV1f2fcDJyQmhoaE4fvw49u7dC7FYDE9PTzx8+LDZc/X09GTWOdTW1pYaHtccTk5O0NPTA1A/xFgkEjHLhUgwNTWVayeg/uHyhx9+QGZmJsRiMaKjo3HkyBGWndzd3REeHo4zZ85gy5YtyMnJwQcffIDS0lIA9XUMqJ8LNWnSJJw5cwZdu3ZFv379WHM1mrN3cxARzp49i6ioKPTt25eVZ2VlBW1tbejr6+PXX3/F0KFDWfWtXbt2MDc3Z7YLCgpk2urFixeslwqNbXXkyBGcO3cOYrEY9+7dw7p16wCAZa/muH//Pg4dOgSRSIRTp05h8eLFWLduHf73v//JlD927BiKi4sVDgTRlC1a2w5bg6TeNVUnFWnf8q5Vw2PIk2mq7gP1w2UrKirg5+cHoL6P27lzJ5Mvqb/N2SwzMxN2dnbM8Gh5qKurw8nJCZqamiz9W6K7jo4OPDw88O233+Lx48cQiUTYu3cv4uLiWPVwwIAB2L17N86dO4fVq1fj4sWLGDhwIOOgN2y333zzDU6cOAEDAwN4e3uz5mQtX74cBw8eRHR0ND766CN8/vnn2LBhA0snTU1N6OnptegFwuvgHxGO/N9KWxsHHHyHo+lJqK2txdGjRwEAAQEBzTb6d43Y2Fh4eXmx0lauXMktbPuOM2NX/RtFtQZfJHoMHY5ug/zBa7QQ9Ofb99XLNnjb6eY3GJ37+YHXaP7GpI2hUrKd+rDfgiqCoo5JYGAgvvzySxw8eBATJkxAREQEVFRUWPNixGIxeDwe9u3bxzxk//DDDxgxYgQ2b94MoVCIyspKmQENQkND4efnByMjIwDAoEGDMHHiRJw/fx79+vV7LecUEBCAgICAFpXdGGNjY0RGRmLatGlYv349VFRUEBgYiK5du7Lm3MyePZv5v3PnzuDz+ZgyZQpWrlwJgUCASZMmYfTo0ejdu7fcY3l4eMDDw4PZ9vT0RIcOHbBt2zZ8++23TeoZERGBDh06MNtt27Zt0XnevXu3RfKy+PnnnzFp0iQ4OzuDx+Ohffv2+PTTT1lfMgcOHMj837lzZ7i7u8Pa2hoHDx7ExIkTmXljU6ZMwaeffgoA6NKlC86dO4fQ0FCsXLkSQPP2lseJEyegra2N2tpaiMVijB49WmptyMuXL0NTUxPXrl3DihUrmNEDEnbv3t06AzVg0qRJyM7Oxocffoja2lro6upi5syZWLp0qdRcrqYQi8UwMTHB9u3boaqqim7duuHRo0dYu3YtQkJCpOR37tyJgQMHyny5IYumbPEm2+HbQGhoKEaNGgU1tfpH7sDAQMybNw/Z2dlo3769wvZSVM7S0vKVtNs9e/ZgwoQJsLS0hKqqKrp27YrAwEAkJCQwMp988gnzv4uLCzp37oz27dsjJiYG/fr1Y9rt119/jY8++ghA/YgFKysrREZGYsqUKQDAmsfVpUsXlJeXY+3atZgxYwZLJ6FQiIqKipc+t5eB++LE8dohIqSlpSEtLa1Vb5HfZm7duiXlNP3111+c0/QvQF1DA+oaGqyhKKpq6lDX0IBao5cHjGyDByNVNbV62QYOUlOyLcXBwQE8Hq/ZG6yuri5GjBjBBIkICwvDyJEjoa2tzciYm5vD0tKScZoAoEOHDiAi5quIkZERM+RFgkgkwq5du3Dy5EmoqalBTU0NmpqaKCwsZD1U6+rqoqSkREq34uJiqKqqMkN/HB0dW/XAYGZmJhXcoK6uDoWFhTAzM5O7n6+vL7Kzs/H06VM8e/YMe/bswaNHj2BnZyd3H3d3d9TV1TFrEp4/fx7ff/89c/4TJ05ESUkJ1NTU5AbKUFdXR5cuXZCVldXsubVt2xb29vbMrynnoTmMjIygqqoqFTnwyZMnTdrJ2NgYx44dQ3l5OR48eIC7d+9CW1u7STvp6+vD0dGROUfJl5yOHTuy5Dp06MAa0teYxvaWhySAQ2ZmJiorK7Fr1y6pIay2trZwcnLC+PHj8dlnnzUbVMPMzEymrXR1dSEUCmXuw+PxsHr1apSVleHBgwcoKChAz549AaBJezXG3Nwcjo6OUG3wkqZDhw4oKCiQGoL44MEDnD17Fp999pnC5Tdli9a2w9YgqXdN1UlF2re8a9XwGPJkmqr7hYWFOHr0KDZv3sy0cUtLS9TV1THt29HREUDzLykcHR1x//79JqMSykIRG8miffv2uHjxIsrKyvDnn3/ixo0bqK2tbbIe2tnZwcjIqMl2KxAIYGdn12y7ffjwIaqrq1nphYWFMDY2lrvfm4BznDheO6qqqhg0aBAGDRrE6sT/DXTt2pX5n8/ng4iYN+scHMrE0NAQfn5+2LRpE8rLy6XyG4b4njhxIq5cuYITJ04gNjZWKjKel5cXHj9+zArve+/ePaioqMDKygpA/VvEu3fvsl6enDp1CqWlpUhMTERSUhLz279/P44cOcLo4OTkhNTUVKmb6K1bt2Bra8t8xR49ejTOnj0rczhabW2tzPME6r/oFBcXs96knj9/HmKxGO7u7jL3aYiRkRH09fVx/vx5PH36FEOHDpUrm5SUBBUVFWboUFxcHOvcly9fDh0dHSQlJcl9Gy8SiZCcnMwaFvYm4PP56NatG86dO8ekicVinDt3jvVFTB4aGhrMQ+Phw4fh7+8vV7asrAzZ2dnMOdrY2MDCwgIZGRksuXv37sHa2lpuOY3tLQ8tLS3Y29ujXbt2zJeBpggODmaGpsrDw8ODZSsAiI6OVshWqqqqsLS0BJ/Px/79++Hh4dGiB0YvLy9kZWWxIjzeu3cP5ubmrHk8QP3LEBMTEwwePFjh8hvS2BatbYetwdbWFmZmZiw7v3jxAtevX2fsrEj79vDwwKVLl1hOSXR0NJycnGBgYMDItPR67tu3D1ZWVrh9+zarna9btw7h4eEQiUTw9fWFkZGRzPDnwN998ejRo1FWVobNmzc3KdcaGzWFlpYWzM3NUVRUhKioqCbb7cOHD/H8+XOm3UqWO2jYbmtra5Gbm9tsuzUwMGC96MnOzkZVVZXMuXNvlFaFpniL+SdF1bt75ybZnrtCtueu0N07N//O4KLqvfVs376d+f/BgwfUr18/JWrD8bpoKkrP20B2djaZmZlRx44d6dChQ3Tv3j1KS0ujn3/+mZydnRk5sVhM9vb2ZGBgwEqXUFpaSlZWVjRixAhKTU2lixcvkoODA3322WeMzLNnz0hdXZ2Sk5OZNH9/fxo1apRUeSKRiMzMzJhIbEVFRWRiYkIjR46kmzdvUmZmJu3cuZN0dHRoy5YtzH5VVVX0wQcfkIGBAW3cuJGSkpIoOzubIiIiqGvXrkyEuSNHjrCiZRERDRgwgLp06ULXr1+nK1eukIODAwUGBjL5Dx8+JCcnJ7p+/TqTFhoaSnFxcZSVlUV79uwhQ0NDmj17NpMfGxtLP/74I6PH3r17ydjYmP773//KvSayooktW7aMoqKiKDs7mxISEuiTTz4hDQ0NSk1NZe3X0qh6mZmZlJiYSFOmTCFHR0dKTEykxMREVmQxJycnOnLkCLN94MABEggEFB4eTmlpaTR58mTS19engoICRmbcuHG0cOFCZvvatWt0+PBhys7OpkuXLlHfvn3J1taWFQVszpw5FBMTQzk5OXT16lXy8fEhIyMjevr0KSPz448/kq6uLkVGRlJmZiZ98803pKGhQVlZWS2ytyJR9RoiK5IcUX3UPBcXFyYKXePzvn//PmlqatK8efMoPT2dNm3aRKqqqnTmzBlGZsOGDdS3b19m+6+//qItW7ZQeno6JSYm0owZM0hDQ4NV74iIuVbdunWj0aNHU2JiIqs+5OXlkY6ODk2fPp0yMjLoxIkTZGJiQv/73/9Y5YhEImrXrh0tWLBA5rkrElWvsS1eph0+ePCAEhMTadmyZaStrc2cZ2lpKSPTuE6uWrWK9PX16fjx43Tnzh3y9/cnW1tbVr/cXPsuLi4mU1NTGjduHKWkpNCBAwdIU1OTtm3bxshcvXqV1NTU6Pvvv6f09HQKCQmR6tMat0NXV1eZti0uLiY+n89ErTt27Bipq6vTkCFDKDo6mnJycig+Pp7mzZvH6iPnz59PqqqqNG/ePIqNjaXc3Fw6e/YsjRgxgom2J6uvUsRGffv2pQ0bNjDbZ86codOnT9P9+/fpjz/+IFdXV3J3d2ciNZaWltLcuXMpLi6OcnJy6OzZs9S1a1dycHCgqqoqppyZM2eSpaUlRUVF0d27d2nixIlkYmJChYWFRFQflfGXX36h5ORkyszMpM2bN5OmpiYtWbKEZbOwsDCys7OTsqWivKqoepzjpETu3rlJpucTyfR8Iuc4vSOcO3eOABAAOnXqlLLV4XjNvO2OExHR48ePKTg4mKytrYnP55OlpSUNHTqU9WBJRLRixQoCQGvWrJFZTnp6Ovn4+JBQKCQrKyuaPXu2VNjYkSNHMg+WBQUFpKamRgcPHpRZ3rRp06hLly7MdkZGBgUEBJCFhQVpaWmRq6sr/fLLL1IhmquqqmjlypXk4uJCGhoaZGhoSF5eXhQeHk61/x+2PSwsjBq/N3z+/DkFBgaStrY26erq0qeffsp6WJM4Ig3tsmDBAjI1NSV1dXVycHCgdevWsfRJSEggd3d30tPTIw0NDerQoQOtWLGC9VDRGFmO06xZs6hdu3bE5/PJ1NSUBg0aJBXquTWOU58+fZj+quEvJyeHkQFAYWFhrP02bNjA6NOzZ0+6du2aVLkNH7ZjYmKoQ4cOJBAIqE2bNjRu3DipMO+jRo0ic3Nzpg6OGjWKcYgasnLlSrKysiJNTU3y8PBghcpX1N6vynHKy8sjNTU1ioiIkHnekn3d3NyIz+eTnZ2dlC1DQkLI2tqa2f7rr7+oV69epKWlRZqamtSvXz8p+xKRzOvWsByiekfS3d2dBAIB2dnZ0XfffUd1dXUsmaioKAJAGRkZMs9dUcepsS1a2w7Hjx8v89waXq/GdVIsFtPixYvJ1NSUBAIB9evXT+p8mmvfRES3b9+m999/nwQCAVlaWtKqVauk7HHw4EFydHQkPp9PnTp1opMnT7LyG7bDmzdvEgC6ceOGTNsOHDiQAgICmO34+HgaPnw4GRsbk0AgIHt7e5o8eTJlZmay9ouIiKDevXuTjo4OaWlpUefOnWn58uXMNZHVVyliI2trawoJCWEdx87Ojvh8PpmZmVFwcDAVFxcz+RUVFeTr60vGxsakrq5O1tbWNGnSJNZLFKL6EPRz5swhExMT0tHRIR8fH0pJSWHyT58+TW5ubqStrc307Vu3biVRgyU3iIh8fX2bXPagOV6V48Qj+ndNOnnx4gX09PRQUlIiN4LRm6K89AWOH98LAPD3HwstSWS98nJAMn+grAyQESr4bUIsFjNzGwwMDFo0wfVtYujQofj999+ZbYFA0OTichxvP1VVVcjJyYGtra3MwAccbO7cuYP+/fsjOzubNUeK4+UJDw9HeHg4E4KYQz42NjYIDw+Ht7e3slX5x7N06VLk5ua2eomAfxtcO3w9pKamom/fvrh37x5rLm1LaOp+3RLfgIuqp0S0dHQxeqzsePzvEnV1dUxYya+++kpqfPW7gK6uLhM2F6gfyy8v3CwHx7+Vzp07Y/Xq1cjJyYGLi4uy1eHg4ODgeAvIz8/H7t27W+00vUo4x4njjfAykZz+yVRUVEhFXvL09MTVq1eVpBEHxz8bRdeJ4eDg4ODgACC18LAy4RwnJfK04DHWRtWvuD3P7xOYmCm2fsLbBp/Px6JFi5StxmuhsdMUHh7OLWrLwcHxxnFzc+OcUgWZNWsWbGxslK3GW4G3t7fcaG0c0nDt8N2Hm+OkRDKSE9DnWX147otGIji5dKvPeMfmOL3L9O7dG5cvXwYAlJeXMyt1c/w74OY4cXBwcHBw/PN5VXOc3s1Z+m8JfKEW3CtS4F6RAr6Qc47eFgYMGMD8f+nSJaxbtw5ExDlNHBwcHBwcHBzvMNxQPSVia++M4/bOylbjtVNXV8dEmxsyZIhCiwv+E2k4n0lDQ4OJmDd79mxlqsXBwcHBwcHBwfEG4L44cbx2xGIxbt++jdu3b7NWMX+b2LVrF2s+U3V1tRK14eDg4ODg4ODgeNO8na/+Od4qVFRU0L9/f+b/t41evXrh+vXrrLTy8nIlacPBwcHBwcHBwaEMOMdJiWSm3cbQgkIAwG9mhnDo6KpkjV4Pampq8PLyUrYarUIoFLIWsdXV1UVJSYkSNeLg4ODg4ODg4FAGb9/r/3cIsagORTwDFPEMIBbVKVsdjkbY2NiwnKZhw4ZxThMHx0vw/PlzmJiYIDc3V9mqvHPExMRwIbYVxMbGBjExMcpW463A29sb4eHhylbjrSE8PBze3t7KVuOdY+vWrRgyZIiy1QDAOU5KxdTKFsuyzmJZ1lmYWtkqW53XhlgsxosXL/DixYu3ao5Tw4e7c+fO4ejRo8pThoPjNVFQUIAvvvgCdnZ2EAgEaNu2LYYMGYKoqCgYGRlh1apVMvf79ttvYWpqitraWgQFBYHH40n9OnXqxNrnu+++g7+/v8wHfD8/P6iqqiI+Pl4qz9vbG7NmzZJKDw8Ph76+PivtxYsX+Prrr+Hs7AwNDQ2YmZnBx8cHR44cQVOrb+Tl5WHw4MHQ1NSEiYkJ5s2bh7q6pl9o3bp1C/3794e+vj7atGmDyZMno6ysjCUTHx+Pfv36QV9fHwYGBvDz88Pt27dllpeVlQUdHR2pcwKAn376CU5OThAKhWjbti2+/PJL1osdWci6Ju+//z6T/91338HT0xOampoyjymPyMhIxr4uLi44depUs/ts2rQJHTp0gFAohJOTE3bv3s3KDw8Pl9K1Ycjg2tpaLFiwAC4uLtDS0oKFhQX++9//4vHjxzKPV11dDTc3N/B4PCQlJTWpW8P6y+fzYW9vj+XLlzPXPyYmhqWXsbExBg0ahOTk5CbLzc/Px+jRo+Ho6AgVFRWZdVgWitTFmJgYdO3aFQKBAPb29jKdm02bNsHGxgYaGhpwd3fHjRs3WPlVVVUIDg5GmzZtoK2tjY8++ghPnjxpUrelS5cydlBVVUXbtm0xefJkFBYWsuTk9Svnzp1rsvw7d+7ggw8+gIaGBtq2bYs1a9Y0KQ/U3589PT2ho6MDMzMzLFiwQMpezZVbW1uL5cuXo3379tDQ0ICrqyvOnDnDkrl06RKGDBkCCwsL8Hg8HDt2rFndGtJUHwcAiYmJ+Pjjj2FqagoNDQ04ODhg0qRJuHfvHkvu8OHD8Pb2hp6eHrS1tdG5c2csX75c6ho0hIiwZMkSmJubQygUwsfHB5mZmU3qW1pailmzZsHa2hpCoRCenp5Susvq+xtGHpZw8uRJuLu7QygUwsDAAMOGDWPly+qrDhw4wORPmDABt27dYpZ/USr0L6OkpIQAUElJibJVkU9ZGRFQ/ysrU7Y2L011dTWFhIRQSEgIVVdXK1udJuncuTPNmTOH2X7w4IESteH4p1NZWUlpaWlUWVmpbFVaRU5ODllYWFDHjh3p0KFDlJGRQSkpKbRu3TpycnKimTNnkqOjo9R+YrGY7OzsaO7cuUREVFxcTPn5+czvzz//JENDQwoJCWH2KS8vJ11dXYqLi5Mq78GDB6StrU0zZsygqVOnSuX36dOHZs6cKZUeFhZGenp6zHZRURF16tSJrKysKDw8nFJTUykjI4O2b99O7du3p6KiIpl2qKuro/fee498fHwoMTGRTp06RUZGRrRo0SK5tnv06BEZGBjQ1KlT6e7du3Tjxg3y9PSkjz76iJEpLS0lQ0NDCgoKort371JKSgp99NFHZGpqSjU1NazyampqqHv37jRw4EDWORER7du3jwQCAe3bt49ycnIoKiqKzM3N6csvv2RkLly4QNbW1qz9AFBYWBjr2jx//pzJX7JkCf3www80e/ZsqWPK4+rVq6Sqqkpr1qyhtLQ0+uabb0hdXZ2Sk5Pl7rN582bS0dGhAwcOUHZ2Nu3fv5+0tbXpt99+Y2TCwsJIV1eXpWtBQQGTX1xcTD4+PhQREUF3796luLg46tmzJ3Xr1k3mMWfMmEEDBw4kAJSYmMjKs7a2pgsXLjDb48ePpwEDBlB+fj7l5ubS5s2bicfj0YoVK4io3rYAKCMjg/Lz8ykhIYH69u1Lbdu2bfKelpOTQzNmzKBdu3aRm5ubzDrcGEXq4v3790lTU5Nmz55NaWlptGHDBlJVVaUzZ84wMgcOHCA+n0+hoaGUmppKkyZNIn19fXry5AkjM3XqVGrbti2dO3eObt68Sb169SJPT0+WPn369KGwsDBmOyQkhDp16kT5+fn08OFDio6OprZt29LIkSNZ591UvyKPkpISMjU1pTFjxlBKSgrt37+fhEIhbdu2Te4+SUlJxOfzadmyZZSZmUkxMTHk7OzMuo8rUu78+fPJwsKCTp48SdnZ2bR582bS0NCgW7duMTKnTp2ir7/+mo4cOUIA6OjRo1L6hIWFUZ8+faTSm+vjfv/9d+Lz+TRkyBCKjo6m+/fv07Vr12jOnDks23711VekqqpKc+fOpatXr1JOTg798ccfNHz4cPrpp5/k2mnVqlWkp6dHx44do9u3b9PQoUPJ1ta2yXvXyJEjqWPHjnTx4kXKzMykkJAQ0tXVpYcPHzIyDduO5FdYWMgq59ChQ2RgYEBbtmyhjIwMSk1NpYiICJaMrL6qsW5z586lESNGyNW3OZq6X7fEN+Acp38i76DjtGzZMlq2bNk/2nFSV1cnAASAc5g4FKKpjlhUXUei6joSi8VMmrhWVJ9WK5ItK2ogW/f/sjWKybaGgQMHkqWlJZXJ6GeKiorozp07BIAuX77MypM8SKanp8ss9+jRo8Tj8Sg3N5dJi4yMJGNjY5nyS5cupU8++YTS09NJT0+PKioqWPmKOk7Tpk0jLS0tevTokZRsaWkp1dbWyjz+qVOnSEVFhfWgvmXLFtLV1ZXbZ23bto1MTExIJPrb9hJ7ZWZmEhFRfHw8AaC8vDy5MhLmz59PY8eOlTonIqLg4GDq27cvK2327Nnk5eXFbMtznGQ93DVG1jHlMXLkSBo8eDArzd3dnaZMmSJ3Hw8PD8bJltBY/5boIOHGjRsy++tTp06Rs7MzpaamKuw4+fv7s2T69+9PvXr1IqK/63tDx/u3334jAHT79m2FdJVXhxujSF2cP38+derUibXfqFGjyM/Pj9nu2bMnBQcHM9sikYgsLCxo5cqVRFTviKqrq1NkZCQjk56eTgBYLzdkOU6urq6sY8+ePZsMDAyY7eb6FXls3ryZDAwMWG1uwYIFTTpbixYtou7du7PSfvvtN9LQ0KAXL14oXK65uTlt3LiRVc7w4cNpzJgxMo/bUsepqT6uvLycjIyMaNiwYTKPJbHZ9evXCYBcB0mebcViMZmZmdHatWuZtOLiYhIIBLR//36Z+1RUVJCqqiqdOHGCld61a1f6+uuvmW1ZbachtbW1ZGlpSTt27JArQ6RYX3Xx4kXi8/lS9wdFeVWOEzdUT4k8LXiMr0PX4uvQtXhaIHu4wbsAn8/HkiVLsGTJEvD5fGWrI8WzZ8/A4/FQW1vLpO3atUuJGnG8CzxeEovHS2IhLv+7XpVeeojHS2JRfDybJZv/7TU8XhILUfHfYe7L4vLxeEksCg+zh2kUrL6Bx0tiUfdXBZNWntD08BpZFBYW4syZMwgODmaF2pegr68PFxcX9OjRA6Ghoay8sLAweHp6wtlZ9jp0O3fuhI+PD6ytrZm0y5cvo1u3blKyRISwsDCMHTsWzs7OsLe3x6FDh1p8PmKxGAcOHMCYMWNgYWEhla+trc2sIbd06VLWcMG4uDi4uLjA1NSUSfPz88OLFy+Qmpoq83jV1dXg8/msSKFCoRAAcOXKFQCAk5MT2rRpg507d6KmpgaVlZXYuXMnOnTowDr++fPnERkZiU2bNsk8lqenJxISEpihVvfv38epU6cwaNAgBSzzctjY2GDp0qXMdlxcHHx8fFgyfn5+iIuLk1tGdXU1a9gdUG+rGzdusPrdsrIyWFtbo23btvD395drewklJSXg8XisYYZPnjzBpEmTsGfPnpdalFwoFKKmpkbucSXDiBre07y9vREUFNTqYwKK1cXmrkFNTQ0SEhJYMioqKvDx8WFkEhISUFtby5JxdnZGu3btmryWjcnNzUVUVBRjB0X6FQlBQUGs+UBxcXHo3bs3y6Z+fn7IyMhAUVGRzOPLq1tVVVVISEhQuFx55Uja8svQXB8XFRWFZ8+eYf78+TL3l9hs37590NbWxueff96kXG5uLng8HjOPLycnBwUFBaxrraenB3d3d7nXuq6uDiKRSCGbxMTEwMTEBE5OTpg2bRqeP3/O5N26dQuPHj2CiooKunTpAnNzcwwcOBApKSlSxwwODoaRkRF69uyJ0NBQqaHV3bt3R11dnVSU4zcN5zgpkaK/8rHTtj922vZH0V/5ylbnX8m3334LY2NjVlpCQgIWL16sJI04ON4MWVlZICK5zo+EiRMnIjIykpm7U1paikOHDmHChAky5R8/fozTp0/js88+Y6U/ePBApkNz9uxZVFRUwM/PDwAwduxY7Ny5s8Xn8+zZMxQVFTV7PgBgZGSE9u3bM9sFBQWsB1UAzHZBQYHMMvr27YuCggKsXbsWNTU1KCoqwsKFCwHUz20BAB0dHcTExGDv3r0QCoXQ1tbGmTNncPr0acaJe/78OYKCghAeHg5dXV2Zxxo9ejSWL1+O999/H+rq6mjfvj28vb3x1VdfNXuugYGB0NbWZn4tnZfRvn17GBkZMdvybCXPTkD9Q+qOHTuQkJAAIsLNmzexY8cO1NbW4tmzZwDqnczQ0FAcP34ce/fuhVgshqenJx4+fCizzKqqKixYsACBgYGM3YgIQUFBmDp1Krp3796i85RARDh79iyioqLQt29fVp6VlRW0tbWhr6+PX3/9FUOHDmXVt3bt2sHc3LxVx5WgSF2UJ/PixQtUVlbi2bNnEIlETV6ngoIC8Pl8qbltzV1LAEhOToa2tjaEQiFsbW2RmpqKBQsWAFC8XwEAc3NztGvXrkXn3hg/Pz/ExsZi//79EIlEePToEZYvXw7g73aoSLl+fn744YcfkJmZCbFYjOjoaBw5coQp42Voro+TzDVqzmaZmZmws7ODurp6k3Lq6upwcnJiXhxIzrEl7VZHRwceHh749ttv8fjxY4hEIuzduxdxcXEsmwwYMAC7d+/GuXPnsHr1aly8eBEDBw6ESCQCUP+SB6h/WfXNN9/gxIkTMDAwgLe3N2tO1vLly3Hw4EFER0fjo48+wueff44NGzawdNLU1ISenh4ePHjQ5Pm/brhw5EqEL9RCl6qk///fTam6/BtxdnZGRkYGs83j8d6q4BUc/2wslnsCAHjqf7+f0ultBe33LcFT4bFkzRf3qpdV+1tW28McWj3NwOOxZc0W9JSS1erGviEqQuO3efIIDAzEl19+iYMHD2LChAmIiIiAiooKRo0aJVN+165d0NfXl5r8W1lZKfX2EgBCQ0MxatQoxpEIDAzEvHnzkJ2dzXJuXtX5AMD06dMxffp0heVl0alTJ+zatQuzZ8/GokWLoKqqihkzZsDU1JT5ClVZWYmJEyfCy8uLebD7/vvvMXjwYMTHx0MoFGLSpEkYPXo0evfuLfdYMTExWLFiBTZv3gx3d3dkZWVh5syZ+Pbbb5t9yfPjjz+y3jS39MG+ucn8irB48WIUFBSgV69eICKYmppi/PjxWLNmDWMrDw8PeHh4MPt4enqiQ4cO2LZtG7799ltWebW1tRg5ciSICFu2bGHSN2zYgNLSUixatKjFOp44cQLa2tqora2FWCzG6NGjWV/agPqvppqamrh27RpWrFiBrVu3svIbB7x4V3FycsJvv/2Gqqoq7N27F0lJSfjiiy8AtKwdrly58qV18fX1xdq1azF16lSMGzcOAoEAixcvxuXLl1u0buTPP/+MSZMmwdnZGTweD+3bt8enn34q9bW9NTTXxylqM0XlLC0tcffu3VbrK2HPnj2YMGECLC0toaqqiq5duyIwMJD5kgcAn3zyCfO/i4sLOnfujPbt2yMmJgb9+vVjnqm+/vprfPTRRwDqRyxYWVkhMjISU6ZMAQBWP9alSxeUl5dj7dq1mDFjBksnoVCIiooKKBPui5MSsbV3xumBn+D0wE9ga9/825m3lbq6Opw8eRInT55sNkrVm+LZs2csp8nCwoJzmjheKSp8VajwVVmOD09NpT5NTUW2bAOHiqf6/7Lqism2FAcHB/B4vGZvsLq6uhgxYgTCwsIA1N/0Ro4cCW1tbSlZIkJoaCjGjRsnNSzXyMhIarhNYWEhjh49is2bN0NNTQ1qamqwtLREXV0d64FF3vppxcXF0NPTAwAYGxtDX1+/VQ8MZmZmUtHEJNtmZmZy9xs9ejQKCgrw6NEjPH/+HEuXLsVff/0FOzs7AMCvv/6K3NxchIWFoUePHujVqxd+/fVX5OTk4Pjx4wDqh+l9//33zPlPnDgRJSUlUFNTY2ywePFijBs3Dp999hlcXFwQEBCAFStWYOXKlc32W2ZmZrC3t2d+soZPtQR5tmrKTkKhEKGhoaioqEBubi7y8vJgY2MDHR0dqS/+EtTV1dGlSxdkZWWx0iVO04MHDxAdHc36Snf+/HnExcVBIBBATU0N9vb2AOqH+IwfP77J8/rPf/6DpKQkZGZmorKyErt27ZKyla2tLZycnDB+/Hh89tlncl8evAyK1EV5Mrq6uhAKhTAyMoKqqmqT18nMzAw1NTUoLi6WKyMPSeTB9957D6tWrYKqqiqWLVsGQPF+RRatbYezZ89GcXEx8vLy8OzZM/j7+wMA0w4VKdfY2BjHjh1DeXk5Hjx4gLt370JbW5spo7Uo0sc5OjoCQLM2c3R0xP3791nDWxVBco4tbbft27fHxYsXUVZWhj///JMZWtuUTezs7GBkZMS0W8mLmo4dOzIyAoEAdnZ2yMvLk1uOu7s7Hj58iOrqalZ6YWGh3D7jTcE5ThyvHbFYjPj4eMTHx/9jnBMjIyNmjsH06dPx6NEj5SrEwfGGMTQ0hJ+fHzZt2oTy8nKp/IYPVBMnTsSVK1dw4sQJxMbGYuLEiTLLvHjxIrKysmTmd+nSBWlpaay0ffv2wcrKCrdv30ZSUhLzW7duHcLDw5nhHk5OTrh165ZUmbdu3WIeOlRUVPDJJ59g3759MkNUl5WVyX1x4+HhgeTkZDx9+pRJkzyUN7zhy8PU1BTa2tqIiIiAhoYG+vfvDwCoqKiAiooKy3mWbEv6wri4ONa5L1++HDo6OkhKSkJAQACrnIaoqqoCaNkb/leBh4eH1Feo6Oho1tcieairq8PKygqqqqo4cOAAPvzwQ7lfBUQiEZKTk1lfyCROU2ZmJs6ePYs2bdqw9lm/fj2rLknCpEdEROC7775rUjctLS3Y29ujXbt2zJeBpggODkZKSsorX6ZCkbrY3DXg8/no1q0bS0YsFuPcuXOMTLdu3aCurs6SycjIQF5enkLXsiHffPMNvv/+ezx+/LhF/Yqsc7906RLLMYiOjoaTkxMMDAya1IHH48HCwgJCoRD79+9H27Zt0bVr1xaXq6GhwTg2hw8fZpyw1qJIH+fr6wsjIyO5odclNhs9ejTKysqwefPmJuUaY2trCzMzM9a1fvHiBa5fv67QtdbS0oK5uTmKiooQFRXVpE0ePnyI58+fM+22W7duEAgErBfVtbW1yM3NZc2BbUxSUhIMDAwgEAiYtOzsbFRVVaFLly7N6vxaaVVoircYLqrem6e2tpbOnz9P58+flxvV6k0wZ84c4vP5Sjs+x7vH2x6OPDs7m8zMzJiwwffu3aO0tDT6+eefydnZmZETi8Vkb29PBgYGrPTGjB07ltzd3WXm3blzh9TU1Fihal1dXWnBggVSssXFxcTn85mITtnZ2aShoUFffPEF3b59m+7evUvr1q0jNTU1On36NLPf8+fPydnZmaysrGjXrl2UmppK9+7do507d5K9vT0TdWrDhg2sKHWSENC+vr6UlJREZ86cIWNjY1YI6OvXr5OTkxMrFO+GDRsoISGBMjIyaOPGjSQUCunnn39m8tPT00kgENC0adMoLS2NUlJSaOzYsaSnp0ePHz+WaSdZ0eVCQkJIR0eH9u/fT/fv36c//viD2rdvzwpT3Jqoeg8ePKDExERatmwZaWtrU2JiIiUmJlJpaSkj07dvX9qwYQOzffXqVVJTU6Pvv/+e0tPTKSQkRCoc+cKFC2ncuHHMdkZGBu3Zs4fu3btH169fp1GjRpGhoSHl5OQwMsuWLaOoqCjKzs6mhIQE+uSTT0hDQ4NSU1OJqD5c+9ChQ8nKyoqSkpJYYYvlRT7MyclpdVS9hsiKqkdUH93OxcWFiZw5btw4WrhwIUtGYtNu3brR6NGjKTExkTknIqIjR46worspUhcl4cjnzZtH6enptGnTJpnhyAUCAYWHh1NaWhpNnjyZ9PX1WdH6pk6dSu3ataPz58/TzZs3ycPDgzw8PFj6KxJVj4gdxU/RfqVxPSkuLiZTU1MaN24cpaSk0IEDB0hTU5MVNryxvYiI1qxZQ3fu3KGUlBRavnw5qaurs+q9IuVeu3aNDh8+TNnZ2XTp0iXq27cv2drasq55aWkpcz0B0A8//ECJiYmsqI6No+op2scdO3aM1NXVmXDkOTk5FB8fT/PmzaNRo0Yx+82fP59UVVVp3rx5FBsbS7m5uXT27FkaMWIEE23v4cOH5OTkRNevX2f2W7VqFenr69Px48fpzp075O/vLxWOvHFbP3PmDJ0+fZrpc1xdXcnd3Z1ZSqG0tJTmzp1LcXFxlJOTQ2fPnqWuXbuSg4MDVVVVMeXMnDmTLC0tKSoqiu7evUsTJ04kExMT5l7w22+/0S+//ELJycmUmZlJmzdvJk1NTVqyZAnLZmFhYWRnZydlS0XhwpG3kn+S43QvNYlcz54h17Nn6F5q0t8Z75jj9E/A2tqaCTVuaGiobHU43hHedseJiOjx48cUHBxM1tbWxOfzydLSkoYOHcp6sCQiWrFiBQGgNWvWyCynuLiYhEIhbd++Xe6xevbsSVu3biUiops3bxIAunHjhkzZgQMHUkBAALN948YN6t+/PxkbG5Oenh65u7vLdAqKi4tp4cKF5ODgQHw+n0xNTcnHx4eOHj3KPOCGhIRIORm5ubk0cOBAEgqFZGRkRHPmzGG96JE8PDd82B83bhwZGhoSn8+nzp070+7du6X0+eOPP8jLy4v09PTIwMCA+vbtK3MtKwmyHKfa2lpaunQptW/fnjQ0NKht27b0+eefsx7qWuM4jR8/nukXG/4aXntra2vWelxERAcPHiRHR0fi8/nUqVMnOnnypFS5DR8e09LSyM3NjYRCIenq6pK/vz/dvXuXtc+sWbOoXbt2zDUbNGgQaw0diRPUnL4Ned2OU15eHqmpqTFr0vTp04fGjx/PkpGlb8PrFBYWRo3fYTdXFyU6ubm5EZ/PJzs7O5ZzI2HDhg2MTXv27EnXrl1j5VdWVtLnn39OBgYGpKmpSQEBAZSfn8+SUdRx2r9/PwkEAib0viL9SuN6QkR0+/Ztev/990kgEJClpSWtWrWKlS/LXv/5z39IT0+PNDQ0yN3dnU6dOiWlX3PlxsTEUIcOHUggEFCbNm1o3LhxUssaSOpB41/Da97QcWppHxcfH0/Dhw8nY2NjEggEZG9vT5MnT5ZauiAiIoJ69+5NOjo6pKWlRZ07d6bly5cz9VNS7xvaWiwW0+LFi8nU1JQEAgH169ePMjIyWOU2busRERFkZ2dHfD6fzMzMKDg4mIqLi5n8iooK8vX1JWNjY1JXVydra2uaNGkSyzknqn/pMWfOHDIxMSEdHR3y8fGhlJQUJv/06dPk5uZG2trapKWlRa6urrR161bWUg9ERL6+vkw4/dbwqhwnHtEb/s6vZF68eAE9PT2UlJTIjWD0pshITkCfZ/XDLS4aieDk8v+hesvLAcn8gbIy4CXHpP/bUVVVZQ0RtLGxQU5OjhI14nhXqKqqQk5ODmxtbWUGPuBgc/LkScybNw8pKSktmrjN0TwxMTEICgpCbm6uslX5x2NjY4Pw8HBWKGwO2UhCrL9smPV/C+Hh4QgPD2dCgXO8GlJTU9G3b1/cu3ePmdfaUpq6X7fEN+DuXErE1MoWizKisCgjCqZWtspW57VBRKisrERlZeUbHY9/9+5dqUh5CxYs4JwmDg4lMXjwYEyePJmbU8jBwcHBoTD5+fnYvXt3q52mVwkXjlyJ6BsYYubUBcpW47VTW1uL1atXAwC++uqrN7II7rRp06RCxT548IC1ZgQHB8ebZ9asWcpWgYODg4PjLaLxgs/KhHOcON5JGjpNampqLQ7fycHBwfE2YWNjwzmlCjJr1iwmqipH0wQFBcHNzU3Zarw1uLm5ccMa33G4OU5K5PlfBdh4dBcAYHrAeLQx/v94+u/YHCciYobLNQ7N+7qoqKiAlpYWOnbsiNTU1Nd+PI5/J9wcJw4ODg4Ojn8+3Bynd4BnBY+wxcEPWxz88Kzg3R3zz+PxoKqqClVV1dfmNMXGxoLH42HXrnpHVFNTE0TEOU0cHBwcHBwcHByvBM5xUiKqfAE61mSjY002VPmC5nfgkMm4cePg5eUFANwncg4ODg4ODg4OjtcCN8dJidg7vYfzTu8pW43XTl1dHc6fPw8A6Nu3r0KrsiuKkZERnj9/zmy/icATHBwcHBwcHBwc/z64L04crx2xWIzY2FjExsayQoO/LDwej+U0de3aFdXV1a+sfA4ODg4ODg4ODg4JnOPE8dpRUVGBp6cnPD09X8mil+fPn5eaK7Vx40YkJCS8dNkcHBwcHBwcHBwcsuAcJyWSlZEC97Mn4H72BLIyUpStzmtDTU0Nvr6+8PX1fSXD9LZs2cLaLi8vR3Bw8EuXy8HB8XqpqamBvb09YmNjla3KO0dubu4biVj6LhAUFISlS5cqW423gpiYGC50ewvg2uHrIS0tDVZWVigvL1e2KpzjpExENdV4oGqFB6pWENVwQ8wUJTIyEvr6+tDQ0AARQVNTU9kqcXC8tRQUFOCLL76AnZ0dBAIB2rZtiyFDhiAqKgpGRkZYtWqVzP2+/fZbmJqaMmuk7du3D66urtDU1IS5uTkmTJjAGkoL1K+vZmtrC09PT6nypkyZAlVVVURGRkrlBQUFYdiwYVLpMTEx4PF4KC4uZtJqamqwZs0aRhcjIyN4eXkhLCysyfXcCgsLMWbMGOjq6kJfXx8TJ05EWVmZXHkAyM7ORkBAAIyNjaGrq4uRI0fiyZMnLJnvvvsOnp6e0NTUhL6+vtyywsPD0blzZ2hoaMDExIT1Mmjp0qXg8XhSP61mlqqwsbGR2sfKyorJ3759O7y9vaGrqytlx6aIiYlB165dIRAIYG9vj/Dw8Gb3OXjwINzc3KCpqQlra2usXbtWqkxZ51hQUMCSe/ToEcaOHYs2bdpAKBTCxcUFN2/eZPKDgoKkyhgwYECTuoWHhzOyKioqsLKywqeffoqnT58yMg3L09XVRY8ePXD8+PFmzzsvLw+DBw+GpqYmTExMMG/ePNTV1TW5z61bt9C/f3/o6+ujTZs2mDx5slRdnDFjBrp16waBQCB3nSUiwvfffw9HR0cIBAJYWlriu+++Y/Jl2YrH46FTp05N6qeoLQ4fPgxvb2/o6elBW1sbnTt3xvLly1FYWCi37CNHjsDX1xdt2rQBj8dDUlJSk7pIiIyMhLOzMzQ0NODi4oJTp05J2WLJkiUwNzeHUCiEj48PMjMzWTKK9AF37tzBBx98AA0NDbRt2xZr1qxRSD8AWLlyJVRVVaXqvgR5ffG5c+dYcomJifj4449hamoKDQ0NODg4YNKkSbh3716Tx2/ORrLYtGkTOnToAKFQCCcnJ+zevZuV37DtSH6yluZIT0/H0KFDoaenBy0tLfTo0QN5eXlMvre3t1Q5U6dOZfI7duyIXr164YcffmhW59cO/csoKSkhAFRSUqJsVaio8Dmt3rKSVm9ZSUWFz//OKCsjAup/ZWXKU/AVIRaLqa6ujurq6kgsFrd4//LycgJAampqr0E7Do7WU1lZSWlpaVRZWalsVVpFTk4OWVhYUMeOHenQoUOUkZFBKSkptG7dOnJycqKZM2eSo6Oj1H5isZjs7Oxo7ty5RER05coVUlFRoZ9//pnu379Ply9fpk6dOlFAQABrHwcHB9q/f79UeeXl5aSrq0sLFy6kAQMGSOWPHz+e/P39pdIvXLhAAKioqIiIiKqrq8nb25sMDAxo48aNlJiYSNnZ2bRv3z7q0qULJSYmyrXFgAEDyNXVla5du0aXL18me3t7CgwMlCtfVlZGdnZ2FBAQQHfu3KE7d+6Qv78/9ejRg0QiESO3ZMkS+uGHH2j27Nmkp6cns6x169aRhYUF7du3j7Kysuj27dt0/PhxJr+0tJTy8/NZv44dO9L48eMZmZycHGp8S7e2tqbly5ez9nv69CmT/+OPP9LKlStp5cqVLDs2xf3790lTU5Nmz55NaWlptGHDBlJVVaUzZ87I3efUqVOkpqZGW7ZsoezsbDpx4gSZm5vThg0bGBnJtczIyGDp29CWhYWFZG1tTUFBQXT9+nW6f/8+RUVFUVZWFiMzfvx4GjBgAKuMwsJClj7jx4+nkJAQZjssLIx0dXUpPz+fHj16RKdOnSJTU1Py9fVlZABQWFgY5efnU0ZGBs2cOZPU1NTozp07cs+7rq6O3nvvPfLx8aHExEQ6deoUGRkZ0aJFi+Tu8+jRIzIwMKCpU6fS3bt36caNG+Tp6UkfffQRS+6LL76gjRs30rhx48jV1VVmWV988QU5OTnR8ePH6f79+3Tz5k36448/mPzi4mKWnf78808yNDRk2ebChQtkbW3NKlcRW3z11VekqqpKc+fOpatXr1JOTg798ccfNHz4cPrpp5/knv/u3btp2bJl9MsvvxCAJtushKtXr5KqqiqtWbOG0tLS6JtvviF1dXVKTk5mZFatWkV6enp07Ngxun37Ng0dOpRsbW1ZfXdzfUBJSQmZmprSmDFjKCUlhfbv309CoZC2bdvGyMhqhxLs7e1p4cKF5OzsLJXXXF8s4ffffyc+n09Dhgyh6Ohoun//Pl27do3mzJlDI0eOfCkbNWbz5s2ko6NDBw4coOzsbNq/fz9pa2vTb7/9xsg0bDuSX0FBAaucrKwsMjQ0pHnz5tGtW7coKyuLjh8/Tk+ePGFk+vTpQ5MmTWKV0/g5XdJv1NbWytW5KZq6X7fEN+Acp38i75jjVF1dTSEhIRQSEkLV1dUt2vfIkSMEgPn5+fm9Ji05OFrO2+44DRw4kCwtLalMRj9TVFREd+7cIQB0+fJlVp7kITc9PZ2IiNauXUt2dnYsmfXr15OlpSWzHR8fTyoqKvTixQupY4WHh1OvXr2ouLiYNDU1KS8vj5WvqOO0evVqUlFRoVu3bknJ1tTUyDxPIqK0tDQCQPHx8Uza6dOnicfj0aNHj2TuExUVRSoqKqx7SXFxMfF4PIqOjpaSDwsLk+k4FRYWklAopLNnz8o8jiySkpIIAF26dIlJk+c4/fjjj82W19iOTTF//nzq1KkTK23UqFFN9s2BgYE0YsQIVtr69evJysqKeZmmiA4LFiyg999/v0n95NWVxjKNHafG1+a7774jFRUVqqioIKJ6Z+Ho0aNM/osXLwgA/fzzz3KPc+rUKVJRUWE9SG7ZsoV0dXXl3gu3bdtGJiYmLIdR0g4zMzOl5ENCQmQ6TmlpaaSmpkZ3796Vq19jjh49Sjwej3Jzc5k0eY5TU7a4fv06AZDrIClSzyT1WRHHaeTIkTR48GBWmru7O02ZMoWI6l/amJmZ0dq1a5n84uJiEggEzIscRfqAzZs3k4GBAevaLViwgOXYyHOcYmJiyNLSkmpqasjCwoKuXr3Kym+uLyaqf8FkZGREw4YNk2mHpuzanI1k4eHhwbwckzB79mzy8vJituX1aw0ZNWoUjR07tkmZPn360MyZM5uUqa6uJoFA0KK+siGvynHihupx/GPx8fHB8OHDWWlHjhxRkjYcHC2jpqYGNTU1ICImra6uDjU1NVJDdSSyDaNOikQi1NTUSA0vkyfbUgoLC3HmzBkEBwfLHPKlr68PFxcX9OjRA6Ghoay8sLAweHp6wtnZGQDg4eGBP//8E6dOnQIR4cmTJzh06BAGDRrE7HP58mU4OjpCR0dH6lg7d+7E2LFjoaenh4EDByo09EsW+/btg4+PD7p06SKVp66uzpynZHiJhLi4OOjr66N79+5Mmo+PD1RUVHD9+nWZx6qurgaPx4NA8PcafBoaGlBRUcGVK1cU1jk6OhpisRiPHj1Chw4dYGVlhZEjR+LPP/+Uu8+OHTvg6OiIDz74QOHjtBZvb2/W+nhxcXHw8fFhyfj5+SEuLk5uGdXV1VLDd4RCIR4+fIgHDx6w0t3c3GBubo7+/fvj6tWrrLzffvsN3bt3x8cffwwTExN06dIFv/zyi9TxYmJiYGJiAicnJ0ybNk1qyKgiCIVCiMVimcPq6urqsHPnTgDsJTCWLl3Kmg8UFxcHFxcXmJqaMml+fn548eKF3MXZq6urwefzWYGUhEIhALSoXv3++++ws7PDiRMnYGtrCxsbG3z22WdNDpPbuXMnfHx8YG1trfBxZNli37590NbWxueffy5zH8mQVcl8oJiYGIWPJ4vm6mROTg4KCgpYMnp6enB3d2dkFOkD4uLi0Lt3b9Y19/PzQ0ZGBoqKiprUcefOnQgMDIS6ujoCAwMZmwGK9cUAEBUVhWfPnmH+/Pkyj9FwKLCNjQ1rHt+rbLc3btxg3ZfKyspgbW2Ntm3bwt/fn1W3xWIxTp48CUdHR/j5+cHExATu7u44duyY1PH27dsHIyMjvPfee1i0aBEqKipY+Xw+H25ubrh8+bJcnd8EnOOkRJ7/VYAVW1dhxdZVeP5XQfM7vKWoq6tjwYIFWLBgAdTV1RXaR0tLizWuV1tbm5vPxPFWsWLFCqxYsYLV+cfGxmLFihVSY8vXrl2LFStWoKSkhEm7ceMGVqxYgd9++40l+9NPP2HFihV49uwZk6boPICGZGVlgYgY50ceEydORGRkJDPWv7S0FIcOHcKECRMYGS8vL+zbtw+jRo0Cn8+HmZkZ9PT0sGnTJkbmwYMHsLCwkCo/MzMT165dw6hRowAAY8eORVhYGMvhVJTMzMxmzweof2hycnJitgsKCmBiYsKSUVNTg6GhodQcGwm9evWClpYWFixYgIqKCpSXl2Pu3LkQiUTIz89XWOf79+9DLBZjxYoV+Omnn3Do0CEUFhaif//+qKmpkZKvqqrCvn37MHHiRIXKX7BgAbS1tZnf+vXrFdYNANq1awdzc3Nmu6CggOUIAICpqSlevHiByspKmWX4+fnhyJEjOHfuHMRiMe7du4d169YBAGMrc3NzbN26FYcPH8bhw4fRtm1beHt749atW0w59+/fx5YtW+Dg4ICoqChMmzYNM2bMwK5duxiZAQMGYPfu3Th37hxWr16NixcvYuDAgS16uZCZmYmtW7eie/fuLEc/MDAQ2traEAgE+PLLL2FjY4ORI0cy+UZGRmjfvn2ztpLkyaJv374oKCjA2rVrUVNTg6KiIixcuJBlK0W4f/8+Hjx4gMjISOzevRvh4eFISEjAiBEjZMo/fvwYp0+fxmeffaZQ+U3ZIjMzE3Z2ds3e79XV1eHk5PTS93V5dpbYWPK3OZnm+oDWXE8AePHiBQ4dOoSxY8cCqO/jDh48yPSpivbFkjlZivRx7du3h5GREbPdnI1k4efnhx07diAhIQFEhJs3b2LHjh2ora1l7j9OTk4IDQ3F8ePHsXfvXojFYnh6euLhw4cAgKdPn6KsrAyrVq3CgAED8McffyAgIADDhw/HxYsXmWONHj0ae/fuxYULF7Bo0SLs2bOHsVdDLCwspF62vGk4x0mJPCt4hPVOA7DeaQCeFTxStjqvDR6PB6FQCKFQqFC0GR6Px3rY9PPzQ2lp6etUkYPjX4eijklgYCBEIhEOHjwIAIiIiICKigrj6AD1EY9mzpyJJUuWICEhAWfOnEFubi5rcm9lZaXMScOhoaHw8/NjbvKDBg1CSUkJs2j26zingIAA3L17t8XlN8TY2BiRkZH4/fffoa2tDT09PRQXF6Nr164tWnZBLBajtrYW69evh5+fH3r16oX9+/cjMzMTFy5ckJI/evQoSktLMX78eIXKnzdvHpKSkpjff//7X4V1A4Ddu3dj5cqVLdqnMZMmTcL06dPx4Ycfgs/no1evXvjkk08AgLGVk5MTpkyZgm7dusHT0xOhoaHw9PTEjz/+yJQjFovRtWtXrFixAl26dMHkyZMxadIkbN26lZH55JNPMHToULi4uGDYsGE4ceIE4uPjm/2qUVJSAm1tbWhqasLJyQmmpqbYt28fS+bHH39EUlISTp8+jY4dO2LHjh0wNDRk8qdPny41kb+ldOrUCbt27cK6deugqakJMzMz2NrawtTUtMX1qrq6Grt378YHH3wAb29v7Ny5ExcuXEBGRoaU/K5du6Cvry8zCIssmrKFou3Q0tISd+/eRc+ePRU+r7eR/fv3o3379nB1dQVQ/1XV2toaERERABS3V0teJp07dw7Tp09vubINWLx4MQYOHIhevXpBXV0d/v7+TL8jqYseHh7473//Czc3N/Tp0wdHjhyBsbExtm3bBgDMyAh/f398+eWXcHNzw8KFC/Hhhx+y2u3kyZPh5+cHFxcXjBkzBrt378bRo0eRnZ3N0kkoFEp9iXrTvHxsaI5Wo8oXwK7u/v//b6dkbf6ZHDlyBAEBAcpWg4OjxXz11VcAwHrr6unpiV69ekk9AM2bNw8AWOH6e/bsiW7dukm9bJg1a5aUrLyoWk3h4OAAHo/XrAOhq6uLESNGICwsDBMmTEBYWBhGjhwJbW1tRmblypXw8vJizqNz587Q0tLCBx98gP/9738wNzeHkZERkpOTWWWLRCLs2rULBQUFrPMRiUQIDQ1Fv379GB1kvWUsLi6GqqoqM7zF0dGxVQ6RmZkZK4IaUD8EqbCwEGZmZnL38/X1RXZ2Np49ewY1NTXo6+vDzMwMdnaK9+eSrzkdO3Zk0oyNjWFkZMSKOiVhx44d+PDDD6XeHsvDyMgI9vb2CuvTHGZmZlKRA588eQJdXV1mSFljeDweVq9ejRUrVqCgoADGxsaMg9GUrXr27MkanmZubs6yEwB06NABhw8flluGnZ0djIyMkJWVxdQnWejo6ODWrVtQUVFhIq81xszMDPb29rC3t0dYWBgGDRqEtLQ0qS8VDeVv3LjBSpPYrql6NXr0aIwePRpPnjyBlpYWeDwefvjhhxbXKzU1NTg6OjJpHTp0AFAf6a/hF1ciQmhoKMaNG8cahtYUTdnC0dERV65cQW1trcKjTF4GeXVSYmPJ3ydPnrC+nj558oTpOxXpA+Qdp+ExZLFz506kpqay+jixWIzQ0FBMnDhR4b5Yci3v3r0LDw+PJmUb05yNZCEUChEaGopt27Yxttu+fTt0dHRgbGwscx91dXV06dIFWVlZAOr7HzU1NZnttqmhp+7u7gDqv8Y1/IpbWFjI2lYG3BcnJWLv9B5i+w9FbP+hsHd6T9nqvDbq6upw4cIFXLhwodkwrEB92Eo1NTWUl5dzThPHWwufzwefz2c5PmpqauDz+VLrmUlkGzpUqqqq4PP5Ug8e8mRbiqGhIfz8/LBp0yaZa2M0DE09ceJEXLlyBSdOnEBsbKzUMLGKigopZ1Cik+QtaZcuXXD37l3WW9NTp06htLQUiYmJrK8i+/fvx5EjRxgdnJyckJqaiupq9rINt27dgq2tLWOj0aNH4+zZs0hMTJQ6n9raWrlrgHh4eKC4uJi1iPb58+chFouZG3hTGBkZQV9fH+fPn8fTp08xdOjQZveR4OXlBQCsrwCFhYV49uyZ1FyTnJwcXLhwQeFheq8DDw8Pqa8q0dHRCj3IqaqqwtLSEnw+H/v374eHh4fcBzCgfghqwwddLy8vqa8l9+7da3JOzsOHD/H8+XNWObJQUVGBvb097Ozs5DqADZG82GgY3rsxHh4eSE5OZj2QR0dHQ1dXV+pBUhampqbQ1tZGREQENDQ00L9//2b3keDl5YW6ujrWG3tJuOrG9rp48SKysrJaXa8a22L06NEoKyvD5s2bZcorGvZeUZqrk7a2tjAzM2PJvHjxAtevX2dkFOkDPDw8cOnSJdb8nujoaDg5OcHAwECmbsnJybh58yZiYmJYfVxMTAzi4uJw9+5dhftiX19fGBkZyQ2B3pRdX6bdqqurw8rKCqqqqjhw4AA+/PBDuV8/RSIRkpOTmfbG5/PRo0ePFrdbyfDzxu02JSVF5hzWN0qrQlO8xXBR9d48zUXVEwgEBEAqygwHxz+dtz2qXnZ2NpmZmTEhcO/du0dpaWn0888/s0LmisVisre3JwMDA5mhdMPCwkhNTY02b95M2dnZdOXKFerevTv17NmTkXn27JlU+Ft/f38aNWqUVHkikYjMzMxo48aNRFQfLcrExIRGjhxJN2/epMzMTNq5cyfp6OjQli1bmP2qqqrogw8+YMKRJyUlUXZ2NkVERFDXrl2ZCF1HjhxhRcIiqg9F3KVLF7p+/TpduXKFHBwcWKGIHz58SE5OTnT9+nUmLTQ0lOLi4igrK4v27NlDhoaGNHv2bFa5Dx48oMTERFq2bBlpa2tTYmIiJSYmUmlpKcsOnTp1oqtXr1JycjJ9+OGH1LFjR6qpqWGV9c0335CFhQXV1dVJ2aw1UfXy8/MpMTGRCf186dIlSkxMpOfP/14eY9y4cbRw4UJmWxKOfN68eZSenk6bNm2SCke+YcMG6tu3L7P9119/0ZYtWyg9PZ0SExNpxowZpKGhwbLljz/+SMeOHaPMzExKTk6mmTNnkoqKCiuC1o0bN0hNTY2+++47yszMpH379pGmpibt3buXiOrDts+dO5fi4uIoJyeHzp49S127diUHBweqqqpiylEkql5j0CiSHFF91DyBQEAPHz6Ued6ScOS+vr6UlJREZ86cIWNjY1Y48uvXr5OTkxNThqSchIQEysjIoI0bN5JQKJSK3peZmUmJiYk0ZcoUcnR0ZOqV5B4rEomoa9eu1Lt3b7p16xbdvHmT3N3dqX///lLnNnbsWHJ3d5d53opE1ZNli/nz55OqqirNmzePYmNjKTc3l86ePUsjRoxgou3JalPPnz+nxMREOnnyJAGgAwcOUGJiIuXn5zMyjevk1atXSU1Njb7//ntKT0+nkJAQmeHI9fX16fjx48zSAbLCkTfVBxQXF5OpqSmNGzeOUlJS6MCBA6SpqdlkOPKZM2fKtW3Pnj2ZqHWK9sXHjh0jdXV1Jhx5Tk4OxcfH07x581h9ad++fVnh/hWx0cKFC2ncuHHMdkZGBu3Zs4fu3btH169fp1GjRpGhoSHl5OQwMsuWLaOoqCjKzs6mhIQE+uSTT0hDQ4NSU1MZmSNHjpC6ujpt376dMjMzmSUMJNFas7KyaPny5XTz5k3Kycmh48ePk52dHfXu3Ztlr5ycHKmojy2BC0feSjjH6c1TW1tLJ06coBMnTrDi7//111+sUOP/Qj+e4y3nbXeciIgeP35MwcHBZG1tTXw+nywtLWno0KF04cIFltyKFSsIAK1Zs0ZmOevXr6eOHTuSUCgkc3NzGjNmDOthkKg+JK7kgaegoIDU1NTo4MGDMsubNm0adenShdnOyMiggIAAsrCwIC0tLXJ1daVffvlFam24qqoqWrlyJbm4uJCGhgYZGhqSl5cXhYeHM/1PWFiYVH/z/PlzCgwMJG1tbdLV1aVPP/2U5dxIHoga2mXBggVkampK6urq5ODgQOvWrZPSZ/z48VL9XONySkpKaMKECaSvr0+GhoYUEBAgFZJdJBKRlZUVffXVVzLt1RrHKSQkRKZuYWFhjEyfPn1Y60UR1T9Mu7m5EZ/PJzs7O5a8pNyGD9t//fUX9erVi7S0tEhTU5P69etH165dY+2zevVqat++PXPNvL296fz581I6//777/Tee++RQCAgZ2dn2r59O5NXUVFBvr6+ZGxsTOrq6mRtbU2TJk2SWlfmVTlOYrGYnJ2dadq0aTLPm4goNzeXBg4cSEKhkIyMjGjOnDms+6AkDHvDh9Fx48aRoaEh8fl86ty5M+3evVtKnz59+si8dg3LefToEQ0fPpy0tbXJ1NSUgoKCWE4xUb0zIBQKWXZsiKKOU2NbEBFFRERQ7969SUdHh7S0tKhz5860fPlyJmy2rDYlaZuNfw2vl6w6efDgQXJ0dCQ+n0+dOnWikydPSum3ePFiMjU1JYFAQP369aOMjAyWTHN9ABHR7du36f333yeBQECWlpa0atUqVn7DdlhdXU1t2rSR22euXr2aTExMmBckivbF8fHxNHz4cDI2NiaBQED29vY0efJkVrh6a2trls0UsdH48eOpT58+zHZaWhq5ubmRUCgkXV1d8vf3lwpvP2vWLGrXrh3x+XwyNTWlQYMGyVwOYufOnWRvb08aGhrk6upKx44dY/Ly8vKod+/eZGhoyJzPvHnzpJ7TV6xY8VJL0rwqx4lH1IrQRW8xL168gJ6eHkpKSqCrq6tUXbIyUjDhQX2UlFBrh7+H65WXA5L5A2VlQDOrw7+N/PDDD5gzZw4r7dy5c+jbt6+SNOLgaDlVVVXIycmBra2tzMAHHGzu3LmD/v37Izs7mzVHiuPlyc3Nha2tbauiEf7bCAoKkgrXzCGbmJgYBAUFITc3V9mqvBVw7fD1UFNTAwcHB/z666/M8OaW0tT9uiW+ARccQomIaqpxT92W+f/fgqurK+7cucNs83g81po0HBwc7yadO3fG6tWrkZOTAxcXF2Wrw8HBwcHxFpCXl4evvvqq1U7Tq4RznJSIkZklpsfWrz9h9J5ioWXfdjQ1NVlrfRgbG0tFsuHg4Hh3abiYKgcHBwcHR3NIIjj+E+AcJyXSxtgM30xZoGw1Xjs1NTVYtWoVgProWJJoKUFBQQgLC1OiZhwcHBzvBvr6+ggJCVG2Gm8Fw4YNg76+vrLVeCuwsbFhlkDgaB6uHb77cHOc/om8Y3OckpOTmXU2vvrqKzg4OODo0aPo2rWrkjXj4Hg5uDlOHBwcHBwc/3y4OU7vAMVFhQiN2A4AmDBqMvQNDJvZ4+3D2dkZGRkZMDAwQG5uLtTU1GQuZMnBwcHBwcHBwcHxT4ZznJTIk4c5WOM0AAAw+GHOO+c4qaurMwveFhUVQU1NTe6iaRwcHBwcHBwcHBz/ZLinWCWioqoGK9FjWIkeQ0X13fFh8/LywOPxGKcJAKZOnQpNTU0lasXBwcHBwcHBwcHRet6dp/W3EIeOrrjZ0VXZarxS5s6di3Xr1rHSUlJSUFxcjKtXr8Ld3R1qaly14+Dg4ODg4ODgeLvgnmA5XhnTpk3D1q1bmW0VFRWIRCLU1NRgxYoVAIAePXooSz0ODg4ODg4ODg6OVsMN1eN4ZTT80mRjYwORSASg3oFydXWFq6srN8eJg4ODg4ODg4PjrYR7ilUiWRkp6H/mIPqfOYisjBRlq9Mqbt26hV9++QVA/eK2+/fvx4IFC5CTk8PIqKmpISAgAAEBAdwwPQ6OfzE1NTWwt7dHbGysslV55wgPD4e3t7ey1Xgr8Pb2Rnh4uLLVeCtYunQpt2h1C+Da4eth69atGDJkiLLVAMA5TkpFVFONZIEjkgWOENVUK1udFjNp0iR069YNkydPxrNnzwAAn3zyCbPYLQcHxz+fgoICfPHFF7Czs4NAIEDbtm0xZMgQREVFwcjISG57/vbbb2Fqaora2loAwKZNm9ChQwcIhUI4OTlh9+7dUvts3boVtra28PT0lMqbMmUKVFVVERkZKZUXFBSEYcOGSaXHxMSAx+OhuLiYSaupqcGaNWvg6uoKTU1NGBkZwcvLC2FhYYyusigsLMSYMWOgq6sLfX19TJw4EWVlZXLlASA7OxsBAQEwNjaGrq4uRo4ciSdPnsiUra6uhpubG3g8HrMIOFD/YMrj8aR+Wo3W74uMjISzszM0NDTg4uKCU6dONalbbm6uzHLHjh3LyMyYMQPdunWDQCCAm5tbk+U1ZNOmTbCxsYGGhgbc3d1x48aNJuVra2uxfPlytG/fHhoaGnB1dcWZM2dYMpcuXcKQIUNgYWEBHo+HY8eOySwrPT0dQ4cOhZ6eHrS0tNCjRw/k5eU1ec48Hk9mvZLQ8BqoqanBxsYGX375JXP9G5draGiIPn364PLly83a6s6dO/jggw+goaGBtm3bYs2aNc3uEx8fj379+kFfXx8GBgbw8/PD7du3ZcpmZWVBR0dH5oK+xcXFCA4Ohrm5OQQCARwdHVn1ZuXKlejRowd0dHRgYmKCYcOGISMjo0ndJG1O8jM2NsagQYOQnJzMkmttO/zuu+/g6ekJTU1NhRcpJiIsWbIE5ubmEAqF8PHxQWZmJktGkfatyLVqaTtsiJ+fH1RVVREfHy8zPzExER9//DFMTU2hoaEBBwcHTJo0Cffu3WPJHT58GN7e3tDT04O2tjY6d+6M5cuXo7CwUO6xFbFRY0pLSzFr1ixYW1tDKBTC09NTSvcjR47A19cXbdq0kerbJEyZMgXt27eHUCiEsbEx/P39cffuXZZMXl4eBg8eDE1NTZiYmGDevHmsAGMTJkzArVu3FGpzrxvOcVIiRmaWmJz9ByZn/wEjM0tlq9MizMzMsGPHDmb7gw8+UKI2HBwcrSE3NxfdunXD+fPnsXbtWiQnJ+PMmTP4z3/+g5kzZ2Ls2LEICwuT2o+IEB4ejv/+979QV1fHli1bsGjRIixduhSpqalYtmwZgoOD8fvvv7P22bhxIyZOnChVXkVFBQ4cOID58+cjNDS01edTU1MDPz8/rFq1CpMnT0ZsbCxu3LiB4OBgbNiwAampqXL3HTNmDFJTUxEdHY0TJ07g0qVLmDx5slz58vJy+Pr6gsfj4fz587h69SpqamowZMgQiMViKfn58+fDwsJCKn3u3LnIz89n/Tp27IiPP/6YkYmNjUVgYCAmTpyIxMREDBs2DMOGDUNKSvMjFc6ePcsqe9OmTaz8CRMmYNSoUc2WIyEiIgKzZ89GSEgIbt26BVdXV/j5+eHp06dy9/nmm2+wbds2bNiwAWlpaZg6dSoCAgKQmJjIyJSXl8PV1VVKv4ZkZ2fj/fffh7OzM2JiYnDnzh0sXryYWcyybdu2UrZctmwZtLW1MXDgwCbPq1OnTsjPz0dubi5Wr16N7du3Y86cOSwZiS0vXboECwsLfPjhh3IdZaB+UU1fX19YW1sjISEBa9euxdKlS7F9+3a5+5SVlWHAgAFo164drl+/jitXrkBHRwd+fn5SDkdtbS0CAwNl3n9ramrQv39/5Obm4tChQ8jIyMAvv/wCS8u/nzUuXryI4OBgXLt2DdHR0aitrYWvry/Ky8ubtBUAZGRkID8/H1FRUaiursbgwYNRU1PDHLu17bCmpgYff/wxpk2b1qwOEtasWYP169dj69atuH79OrS0tODn54eqqipGprn2rci1epl2mJeXh9jYWEyfPl1mH3fixAn06tUL1dXV2LdvH9LT07F3717o6elh8eLFjNzXX3+NUaNGoUePHjh9+jRSUlKwbt063L59G3v27HkpGzXms88+Q3R0NPbs2YPk5GT4+vrCx8cHjx49YmTKy8vx/vvvY/Xq1XLL6datG8LCwpCeno6oqCgQEXx9fZnpHCKRiKk/sbGx2LVrF8LDw7FkyRKmDD6fj9GjR2P9+vVyj/PGoH8ZJSUlBIBKSkqUrYp8ysqIgPpfWZmytZGCx+MRAObn7OzcpHx1dTWtWLGCVqxYQdXV1W9ISw6O109lZSWlpaVRZWWlVF5dXTnV1ZWTWCxm0kSiaqqrKyeRqEqOrKiBbM3/pysm2xoGDhxIlpaWVCajnykqKqI7d+4QALp8+TIr78KFCwSA0tPTiYjIw8OD5s6dy5KZPXs2eXl5Mdvx8fGkoqJCL168kDpWeHg49erVi4qLi0lTU5Py8vJY+ePHjyd/f3+p/SR6FBUVERHR6tWrSUVFhW7duiUlW1NTI/M8iYjS0tIIAMXHxzNpp0+fJh6PR48ePZK5T1RUFKmoqLDuJcXFxcTj8Sg6Opole+rUKXJ2dqbU1FQCQImJiTLLJCJKSkoiAHTp0iUmbeTIkTR48GCWnLu7O02ZMoXZDgsLoz59+jDbOTk5zR5LQkhICLm6ujYrR0TUs2dPCg4OZrZFIhFZWFjQypUr5e5jbm5OGzduZKUNHz6cxowZI1MeAB09elQqfdSoUTR27FiF9JTg5uZGEyZMYKX16dOHwsLCmG1Z5z9p0iQyMzMjItm2lLSN48ePyz325s2bycDAgHXfW7BgATk5OcndJz4+ngCw2oDkWJmZmSzZ+fPn09ixYyksLIz09PRYeVu2bCE7OzuqqVG8b3j69CkBoIsXLzJpISEhNH78eGa7cZsjIvrtt98IAN2+fZuIWt8OGyLrnGQhFovJzMyM1q5dy6QVFxeTQCCg/fv3E5Fi7VuRa9Wadihh6dKl9Mknn1B6ejrp6elRRUUFk1deXk5GRkY0bNgwmecosfX169cJAP30009NyjVGERs1pqKiglRVVenEiROs9K5du9LXX38tJd+S/ub27dsEgLKysoiovn9UUVGhgoICRmbLli2kq6vLuh4XL14kPp/Psl1LaOp+3RLfgPvixKEwsbGx4PF4ICImbeXKlUhPT2923+rqalRXv33DETk4WkvMRRfEXHRBbe3fwyce5P2CmIsuyMhYypK9dLknYi66oKrqMZP28NFexFx0QfrdhSzZq7F9EHPRBeXlWUxafv7hFutXWFiIM2fOIDg4WGpYGADo6+vDxcUFPXr0kHpDGhYWBk9PTzg7OwOob9+St/4ShEIhbty4wbwlv3z5MhwdHaGjoyN1rJ07d2Ls2LHQ09PDwIEDWz3/ZN++ffDx8UGXLl2k8tTV1ZnzDA8PB4/HY/Li4uKgr6+P7t27M2k+Pj5QUVHB9evXZR6ruroaPB4PAoGASdPQ0ICKigquXLnCpD158gSTJk3Cnj17FFrLbseOHXB0dGR9RYiLi4OPjw9Lzs/PD3Fxcc2W97LweDzmetTU1CAhIYGli4qKCnx8fJrURV79aGin5hCLxTh58iQcHR3h5+cHExMTuLu7yx3SBwAJCQlISkqS+ZWzOYRCIfMFpTGVlZXMUFQ+n8+kBwUFsea3xMXFoXfv3iwZPz8/ZGRkoKioSGbZTk5OaNOmDXbu3ImamhpUVlZi586d6NChA2xsbBi58+fPIzIyUu4Xut9++w0eHh4IDg6Gqakp3nvvPaxYsYJ5yy+LkpISAIChoaFcGVn7HDhwAMDftmhtO2wNOTk5KCgoYNVJPT09uLu7M3VSkfatyLVqbTskIoSFhWHs2LFwdnaGvb09Dh06xORHRUXh2bNnmD9/vsz9JUMW9+3bB21tbXz++edNykmGlsbExChso8bU1dVBJBK9dLttTHl5OcLCwmBra4u2bdsCqLeri4sLTE1NGTk/Pz+8ePGC9XWye/fuqKurk9snvyk4x0mJFBcVYtO2tdi0bS2Ki+SPTf2n4OXlxdr+66+/sHDhQjnSf6OmpoYvvvgCX3zxBRccgoPjH0JWVhaIiHF+5DFx4kRERkYy8wFKS0tx6NAhTJgwgZHx8/PDjh07kJCQACLCzZs3sWPHDtTW1jLzHx88eCBzqFpmZiauXbvGDBeTDA9s+IJGUTIzM5s9H6D+ocHJyYnZLigogImJCUtGTU0NhoaGKCgokFlGr169oKWlhQULFqCiogLl5eWYO3cuRCIR8vPzAdQ/MAUFBWHq1KmshzZ5VFVVYd++fVIP+gUFBayHCgAwNTWVq1tDPD09oa2tzfwaDo9TBCcnJ+jp6QEAnj17BpFI1GJd/Pz88MMPPyAzMxNisRjR0dE4cuQIYydFePr0KcrKyrBq1SoMGDAAf/zxBwICAjB8+HBcvHhR5j4Sh0PWnLqmSEhIwK+//oq+ffuy0iW21NLSwvfff49u3bqhX79+TL65uTnatWvHbMu7bpI8Wejo6CAmJgZ79+6FUCiEtrY2zpw5g9OnTzP3z+fPnyMoKAjh4eHQ1dWVWc79+/dx6NAhiEQinDp1CosXL8a6devwv//9T6a8WCzGrFmz4OXlhffee68ZCwFWVlbQ1taGvr4+fv31VwwdOpRpe61th61BYsem6qQi7VuRa9Xadnj27FlUVFTAz88PQH0ft3PnTiZfMteoOZtlZmbCzs4O6urqTcqpq6vDycmJeVGjiI0ao6OjAw8PD3z77bd4/PgxRCIR9u7di7i4uBa1WwmbN29m+qDTp08jOjqacVIVbSeamprQ09PDgwcPWnz8VwnnOCmRJw9z8K1jf3zr2B9PHuY0v4OSkVRkPp8PIoKRkZFC+6moqKBNmzZo06YNF46c41+Dd59kePdJhrr6329vrdtNgnefZDg5LWXJ9v7gBrz7JEND42/HwspyLLz7JKODMzs4g5fnRXj3SYaWlj2TZm7+UYv1U9QxCQwMhEgkwsGDBwHUz3FRUVFhzYtZvHgxBg4ciF69ekFdXR3+/v4YP348ADBtvrKyUurtJQCEhobCz8+P6U8GDRqEkpISnD9//rWdU0BAgNTk5JZibGyMyMhI/P7779DW1oaenh6Ki4vRtWtX5pw3bNiA0tJSLFq0SKEyjx49itLSUsZ2r4KIiAgkJSUxv44dO7Zo/7t37yIgIOCldPj555/h4OAAZ2dn8Pl8TJ8+HZ9++mmL7geSeWP+/v748ssv4ebmhoULF+LDDz9krR8oobKyEr/++qvCX5uSk5Ohra0NoVCInj17wsPDAxs3bmTJREREIDExEYcPH4a9vT3Cw8NZD7ErV66UGRSlJVRWVmLixInw8vLCtWvXcPXqVbz33nsYPHgwKisrAdQHZho9ejR69+4ttxyxWAwTExNs374d3bp1w6hRo/D111/LtBUABAcHIyUlhfl61ByXL19GQkICwsPD4ejoyCr3TbbDt4HQ0FCMGjWKcXwDAwNx9epVZGdnA1DcXorKWVpa4u7du+jZs2frFP5/9uzZAyKCpaUlBAIB1q9fj8DAwFY9x40ZMwaJiYm4ePEiHB0dMXLkyCbnV8lDKBSioqKixfu9SrinWCWioqoGI/EzGImfQUX1n/clpqKiAt26dWO2CwoKcOTIEW7IHQeHAqiqakJVVZM1FEVFhQ9VVU2oqAjkyKo0kFX//3TFZFuKg4MDeDxesw8uurq6GDFiBBMkIiwsDCNHjoS2tjYjIxQKERoaioqKCuTm5iIvLw82NjbQ0dGBsbExAMDIyEhqeJJIJMKuXbtw8uRJqKmpQU1NDZqamigsLGQND9TV1WWGETWkuLgYqqqqzNAfR0fHVj2ImZmZSQU3qKurQ2FhIczMzOTu5+vri+zsbDx9+hTPnj3Dnj178OjRI9jZ2QGoH04VFxcHgUAANTU12NvXO7vdu3eX6Rzt2LEDH374odTbVzMzM6kgBE+ePGlSNwlt27aFvb0982s4tLClGBkZQVVVtcW6GBsb49ixYygvL8eDBw9w9+5daGtrM3ZS9NhqampSjl+HDh2YqHoNOXToECoqKvDf//5XofKdnJyQlJSE9PR0VFZW4rfffpO6Dm3btoWDgwMCAgKwYsUKBAQENHk/lHfdJHmy+PXXX5Gbm4uwsDD06NEDvXr1wq+//oqcnBwcP34cQH29+v7775k2M3HiRJSUlEBNTY1pN+bm5nB0dISqqipTdocOHVBQUCA1BHH69Ok4ceIELly4ACsrK4XsZWtrCycnJ4wfPx6fffYZ60VKa9tha5DYsak6qUj7VuRataYdFhYW4ujRo9i8eTNzvSwtLVFXV8dcK0dHRwBo1maOjo64f/9+k1EJZaGIjWTRvn17XLx4EWVlZfjzzz+ZodctabcS9PT04ODggN69e+PQoUO4e/cujh49yuinaDspLCxk7inKgnOclIhDR1ek9PNBSj8fOHR0VbY6LE6fPg0tLS3cunWLFYWnNW8eRSIRbty4gRs3bjQ5vpqDg+PNYWhoCD8/P2zatElmFK2GIb4nTpyIK1eu4MSJE4iNjZX7Fl9dXR1WVlZQVVXFgQMH8OGHHzJvJ7t06YK7d++y3pqeOnUKpaWlSExMZH0V2b9/P44cOcLo4OTkhNTUVKmH1Fu3bsHW1pZ56z969GicPXtW5nC02tpaudHCPDw8UFxcjISEBCbt/PnzEIvFcHd3l7lPQ4yMjKCvr4/z58/j6dOnGDp0KABg/fr1uH37NnNektDFERER+O6771hl5OTk4MKFCzJt6+HhgXPnzrHSoqOj4eHh0axurxI+n49u3bqxdBGLxTh37pxCumhoaDAPjYcPH4a/v3+Ljt2jRw+pcNn37t2DtbW1lPzOnTsxdOhQhR+y+Hw+7O3tYWNjw5rnIo8RI0ZATU0Nmzdvlivj4eGBS5cusR50o6Oj4eTkBAMDA5n7VFRUQEVFpdELl/ptyVe3uLg4VntZvnw5dHR0kJSUxNyjvby8kJWVxYrweO/ePZibmzPnR0SYPn06jh49ivPnz8PW1rbZ85aF5GuV5EG4te2wNdja2sLMzIxVJ1+8eIHr168zdVKR9q3ItWpNO9y3bx+srKxY/UBSUhLWrVuH8PBwiEQi+Pr6wsjISG6oekk/OHr0aJSVlcmtcw377JbaqCm0tLRgbm6OoqIiREVFtajdyoKIQERMf+7h4YHk5GSWcxsdHQ1dXV3Wi5Ls7GxUVVXJnDv3RmlVaIq3GC6qXvMMGTKEFTXvZatJdXU1hYSEUEhICBdVj+OdoqkoPW8D2dnZZGZmRh07dqRDhw7RvXv3KC0tjX7++WdWtEyxWEz29vZkYGAgM4pmRkYG7dmzh+7du0fXr1+nUaNGkaGhIeXk5DAyz549I3V1dUpOTmbS/P39adSoUVLliUQiMjMzYyKxFRUVkYmJCY0cOZJu3rxJmZmZtHPnTtLR0aEtW7Yw+1VVVdEHH3xABgYGtHHjRkpKSqLs7GyKiIigrl27MhGfjhw5IhXZbMCAAdSlSxe6fv06XblyhRwcHCgwMJDJf/jwITk5OdH169eZtNDQUIqLi6OsrCzas2cPGRoa0uzZs+Xau6nIU9988w1ZWFhQXV2dVN7Vq1dJTU2Nvv/+e0pPT6eQkBApW7Ymql5mZiYlJibSlClTyNHRkRITEykxMZHVTzs5OdGRI0eY7QMHDpBAIKDw8HBKS0ujyZMnk76+Pisi1rhx42jhwoXM9rVr1+jw4cOUnZ1Nly5dor59+5KtrS0rClhpaSlzfAD0ww8/UGJiIj148ICROXLkCKmrq9P27dspMzOTNmzYQKqqqlJRHzMzM4nH49Hp06dlnrciUfUaIs+WmzdvJhMTEyovLyciooULF9K4ceOY/OLiYjI1NaVx48ZRSkoKHThwgDQ1NWnbtm2sc2pYF9PT00kgENC0adMoLS2NUlJSaOzYsaSnp0ePHz+WqZ+sCHR5eXmko6ND06dPp4yMDDpx4gSZmJjQ//73P0Zm2rRppKenRzExMZSfn8/8GkYtUySqHlF9hD8XFxcSi8Uv1Q4fPHhAiYmJtGzZMtLW1mbqRGlpKSPTuE6uWrWK9PX16fjx43Tnzh3y9/cnW1tbVr/cXPtW5Fq1ph26urrSggULpK5ZcXEx8fl8JmrdsWPHSF1dnYYMGULR0dGUk5ND8fHxNG/ePFYfOX/+fFJVVaV58+ZRbGws5ebm0tmzZ2nEiBFMtD1ZfZUiNurbty9t2LCB2T5z5gydPn2a7t+/T3/88Qe5urqSu7s7K1Lj8+fPKTExkU6ePEkA6MCBA5SYmEj5+flEVH+PWbFiBd28eZMePHhAV69epSFDhpChoSE9efKEiIjq6urovffeI19fX0pKSqIzZ86QsbExLVq0iGWzsLAwsrOzk7KloryqqHqc46RsxOJ656jh78kTpTlOOjo6LIdJKBS+dJk1NTUUERFBERERLQqNysHxT+dtd5yIiB4/fkzBwcFkbW1NfD6fLC0taejQoXThwgWW3IoVKwgArVmzRqqMtLQ0cnNzI6FQSLq6uuTv7093796Vkhs5ciTzQF1QUEBqamp08OBBmXpNmzaNunTpwmxnZGRQQEAAWVhYkJaWFrm6utIvv/zCCvdOVO88rVy5klxcXEhDQ4MMDQ3Jy8uLwsPDqba2lojqb8CNXwg9f/6cAgMDSVtbm3R1denTTz9lPaxJHp4b2mXBggVkampK6urq5ODgQOvWrZPSpyHyHsBFIhFZWVnRV199JXffgwcPkqOjI/H5fOrUqROdPHmSld8ax6lPnz5SL8kAsBxeACwng4how4YN1K5dO+Lz+dSzZ0+6du2aVLkNH7ZjYmKoQ4cOJBAIqE2bNjRu3DipMO+SB/LGv4blEBHt3LmT7O3tSUNDg1xdXenYsWNS57Vo0SJq27YtiUQiqTyJfq/CcSovLycDAwNavXo1EdWHzW8civr27dv0/vvvk0AgIEtLS1q1ahUrX1Zd/OOPP8jLy4v09PTIwMCA+vbtS3FxcXL1kxe6OzY2ltzd3UkgEJCdnR199913LMdclr0bX29FHae8vDxSU1OjiIgIImp9Oxw/frxMnRq2u8Y6isViWrx4MZmampJAIKB+/fpRRkYGq9zm2jdR89eKqGXt8ObNmwSAbty4IVUOUf1yEAEBAcx2fHw8DR8+nIyNjUkgEJC9vT1NnjxZKgx9REQE9e7dm3R0dEhLS4s6d+5My5cvZ66JrL5KERtZW1tTSEgI6zh2dnbE5/PJzMyMgoODqbi4WOp8ZV0vSTmPHj2igQMHkomJCamrq5OVlRWNHj1a6v6Qm5tLAwcOJKFQSEZGRjRnzhymnkjw9fVtctmD5nhVjhOPqBWhi95iXrx4AT09PZSUlMiNRvOmyMlMR6W/PzqmN7F6c1kZICNU8KumoqJCKiTxBx98gEuXLr32Y3NwvK1UVVUhJycHtra2MgMfcLC5c+cO+vfvj+zsbNYcKY6XJzw8HOHh4UwIYg75eHt7IygoCEFBQcpW5R/P0qVLkZub2+olAv5tcO3w9ZCamoq+ffvi3r17TJTPltLU/bolvgE3x0mJ1BY+b9pp8vICFFj341XQuCLu37+fc5o4ODheKZ07d8bq1auRk/PPjyLKwcHBwfHPID8/H7t372610/Qq+eeFcvsXoW/8d7SQv5Jvw9i2PVtAUxN4ycXhFCU7O5uZYFteXq7QQo0cHBwcLYV7y8/BwcHB0RIaLzysTDjHSYmYmJoz/xvbtn8jQ/IaYmhoiH379mHgwIFo167da3OYampqsGHDBgDAF198oVDEIg4ODg4OxXFzc+OcUgUJCgqCm5ubstV4K/D29pYbrY1DGq4dvvtwjtO/kIbzmQYNGsQ4TK/zK1NpaelrK5uDg4Pj346bmxvnDCgI92CrON7e3spW4a2Ca4fvPtwcJyVSXvpC5v+vk127dkkFgZC1eOCrRE1NDVOmTMGUKVOYlbM5ODg4ODg4ODg43ia4p1gl8uhBNhwb/m9m3qT8y9KrVy9cv36dlfYm5jOpqKjA3Pz1nhsHBwcHBwcHBwfH64RznP4lCIVCVFVVMdv6+vooKipSokYcHBwcHBwcHBwcbw/cUD0l4vheF5n/v2p++eUXltM0YsSIN+o0iUQiJCYmIjExESKR6I0dl4ODg4ODg4ODg+NVwTlO/wImTZoEoVAIADh37hwiIyPf6PFFIhGOHz+O48ePc44TBwcHBwcHBwfHWwnnOL2juLq6Qltbm9muqKgAEaFv375vXBcejwcHBwc4ODiA94bWpeLg4Hg7ycjIgJmZGReJ8zUQFBSEpUuXKluNfzwxMTGwsbFRthpvBbm5udx9vYXweDzk5uYqW413irS0NFhZWaG8vPy1H4tznJRIbnaGzP9fFj6fjzt37qC8vBzdunV7ZeW2FnV1dYwZMwZjxoyBurq6stXh4OBoQEFBAb744gvY2dlBIBCgbdu2GDJkCKKiomBkZIRVq1bJ3O/bb7+FqakpamtrkZ+fj9GjR8PR0REqKiqYNWuWzH0iIyPh7OwMDQ0NuLi44NSpU1IyixYtwhdffAEdHR2pPGdnZwgEAhQUFEjl2djY4KeffpJKX7p0qVR4YHnnfO7cOZl6A0BVVRWCgoLg4uICNTU1DBs2TK5sQwoLCzFmzBjo6upCX18fEydORFlZGUvmzp07+OCDD6ChoYG2bdtizZo1UuU0ZzsiwpIlS2Bubg6hUAgfHx9kZmY2qVt4eDh4PJ7Ub8eOHQCg8HVtzPbt2+Ht7Q1dXV3weDyF1wHatGkTbGxsoKGhAXd3d9y4cYOVX1VVheDgYLRp0wba2tr46KOP8OTJE5ZMXl4eBg8eDE1NTZiYmGDevHmoq6tjycTExKBr164QCASwt7dHeHh4s7o1tI+enh68vLxw/vx5Jj8oKIjJV1dXh62tLebPn88aJi+LI0eOwNfXF23atAGPx0NSUlKzugCvpj68qrrZGBsbG8YWmpqacHFxYepUQy5cuIBBgwahTZs20NTURMeOHTFnzhw8evRIbtmXLl3CkCFDYGFhAR6Ph2PHjjWrD6DYNX9T9U8e+/fvh6qqKoKDg2Xmv3jxAl9//TVz3c3MzODj44MjR46AiBi5rKwsfPrpp7CysoJAIICtrS0CAwNx8+ZNucdubVt/Ve3tZW3fsWNH9OrVCz/88INCer8U9C+jpKSEAFBJSYmyVaGMa5eJACKg/v+X5K+//iIArF9QUNAr0JSDg0MWlZWVlJaWRpWVlcpWpVXk5OSQhYUFdezYkQ4dOkQZGRmUkpJC69atIycnJ5o5cyY5OjpK7ScWi8nOzo7mzp3LlDNjxgzatWsXubm50cyZM6X2uXr1KqmqqtKaNWsoLS2NvvnmG1JXV6fk5GRG5sGDB6Surk4PHz6U2v/y5cvUrl07Gj16NK1atUoq39ramn788Uep9JCQEHJ1dVX4nOVRVlZGU6dOpe3bt5Ofnx/5+/vLlW3IgAEDyNXVla5du0aXL18me3t7CgwMZPJLSkrI1NSUxowZQykpKbR//34SCoW0bds2RkYR261atYr09PTo2LFjdPv2bRo6dCjZ2tqy6ub48eMpJCSE2Q4LCyNdXV3Kz89n/SoqKhhbNXddZfHjjz/SypUraeXKlQSAioqKmt3nwIEDxOfzKTQ0lFJTU2nSpEmkr69PT548YWSmTp1Kbdu2pXPnztHNmzepV69e5OnpyeTX1dXRe++9Rz4+PpSYmEinTp0iIyMjWrRoESNz//590tTUpNmzZ1NaWhpt2LCBVFVV6cyZM4zMhQsXyNramqUfAAoLC6P8/HxKTk6moUOHklAopOzsbMa2AwYMoPz8fMrLy6OjR4+Srq4uzZ8/v8nz3r17Ny1btox++eUXAkCJiYnN2upV1YdXUTdzcnKo8aOktbU1LV++nPLz8yk7O5tWrVpFAOjUqVOMzNatW0lFRYU+/fRTunDhAuXk5NDFixdp4sSJ9OWXX8o991OnTtHXX39NR44cIQB09OjRZu2lyDV/U/WPqL4u5eTkSOnZr18/WrhwIRkYGEjdU4qKiqhTp05kZWVF4eHhlJqaShkZGbR9+3Zq374908bi4+NJV1eXPD096cSJE5SVlUWJiYm0dOlS6t27t1wbtaatv6r29ipsT0R04sQJMjc3p9raWpn6NnW/bolvwDlOSuRJdibjOD3JznypspYvXy7lNCUkJLwiTTk4OGTRVEdcVldHZXV1JBaLmbRqkYjK6uqoSiSSKStqIFsjElNZXR1V1ikm2xoGDhxIlpaWVFZWJpVXVFREd+7cIQB0+TL7xc6FCxcIAKWnp0vt16dPH5k33ZEjR9LgwYNZae7u7jRlyhRme+3atdS9e3eZugYFBdHChQvp9OnTMp05RR2n5s5ZEcaPH6+Q45SWlkYAKD4+nkk7ffo08Xg8evToERERbd68mQwMDKi6upqRWbBgAcuJa852YrGYzMzMaO3atUx+cXExCQQC2r9/P0vvxo6Tnp6eQucs77o2haSeKGLXnj17UnBwMLMtEonIwsKCVq5cSUT156Ourk6RkZGMTHp6OgGguLg4Iqp/qFZRUaGCggJGZsuWLaSrq8vYd/78+dSpUyfWsUeNGkV+fn4svWU5Tg0f0h89ekQAaOvWrUQku04MHz6cunTp0uy5E/3tgCjiOL2K+vCq6qY8x6lxWzQ0NGQcoj///JP4fD7NmjVL5vkp2g4VdZwUueZvqv5J9G7sON2/f5+EQiEVFxeTu7s77du3j5U/bdo00tLSYq5NQ0pLS6m2tpbEYjF16tSJunXrRqJG9xgixe2qaFt/Ve3tVdieiKi6upoEAgGdPXtWpr6vynHihuopERNTc5n/txQHBwcsWbKE2ebxeCAidO3a9aX0e1XU1NRg/fr1WL9+PWpqapStDgfHG6H9pWS0v5SM57V/B0TZnPcU7S8l46t7D1my711JRftLyXhY9Xf7CHv0F9pfSsbsjD9Zsj3i0tD+UjLuVfw9BCiioLDF+hUWFuLMmTMIDg6WWhQbqF+ywMXFBT169EBoaCgrLywsDJ6ennB2dlb4eHFxcfDx8WGl+fn5IS4ujtm+fPkyunfvLrVvaWkpIiMjMXbsWPTv3x8lJSW4fPmywseWoMg5SwgKCoK3t3eLj9GQuLg46Ovrs87Jx8cHKioqzJp6cXFx6N27N/h8PiPj5+eHjIwMJvppc7bLyclBQUEBS0ZPTw/u7u4s+74OYmJiXnrORk1NDRISElj6q6iowMfHh9E/ISEBtbW1LBlnZ2e0a9eOkYmLi4OLiwtMTU0ZGT8/P7x48QKpqamMTHP1UBEkAZfk3dNSUlIQGxvLuq6S+UAxMTEtOlZjXkV9eFV1sznEYjEOHz6MoqIippzIyEjU1NRg/vz5Mvdp2A55PJ5CQymbojl7vcn6J4+wsDAMHjwYenp6GDt2LHbu3MnkicViHDhwAGPGjIGFhYXUvtra2lBTU0NSUhJSU1MxZ84cqKhIP943tKu3tzeCgoKa1Kk5XkV7e1W2B+qnqbi5ubXq3tASOMfpHSArK4v538LCAmKxWInayKawsBCFhS1/uOPg4Hg9ZGVlgYiadX4mTpyIyMhIZu5DaWkpDh06hAkTJrToeAUFBawbLACYmpqy5is9ePBA5oPBgQMH4ODggE6dOkFVVRWffPIJ68FCURQ9ZwAwNzdHu3btWnyMhhQUFMDExISVpqamBkNDQ+a85dlFkteUTMP8hvvJkpFHSUkJtLW1mZ+ZmVlLThGamppwcnJ6qfmrz549g0gkavYc+Xw+6+FPlkxrbfnixQtUVlYqpG9FRQW++eYbqKqqok+fPkz6iRMnoK2tzcw7evr0KebNm8fkq6urw8nJ6aUXnX8V9eFV1U15LFiwANra2hAIBBgxYgQMDAzw2WefAQAyMzOhq6sLc/PmXxg7OTlB7//au/PwmM73f+DvmUlmMlkmCdn3RCRBSdCK0NYWjVJL1Qcpaex8UUWLoD5RKnytRa0VCS2CElWUpoTaGkQSIhpEYk1QZLFmmfv3h9+cb05mshEZ9H5d11xX5pznnHOfZ545Oc+cc+7H3LzSchWp7DOvzfani1qtRnR0NPr37w8A6Nu3L44cOYLMzEwAz74f9+/fr/S4pXmGrSrHNxcXlyrVf0Vq4vtWU3Wv4eDggCtXrrzQflWGB8DVo4cF+TAp/beOX0CrYv/+/ejQoQNGjx6NpUuX1lyANcTAwEA4yTIw4CbH/h0y3m8MADAu9cvfSBcbDHW2hkGZLFSp7zYCAChLlR3oaI1+DnUhg7jsyYCGWmX72NWpdnxU6mHiigQHB2PcuHHYsmULBg0ahM2bN0MqlaJPnz7V3mZlHj9+DCMjI63pa9euFU4qAKB///5o06YNli5dqjOJRHmqus8AMHv27CqXfZ2ZmZnh9OnTwntdv1RXpEWLFvj7779rOqxXUnBwMGQyGR4/fgxra2tERkaiSZMmwvx27dphxYoVePjwIRYtWgQDAwN88sknwnxHR8d/TV1NmDABAwYMQHZ2NiZMmICRI0fC09MTwLPvYVUz8f0b6isuLg4PHz5E586dAQBWVlbo2LEj1q5di5kzZ1b5uFWd49v69eufK9ZXnVKpxKNHj17qNviKkx7duJKh8+/KfPXVV5BIJMIBpX379iCiV7LTBDz7R+zi4gIXF5dq/1Nm7HVlIpPBRCYTnSDIpVKYyGRQlPkeaMpKS5U1lEpgIpPBSFa1stWlGR6gshMTlUqFXr16ISoqCsCzW0p69+4tGu6gKuzs7LQyUN26dUt0hcPKykrrFqC0tDT89ddfmDhxIgwMDGBgYICWLVvi0aNHiImJEcWZl5entd3c3FzhF+uq7nNNsbOzw+3bt0XTiouLce/ePWG/y6sXzbyKypSeX3o5XWXKI5VK4enpKbw8PDyqs4s1wsrKCjKZrNJ9LCws1MrQV7bM89alSqUSbr8rz6JFi5CcnIycnBzk5OQgNDRUNN/ExASenp7w9fXF2rVrkZCQ8FxXRitTE+2hptpmeaysrODp6Yn33nsPW7duxZgxY5CWlgYA8PLyQl5eHrKzs6u8zy+iss+8NtufLpGRkbh37x6USqVwjNuzZw/WrVsHtVoNa2trWFhYVHrc8vLyAlB7nc2a+L7VVN1r3Lt3D9bW1i+8bxXhs9jXjKurKxYsWAAAaNCggZ6jYYy9rurUqYOgoCAsW7ZM59gXpf9JDR48GEeOHMGuXbtw7NgxDB48uNrbCwgI0Er3HRcXh4CAAOF906ZNhZMrjcjISLz//vtISUlBcnKy8Bo/frzopNTb2xuJiYla2z19+rRwQlGdfa4JAQEByM3NFcV14MABqNVq+Pv7C2X+/PNPFBUVCWXi4uLg7e0NS0tLoUxFdefu7g47OztRmfz8fCQkJIjq91Ull8vRvHlzUfxqtRr79+8X4m/evDkMDQ1FZdLT03H16lWhTEBAAM6ePSvqEMTFxUGlUqFhw4ZCmcraYXns7Ozg6elZpRMzqVSKKVOm4Ouvv67yLYBVVRPtoabaZlU4OzujT58+mDx5MgCgV69ekMvl5aY2fxnfw4rqqzbbX1l3797FL7/8gpiYGNHxLSkpCffv38fvv/8OqVSKvn37YsOGDbh586bWOh48eIDi4mL4+fmhYcOGWLBggc5HNl5Gvb7o962m6l4jNTUVTZs2rdH91FJp+og3zKuUVY8ePBCy6pGODE9lSSQSUdY8Nze3WgjyxRUXF1NqaiqlpqZScXGxvsNhrMa87unIMzIyyM7OTkjNfeHCBUpLS6PFixeTj4+PUE6tVpOnpydZWlqKppeWlJRESUlJ1Lx5c/r0008pKSmJzp07J8w/evQoGRgY0Pz58+n8+fMUHh6ulUJ5586dZGNjIxwnCgsLydramlasWKG1PU1WsNTUVGH9UqmUvv32W0pLS6OzZ8/SlClTyMDAQLSNqu5zWFgYhYSEiLZ57tw5SkpKoq5du1Lbtm2FfdZISEggb29vUTr1Tp06UdOmTSkhIYGOHDlC9evXF6V8zs3NJVtbWwoJCaHU1FSKiYkhY2NjrXTkldXdnDlzyMLCgn755Rc6c+YMde/evUrpyCvLqlfZ56prn7OzsykpKUlIsf3nn39SUlIS3b17VyjTvn17Wrp0qfA+JiaGFAoFRUdHU1paGg0bNowsLCxEGbtGjBhBLi4udODAATp16hQFBARQQECAMF+THvmDDz6g5ORk2rt3L1lbW+tMjzxhwgQ6f/48LVu2rMrpyCvK4KYrq15RURE5OjoK2e2uX79O3t7elJCQIJS5e/cuJSUl0e7duwkAxcTEUFJSEmVnZwtlQkJCKCwsTHhfU+2hJtpmVbPqnTt3jiQSiZDFb9myZSSRSGjQoEF08OBBysrKoiNHjtCwYcNo/PjxwnLe3t60fft24X1BQYHQJgHQwoULKSkpia5cuSKUKfvdrcpnXlvtj0icVW/RokVkb28vyr6q0bt3b+rVqxcRPWsnPj4+5OTkROvWraNz587RhQsXKDIykjw9PYWMeQkJCWRmZkatWrWi3bt3U0ZGBqWkpNC3334rSkdetk0RVf5d3759uyijYk1932qi7ometUWJREJZWVladUnE6cif2+vYcUpMTNRKNT5p0qRaDPTFPH36lMLDwyk8PFyUkpOx193r3nEiIrp58yaNGjWKXF1dSS6Xk6OjI3Xr1o3i4+NF5SIiIggAzZ07V+d6yh6jAGidfG7ZsoW8vLxILpdTo0aNaPfu3aL5RUVF5ODgIPxT/fnnn7XS3ZbWoEED0Zgv+/bto9atW5OlpSXVrVuX2rZtS4cOHXqufQ4NDaU2bdqIlnN1ddW5nxqa9NulUw3fvXuXgoODydTUlFQqFQ0cOJAKCgpE601JSaF3332XFAoFOTo66hynqrK6U6vVNG3aNLK1tSWFQkEdOnSg9PR0UZnn6ThV9rnq2ufw8HCdy0VFRYnqsnQsRERLly4lFxcXksvl1KJFC/rrr79E8x8/fkwjR44kS0tLMjY2po8//ljUwSAiysrKog8//JCUSiVZWVnRl19+qTWuS3x8PPn5+ZFcLicPDw9RXJr5NdFxIiKaPXs2WVtb04MHD4RORul2FhUVpbOuStdNmzZtKDQ0VLTemmgPNdE2q9pxIiIKCgqiDz/8UHgfFxdHQUFBZGlpSUZGRuTj40NfffUV3bx5UyhTtt1o2lvZV+n60fXdrewzJ6q99lf6+9K4cWMaOXKkVixERJs3bya5XE537twhomcd2bCwMKpfvz7J5XKytbWlwMBAio2NFXW80tPT6bPPPiMHBweSy+Xk6upKwcHBdPr0aaGMrjZV2Xdd01aru7+1VfcRERGiNOdl1VTHSUJUjafJ3gD5+fkwNzdHXl4eVCqVfoN5+BDQPCfw4AGgIznE0KFDtUbcvnLlygtne6pNRUVF+OmnnwA8e6j7RbIvMfYqefLkCTIzM+Hu7q4zqQGrvmXLlmHnzp3Yt2+fvkN54wwYMABubm6YPn26vkN5pR08eBADBgx4oRTr/xZZWVlwd3evVmKCfzuJRILMzEy4ubnpO5Q3RmFhIerXr4+NGzeidevWOstU9P+6On0DTnGmR9evXIJT6b8b+mqVOXDggPC3gYGB6F7j14WhoSEGDhyo7zAYY6+B4cOHIzc3FwUFBdXKmMcYY+zf6erVq5gyZUq5naaaxB0nPXpUUKDz79IyMjKgUCjg4eGB8+fP11ZojDGmFwYGBpg6daq+w2CMMfaa0GQFrQ3ccdIjM8u6Wn8fO3YMrVu3hqWlpTBg7NOnT/USH2OMsTdHjx49tAaRZNrc3NwwduxYfYfxWrCwsEB4eLi+w3ithIeH8/fwNcbPOOlTmWecgocMEY1LsmnTJvTt21dPwdWcoqIi4TmtIUOG8DNO7I3Bzzgxxhhjr76aesbplRjHadmyZXBzc4ORkRH8/f1x4sSJCstv3boVPj4+MDIyQuPGjbFnz55aivTlcXJyEnWa5HL5G9FpAp6NZn3r1i3cunWLHyBljDHGGGOvJb13nDZv3ozx48cjPDwcp0+fhq+vL4KCgrRGtNY4duwYgoODMXjwYCQlJaFHjx7o0aMHUlNTaznyF/ek1KB490sNTNasWbM36vY8AwMDhISEICQkBAYGfHcoY4wxxhh7/ei947Rw4UIMHToUAwcORMOGDbFy5UoYGxtj7dq1OssvXrwYnTp1woQJE9CgQQPMnDkTzZo1w/fff1/Lkb+4/XG7/u+NQgEAWL16NRITE1FcXIzCwkIUFxeLliksLERhYaFoVOiSkhIUFhZqZdx7FcoWFRWhuLgYbm5uqFevHqRSvTc5xhhjjDHGqk2vZ7GFhYVITExEYGCgME0qlSIwMBDHjx/Xuczx48dF5QEgKCio3PJPnz5Ffn6+6PWqcHJ0FP72rOeGO3fuYOjQoQCeXVmLiIjQug1x3rx5iIiIQF5enjDtxIkTiIiIwM6dO0Vlv/vuO0REROCff/4RpiUnJyMiIgI///yzqOyyZcsQERGB7OxsYdq5c+cQERGBTZs2icr+8MMPiIiIwJUrV4RpFy5cQEREBNavXy8qGxUVhYiICGRkZFSpThhjjDHGGHsV6bXj9M8//6CkpAS2trai6ba2tsjJydG5TE5OTrXKz549G+bm5sLL2dm5ZoKvAb7NWwl/9+7eE0qlUo/RMMYYY4wxxsqj16x6N2/ehKOjI44dO4aAgABh+sSJE3Ho0CEkJCRoLSOXy7Fu3ToEBwcL05YvX45vvvkGt27d0ir/9OlT0fNC+fn5cHZ2fjWy6hEBjx6hsLAQMDaGoVwOiUQCACguLoZarYZUKhU9F1RYWAjg2XNDmtveSkpKUFJSAolEIspY9yqULSoqAhFBJpNBJpPVRK0x9srgrHqMMcbYq++NyKpnZWUFmUym1eG5desW7OzsdC5jZ2dXrfIKhQIqlUr0emVIJICJCeSWlpArFEKnCXjWKZHL5VrJFORyOeRyuehZIZlMBrlcrpXm+1Uoa2hoCLlczp0mxhjS09NhZ2eHgnIG/GbPb/r06RgwYIC+w3gtSCQSZGVl6TuM14KbmxsOHjyo7zBeG23btkV0dLS+w3jjtGzZEtu2bdN3GAD03HGSy+Vo3rw59u/fL0xTq9XYv3+/6ApUaQEBAaLyABAXF1duecYYY+U7fvw4ZDIZunTpAuDZD1GGhoai4RFKGzx4MJo1aya8z8/Px7Rp09CoUSMolUrUrVsX77zzDubOnYv79++Llp08eTI+//xzmJmZaa3Xx8cHCoVC523Xbm5u+O6777SmT58+HX5+fqJpOTk5+Pzzz+Hh4QGFQgFnZ2d07dpV6/9GWWfOnMF7770HIyMjODs7Y+7cuRWWB4D9+/ejVatWMDMzg52dHSZNmqSV0IeIMH/+fHh5eUGhUMDR0RGzZs0S5h88eBASiUTrVboeZs+ejXfeeQdmZmawsbFBjx49kJ6eXmFs5a3366+/BvDs19cBAwagcePGMDAwQI8ePSrdX83+/Pe//4W9vT2USiUCAwNx8eLFCpcpKCjA2LFj4erqCqVSiVatWuHkyZOiMtOnT4ePjw9MTExgaWmJwMBArbtOLly4gO7du8PKygoqlQrvvvsu4uPjRWV07XN5bVmjbdu2QlkjIyM0bNgQy5cvF+ZHR0cL86VSKezt7dGnTx9cvXq10vp6nuFTli1bhgYNGkCpVMLb21vr2eGqrFdXPUgkEsybN08o4+bmpjV/zpw5FcY2YMAAoayhoSHc3d0xceJEPHnyRFTu0qVLGDhwIJycnKBQKODu7o7g4GCcOnWqwvUfPHgQzZo1g0KhgKenZ5U6IVu2bIGfnx+MjY3h6uoq2seyMZd+NWrUSOf65syZA4lEIhoE+d69e/j888/h7e0NpVIJFxcXjBkzRvS8eUWuX78OuVyOt956S+d8IsLq1avh7+8PU1NTWFhY4O2338Z3332HR48eCeXy8/MxdepU4bO3s7NDYGAgtm/fXuFwL1evXkWXLl1gbGwMGxsbTJgwQetYVdbp06fRsWNHWFhYoG7duhg2bBgePHggzL979y46deoEBwcH4Vg7evTocnMJHD16FAYGBlrH7JKSEkybNg3u7u5QKpWoV68eZs6cKdqfr7/+GmFhYaKkZHpDehYTE0MKhYKio6MpLS2Nhg0bRhYWFpSTk0NERCEhIRQWFiaUP3r0KBkYGND8+fPp/PnzFB4eToaGhnT27NkqbS8vL48AUF5e3kvZH8bYv8fjx48pLS2NHj9+rO9QntvgwYPpiy++IFNTU7px4wYREXXv3p06duyoVfbBgwdkampK33//PRER3b17lxo0aECOjo60du1aSklJoaysLNq3bx/17dtXKEdEdOXKFTI0NKTr169rrffw4cPk4uJCn376Kc2ZM0drvqurKy1atEhrenh4OPn6+grvMzMzycHBgRo2bEg///wzpaenU2pqKi1YsIC8vb3LrYO8vDyytbWlfv36UWpqKm3atImUSiWtWrWq3GWSk5NJLpfTN998QxcvXqSDBw+Sj48Pffnll6Jyn3/+OXl7e9Mvv/xCly9fplOnTtHvv/8uzI+PjycAlJ6eTtnZ2cKrpKREKBMUFERRUVGUmppKycnJ1LlzZ3JxcaEHDx6I6iI0NLTS9RYUFBDRs89yxIgRtHr1agoKCqLu3buXu6+lzZkzh8zNzWnHjh2UkpJC3bp1I3d39wq/A71796aGDRvSoUOH6OLFixQeHk4qlUrUFjZs2EBxcXGUkZFBqampNHjwYFKpVHT79m2hTP369alz586UkpJCFy5coJEjR5KxsTFlZ2cLZQBQVFSUaJ/LxgaAMjMzhfdt2rShoUOHUnZ2NmVkZFB4eDgBoI0bNxIRUVRUFKlUKsrOzqabN2/S0aNHydfXl1q0aFFhXR09epRkMhnNnTuX0tLS6Ouvv670fGX58uVkZmZGMTExlJGRQZs2bSJTU1PauXNntdZbev+zs7Np7dq1JJFIKCMjQyjj6upKM2bMEJUr3aY0ZeLj44X3oaGh1KlTJ8rOzqarV69SbGwsqVQqmjhxolDm5MmTpFKpqFWrVrRr1y66dOkSJSUl0fTp0+n9998vd98vX75MxsbGNH78eEpLS6OlS5eSTCajvXv3lrvMnj17yMDAgFasWEEZGRm0a9cusre3p6VLlwplcnNzRft47do1qlOnDoWHh2ut78SJE+Tm5kZNmjShL774Qph+9uxZ6tmzJ+3cuZMuXbpE+/fvp/r169Mnn3wiWr5NmzYUFRWltd6ZM2dSv379yNnZmf766y+t+f369SOlUkmzZs2iEydOUGZmJu3YsYPatm1LsbGxRER0//59atSoETk5OVF0dDSdO3eO0tPTafXq1VSvXj26f/++zjoqLi6mt956iwIDAykpKYn27NlDVlZWNHny5HLr9caNG2RpaUkjRoygv//+m06cOEGtWrUS7e+9e/do+fLldPLkScrKyqI//viDvL29KTg4WGt99+/fJw8PD/rggw9Ex2wiolmzZlHdunVp165dlJmZSVu3biVTU1NavHixaB9sbW1p165d5cZcmYr+X1enb6D3jhMR0dKlS8nFxYXkcjm1aNFC1KjatGkj+mdARLRlyxby8vIiuVxOjRo1ot27d1d5W9xxYozVFJ0HYrWa6MED/bzU6mrFX1BQQKampvT3339Tnz59aNasWUREtHPnTpJKpXTlyhVR+aioKDIyMhL+QQ8fPpxMTEyEDldZ6lLxzJs3j95++22d5QYMGEBhYWH022+/kZeXl9b8qnacPvzwQ3J0dNQ6+SOick8qiJ6drFpaWtLTp0+FaZMmTaqwszV58mSt/dm5cycZGRlRfn4+ERGlpaWRgYEB/f333+WuR9PBqSi+sm7fvk0A6NChQ8K08jpOVVlvaGholTpOarWa7OzsaN68ecK03NxcUigUtGnTJp3LPHr0iGQymdYJT7NmzWjq1Knlbkvzv/qPP/4gIqI7d+4QAPrzzz+FMvn5+QSA4uLihGkAhBPN8ujqOJU+SSZ61knr27cvET1r9+bm5qL5S5YsqfRconfv3tSlSxfRNH9/fxo+fHi5ywQEBNBXX30lmjZ+/Hhq3br1C623e/fu1L59e9G08r5XZcuU7TiVbSs9e/akpk2bEtGzNtKoUSNq3ry5qPOvUVF7nDhxIjVq1Eg0rU+fPhQUFFTuMsHBwdSrVy/RtCVLlpCTk5Po+FNabGwsSSQSysrKEk0vKCig+vXrU1xcnM42UdaWLVtILpdTUVGRME1Xx0mtVpOHhwft3buXJk2aREOHDhXN37x5MwGgHTt2aG1DrVZTbm4uERH9z//8T7nH24KCAlEcpe3Zs4ekUqlwQYKIaMWKFaRSqUTHvNJWrVpFNjY2os/wzJkzBIAuXryocxkiosWLF5OTk5PW9D59+tDXX3+tdcwmIurSpQsNGjRINK1nz57Ur18/0bSBAwdS//79y912ZWqq4/RKDKozevRoXLlyBU+fPkVCQgL8/f2FeQcPHtS6VPuf//wH6enpePr0KVJTU9G5c+dajpgxxsrx6BFgaqqfV6lbOqpiy5Yt8PHxgbe3N/r374+1a9eCiNC5c2fY2tpqHXujoqLQs2dPWFhYQK1WY/Pmzejfvz8cHBx0rr/0c5uHDx/G22+/rVWmoKAAW7duRf/+/dGxY0fk5eXh8OHD1doP4NmtNHv37sWoUaNgYmKiNd/CwkL4e8CAAWjbtq3w/vjx43j//fchl8uFaUFBQUhPT9e63VDj6dOnWg8YK5VKPHnyBImJiQCAX3/9FR4eHti1axfc3d3h5uaGIUOG4N69e1rr8/Pzg729PTp27IijR49WuK+a24Pq1KlTYbkXlZWVBYlEIjzjkpmZiZycHNGQIObm5vD39y93SJDi4mKUlJTorKsjR47oXKawsBCrV6+Gubk5fH19AQB169YVblt7+PAhiouLsWrVKtjY2KB58+ai5UeNGgUrKyu0aNFCaNPVpVQqhURIZd2+fRuxsbFaSY/c3Nwwffp04X11h08Bym9XJ06cEMZJrO56b926hd27d2Pw4MFa8+bMmYO6deuiadOmmDdvXqW3b5WVmpqKY8eOCd+d5ORknDt3Dl9++aXOcRtLfw/btm0rei6vJuvr+vXroiFTSouMjERgYCBcXV1F00eNGoUuXbpoxVAeTSKBss+ilxUfH49Hjx4hMDAQ/fv3R0xMDB4+fCjM37BhA7y9vdG9e3etZSUSCczNzaFWqxETE4N+/frpPN6ampoKcUyfPh1ubm7CvOPHj6Nx48aijNRBQUHIz8/HuXPndMb89OlTrWfZNZmfy/ve3rx5E9u3b0ebNm1E06OionD58mWEh4frXK5Vq1bYv38/Lly4AABISUnBkSNH8OGHH4rKtWjR4rn+N9S0V6LjxBhjrPZFRkaif//+AIBOnTohLy8Phw4dgkwmQ2hoKKKjo4WTzoyMDBw+fBiDBg0CANy5cwe5ubnw9vYWrbN58+YwNTWFqampKPvplStXdP7Dj4mJQf369dGoUSPIZDL07dsXkZGR1d6XS5cugYjg4+NTaVl7e3u4uLgI78sb5kIzT5egoCAcO3YMmzZtQklJCW7cuIEZM2YAgDAe3uXLl3HlyhVs3boV69evR3R0NBITE9GrVy9RLCtXrsS2bduwbds2ODs7o23btjh9+rTO7arVaowdOxatW7cu93mJ0pycnITPw9TUFHfv3q10GQ1DQ0N4e3vD2NhYVBfVGRLEzMwMAQEBmDlzJm7evImSkhL89NNPOH78uGjcQADYtWsXTE1NYWRkhEWLFiEuLg5WVlYAnp1A/vHHH0hKSoKZmRmMjIywcOFC7N27F5aWlsI6ZsyYgS1btiAuLg6ffPIJRo4ciaVLl1Z5nzXxnTlzBu3btxem5+XlwdTUFCYmJrC1tUV8fLxWJ71evXpCvJr6qk5dAc/a1Zo1a5CYmAgiwqlTp7BmzRoUFRUJYzJWd73r1q2DmZkZevbsKZo+ZswYxMTEID4+HsOHD0dERAQmTpxYSQ2JP6fGjRvj9u3bmDBhAgAIz7tV5Xvo4uICe3t74X15+5Wfn4/Hjx/rXEdQUBC2b9+O/fv3Q61W48KFC1iwYAEAaLUv4NnJ/W+//YYhQ4aIpsfExOD06dOYPXt2pXEDz4bTmTlzJoYNG1Zp2cjISPTt2xcymQxvvfUWPDw8sHXrVmH+xYsXtY6jurZ3//79KtWrlZUV6tWrJ7x/nuNb+/btkZOTg3nz5qGwsBD3799HWFgYAO16DQ4OhrGxMRwdHaFSqbBmzRrRvoWFheGnn34qt4MZFhaGvn37wsfHB4aGhmjatCnGjh2Lfv36ico5ODjg2rVren/OqeJuMmOMseoxNgZKPUBb69uuovT0dJw4cQKxsbEAnmXy7NOnDyIjI9G2bVsMGjQIc+bMQXx8PNq3b4+oqCi4ubmJTiZ1iY2NRWFhISZNmiQ62Xn8+LHOlO1r164VOm8A0L9/f7Rp0wZLly7VmUSiPNW5qlDVk6OKfPDBB5g3bx5GjBiBkJAQKBQKTJs2DYcPHxZ+pVWr1Xj69CnWr18PLy8vAM9Oopo3b4709HR4e3sLL41WrVohIyMDixYtwo8//qi13VGjRiE1NbXcX33LOnz4sKgeS3cyKuPo6Ii///67yuXL8+OPP2LQoEFwdHSETCZDs2bNEBwcLFyZ02jXrh2Sk5Pxzz//4IcffkDv3r2RkJAAGxsbEBFGjRoFGxsbHD58GEqlEmvWrEHXrl1x8uRJ4QR82rRpwvqaNm2Khw8fYt68eRgzZkyFMS5fvhxr1qxBYWEhZDIZxo0bh//5n/8R5puZmeH06dMoKirCb7/9hg0bNoiSfACoNAFJVUybNg05OTlo2bIliAi2trYIDQ3F3LlzdV7BqYq1a9eiX79+Wt+/8ePHC383adIEcrkcw4cPx+zZs6FQKMpdX7t27bBixQo8fPgQixYtgoGBAT755BMA1fse6kp6UV1Dhw5FRkYGPvroIxQVFUGlUuGLL77A9OnTddbXunXrYGFhIUqGcu3aNXzxxReIi4ur0rAS+fn56NKlCxo2bCi6wqhLbm4utm/fLvq+9u/fH5GRkcLVtqrUWXXqdfTo0Rg9enSVy+vSqFEjrFu3DuPHj8fkyZMhk8kwZswY2NraatXrokWLEB4ejgsXLmDy5MkYP348li9fjpKSEnz66af45ptvhOOfLlu2bMGGDRuwceNGNGrUCMnJyRg7diwcHBwQGhoqlFMqlcIxVZ/jnvIVJ8YYq0n/f5gBvbxK3RpXmcjISBQXF8PBwQEGBgYwMDDAihUrsG3bNuTl5aF+/fp47733EBUVBbVajfXr12PgwIHC7XfW1tawsLDQyu7m4uICT09PrU6PlZWV1m1vaWlp+OuvvzBx4kQhhpYtW+LRo0eiTGgqlUpn9qrc3FyYm5sDAOrXrw+JRPJcJ/rlDXOhmVee8ePHIzc3F1evXsU///wj3Grj4eEB4NnVJAMDA9FJQ4MGDQCgwoxsLVq0wKVLl7Smjx49Grt27UJ8fDycnJyqtG/u7u7w9PQUXs978g38X11UZ0gQ4NmVmEOHDuHBgwe4du2acNuZpp40TExM4OnpiZYtWyIyMhIGBgbC1ccDBw5g165diImJQevWrdGsWTMsX74cSqUS69atK3fb/v7+uH79umg8R1369euH5ORkZGZm4uHDh1i4cKGorqRSKTw9PdGgQQOMHz8eLVu2FHWsdKnu8CnAs5PDtWvX4tGjR8jKysLVq1fh5uYGMzMzWFtbV3u9hw8fRnp6utYVFl38/f1RXFxcaap2zefk6+uLtWvXIiEhQficNG29Jr+HKpWq3BNliUSC//3f/8WDBw9w5coV5OTkoEWLFgCg1b6ICGvXrkVISIjottzExETcvn0bzZo1E45Dhw4dwpIlS2BgYICSkhKhbEFBATp16gQzMzPExsZqDcFS1saNG/HkyRP4+/sL6540aRKOHDki3Jrm5eVVaX1pjre1eXz79NNPkZOTgxs3buDu3buYPn067ty5o1WvdnZ28PHxQbdu3bBq1SqsWLEC2dnZKCgowKlTpzB69Ghh32fMmIGUlBQYGBjgwIEDAIAJEyYIV50aN26MkJAQjBs3TusHrnv37sHExESvnSaAO06MMfavU1xcjPXr12PBggVITk4WXikpKXBwcMCmTZsAPEs9rrmF7MaNG6LnEaRSKXr37o2ffvoJN2/erHSbTZs2RVpammhaZGQk3n//faSkpIjiGD9+vOh2PW9vb62rE8CzdLmaE7U6deogKCgIy5YtEz0/oJGbm1tubAEBAfjzzz+FZ0iAZ8NceHt7V3qFRiKRwMHBAUqlEps2bYKzs7OQrr1169YoLi5GRkaGUF5zslT2+YrSkpOTRbcwERFGjx6N2NhYHDhwAO7u7hXG9LK4u7vDzs5OdGUlPz8fCQkJVRoSxMTEBPb29rh//z727dun85mO0jS/LgMQUjKX7fhJpdIKb91JTk6GpaVlhVdQgGfPanl6esLR0bFKncuwsDBs3ry53FsqgRcbPsXQ0BBOTk6QyWSIiYnBRx99JMRVnfVqrnBqnhWrSHJyMqRSKWxsbCotqyGVSjFlyhR8/fXXePz4Mfz8/NCwYUMsWLBA5+dS2ffweetLJpPB0dERcrkcmzZtQkBAgNDR1Dh06BAuXbqk9axXhw4dcPbsWdEx6O233xY605rn2PLz8/HBBx9ALpdj586dVbo6FRkZiS+//FLrOPvee+9h7dq1AJ51UC5cuIBffvlFa3kiQl5eHqRSKfr27YsNGzboPN4+ePCg3OfTAgICcPbsWdy+fVuYFhcXB5VKhYYNG1a6D7a2tjA1NcXmzZthZGSEjh07lltW85k/ffoUKpVKq15HjBgBb29vJCcnC/kMHj16pPWdk8lkWu0nNTUVTZs2rTTel+6501O8pjirHmOspryu6chjY2NJLpcL2ZpKmzhxopAt7uHDh6RSqcjS0pI6deqkVfaff/4hLy8vcnR0pMjISEpJSaFLly7R9u3bycvLi3r27CmU3blzJ9nY2FBxcTERERUWFpK1tTWtWLFCa71paWkEgFJTU4noWfplqVRK3377LaWlpdHZs2dpypQpZGBgIErBnJGRQXZ2dkI68gsXLlBaWhotXryYfHx8hHJhYWEUEhIivM/NzSVbW1sKCQmh1NRUiomJIWNjY1E68u3bt2tl2Zs7dy6dOXOGUlNTacaMGWRoaCjK6FZSUkLNmjWj999/n06fPk2nTp0if39/Uar3RYsW0Y4dO+jixYt09uxZ+uKLL0gqlQrZ5IieZdMyNzengwcPitIqP3r0SCjzPFn1zp07R0lJSdS1a1dq27YtJSUlUVJSkjD/+vXr5O3tTQkJCcK0OXPmkIWFBf3yyy905swZ6t69u1Y68vbt24vSQe/du5d+++03unz5Mv3+++/k6+tL/v7+VFhYSETPUqNPnjyZjh8/TllZWXTq1CkaOHAgKRQKoQ3cuXOH6tatSz179qTk5GRKT0+nr776igwNDSk5OZmInrWxH374gc6ePUsXL16k5cuXk7GxMf33v/8V7TeqkFWvNF1Z9Yi0s9uV3e+qDJ9Sti2mp6fTjz/+SBcuXKCEhATq06cP1alTRxRvVYdlycvLI2NjY53fsWPHjtGiRYsoOTmZMjIy6KeffiJra2v67LPPROWqklWvqKiIHB0dhWyLCQkJZGZmRq1ataLdu3dTRkYGpaSk0LfffitKR152uBlNOvIJEybQ+fPnadmyZVrpyJcuXSrKDnjnzh1asWIFnT9/npKSkmjMmDFkZGQkarMa/fv3J39/f63pupRtE3l5eeTv70+NGzemS5cuib6HmmOaZjlNVr2kpCQCQOfPn9da//Lly8nOzo6KiopIrVZTnz59hHTkmvTev/76K7Vv3144pty9e5d8fHzIycmJ1q1bR+fOnaMLFy5QZGQkeXp6Ct/1snWkSUf+wQcfUHJyMu3du5esra1F6cgTEhLI29tbNETA0qVLKTExkdLT0+n7778npVIpShG+e/duWrt2LZ09e5YyMzNp165d1KBBA1EGyLJ0ZdULDQ0lR0dHIR359u3bycrKSpTiXlO3M2bMKHfdlXmj0pHXJu44McZqyuvacfroo4+oc+fOOuclJCQQAEpJSSEiomHDhhEA2rJli87yubm5NHnyZPLx8SGFQkFKpZKaNGlC06ZNo7t37wrlioqKyMHBQTgJ+vnnn7VS5JbWoEEDGjdunPB+37591Lp1a7K0tKS6detS27ZtRem4NW7evEmjRo0iV1dXksvl5OjoSN26ddM6+WvTpo1ouZSUFHr33XdJoVCQo6Oj1nhSUVFRVPa3xnbt2pG5uTkZGRmRv78/7dmzRyueGzduUM+ePcnU1JRsbW1pwIABonr53//9X6pXrx4ZGRlRnTp1qG3btnTgwAHROgDofJVOe/w8HSdXV1ed69XIzMwkAKK6U6vVNG3aNLK1tSWFQkEdOnSg9PR0rfWWHiNn8+bN5OHhQXK5nOzs7GjUqFGiTvvjx4/p448/JgcHB5LL5WRvb0/dunWjEydOiNZ78uRJ+uCDD6hOnTpkZmZGLVu2FNX5b7/9Rn5+fmRqakomJibk6+tLK1eu1EqLXVMdp+PHjxMA4SS97H4TVT58Stm2mJaWRn5+fqRUKkmlUlH37t11prOvyrAsq1atIqVSqfMHksTERPL39xfab4MGDSgiIoKePHkiKleVjhMR0ezZs8na2loYCiA9PZ0+++wz4TN1dXWl4OBgOn36tLCMruFm4uPjyc/Pj+RyOXl4eGil9g4PDydXV1fh/Z07d6hly5ZkYmJCxsbG1KFDB53jJOXm5pJSqaTVq1drzdOlbJvQfJ90vcq2JU3Mo0ePpoYNG+pcf3Z2NkmlUvrll1+I6NmPLCtWrKB33nmHjI2NSaVSUfPmzWnx4sWiH0hyc3MpLCyM6tevT3K5nGxtbSkwMJBiY2OF9Otl64iIKCsriz788ENSKpVkZWVFX375pSh9uWb/Su9LSEgI1alTh+RyOTVp0oTWr18vWueBAwcoICBAaEP169enSZMmVXjM0dVxys/Ppy+++IJcXFzIyMiIPDw8aOrUqaJU6devXydDQ0O6du1aueuuTE11nCREz5Gn8zWWn58Pc3NzIY0kY4w9rydPniAzMxPu7u5Vum3j327ZsmXYuXMn9u3bp+9Q3jjTp09HVlaWVgp5pk0ikSAzM1OUspnp5ubmhujoaFH6flY+TYr10rc1sxc3adIk3L9/H6tXr37udVT0/7o6fQPOqscYY6xWDB8+HLm5uSgoKKhWxjzGGGP/XjY2NqIskPrEHSfGGGO1wsDAAFOnTtV3GIwxxl4jX375pb5DEHDHiTHGGHvNtW3btsKMZez/hIeHw8LCQt9hvBbGjh3LtzRWw4ABA+Dn56fvMNhLxM84McbYc+JnnBhjjLFXX00948TjODHG2Av6l/3+xBhjjL1Waur/NHecGGPsOWlGjdcMzskYY4yxV09hYSEACAMaPy9+xokxxp6TTCaDhYWFMCK7sbExJBKJnqNijDHGmIZarcadO3dgbGwMA4MX6/pwx4kxxl6AnZ0dAAidJ8YYY4y9WqRSKVxcXF74x03uODHG2AuQSCSwt7eHjY0NioqK9B0OY4wxxsqQy+WQSl/8CSXuODHGWA2QyWQvfO80Y4wxxl5dnByCMcYYY4wxxirBHSfGGGOMMcYYqwR3nBhjjDHGGGOsEv+6Z5w0A2Dl5+frORLGGGOMMcaYPmn6BFUZJPdf13EqKCgAADg7O+s5EsYYY4wxxtiroKCgAObm5hWWkVBVuldvELVajZs3b8LMzOyVGKgyPz8fzs7OuHbtGlQqlb7DYa84bi+surjNsOriNsOqi9sMq65Xqc0QEQoKCuDg4FBpyvJ/3RUnqVQKJycnfYehRaVS6b3hsNcHtxdWXdxmWHVxm2HVxW2GVder0mYqu9KkwckhGGOMMcYYY6wS3HFijDHGGGOMsUpwx0nPFAoFwsPDoVAo9B0Kew1we2HVxW2GVRe3GVZd3GZYdb2ubeZflxyCMcYYY4wxxqqLrzgxxhhjjDHGWCW448QYY4wxxhhjleCOE2OMMcYYY4xVgjtOjDHGGGOMMVYJ7ji9ZMuWLYObmxuMjIzg7++PEydOVFh+69at8PHxgZGRERo3bow9e/bUUqTsVVGdNvPDDz/gvffeg6WlJSwtLREYGFhpG2NvnuoeZzRiYmIgkUjQo0ePlxsge+VUt83k5uZi1KhRsLe3h0KhgJeXF/9/+pepbpv57rvv4O3tDaVSCWdnZ4wbNw5PnjyppWiZvv3555/o2rUrHBwcIJFIsGPHjkqXOXjwIJo1awaFQgFPT09ER0e/9DiriztOL9HmzZsxfvx4hIeH4/Tp0/D19UVQUBBu376ts/yxY8cQHByMwYMHIykpCT169ECPHj2Qmppay5Ezfalumzl48CCCg4MRHx+P48ePw9nZGR988AFu3LhRy5Ezfalum9HIysrCV199hffee6+WImWviuq2mcLCQnTs2BFZWVn4+eefkZ6ejh9++AGOjo61HDnTl+q2mY0bNyIsLAzh4eE4f/48IiMjsXnzZkyZMqWWI2f68vDhQ/j6+mLZsmVVKp+ZmYkuXbqgXbt2SE5OxtixYzFkyBDs27fvJUdaTcRemhYtWtCoUaOE9yUlJeTg4ECzZ8/WWb53797UpUsX0TR/f38aPnz4S42TvTqq22bKKi4uJjMzM1q3bt3LCpG9Yp6nzRQXF1OrVq1ozZo1FBoaSt27d6+FSNmrorptZsWKFeTh4UGFhYW1FSJ7xVS3zYwaNYrat28vmjZ+/Hhq3br1S42TvZoAUGxsbIVlJk6cSI0aNRJN69OnDwUFBb3EyKqPrzi9JIWFhUhMTERgYKAwTSqVIjAwEMePH9e5zPHjx0XlASAoKKjc8uzN8jxtpqxHjx6hqKgIderUeVlhslfI87aZGTNmwMbGBoMHD66NMNkr5HnazM6dOxEQEIBRo0bB1tYWb731FiIiIlBSUlJbYTM9ep4206pVKyQmJgq3812+fBl79uxB586dayVm9vp5Xc6BDfQdwJvqn3/+QUlJCWxtbUXTbW1t8ffff+tcJicnR2f5nJyclxYne3U8T5spa9KkSXBwcNA6+LA30/O0mSNHjiAyMhLJycm1ECF71TxPm7l8+TIOHDiAfv36Yc+ePbh06RJGjhyJoqIihIeH10bYTI+ep818+umn+Oeff/Duu++CiFBcXIwRI0bwrXqsXOWdA+fn5+Px48dQKpV6ikyMrzgx9oaYM2cOYmJiEBsbCyMjI32Hw15BBQUFCAkJwQ8//AArKyt9h8NeE2q1GjY2Nli9ejWaN2+OPn36YOrUqVi5cqW+Q2OvqIMHDyIiIgLLly/H6dOnsX37duzevRszZ87Ud2iMvRC+4vSSWFlZQSaT4datW6Lpt27dgp2dnc5l7OzsqlWevVmep81ozJ8/H3PmzMEff/yBJk2avMww2Sukum0mIyMDWVlZ6Nq1qzBNrVYDAAwMDJCeno569eq93KCZXj3Pccbe3h6GhoaQyWTCtAYNGiAnJweFhYWQy+UvNWamX8/TZqZNm4aQkBAMGTIEANC4cWM8fPgQw4YNw9SpUyGV8u/2TKy8c2CVSvXKXG0C+IrTSyOXy9G8eXPs379fmKZWq7F//34EBAToXCYgIEBUHgDi4uLKLc/eLM/TZgBg7ty5mDlzJvbu3Yu33367NkJlr4jqthkfHx+cPXsWycnJwqtbt25CFiNnZ+faDJ/pwfMcZ1q3bo1Lly4JnWwAuHDhAuzt7bnT9C/wPG3m0aNHWp0jTcebiF5esOy19dqcA+s7O8WbLCYmhhQKBUVHR1NaWhoNGzaMLCwsKCcnh4iIQkJCKCwsTCh/9OhRMjAwoPnz59P58+cpPDycDA0N6ezZs/raBVbLqttm5syZQ3K5nH7++WfKzs4WXgUFBfraBVbLqttmyuKsev8+1W0zV69eJTMzMxo9ejSlp6fTrl27yMbGhr799lt97QKrZdVtM+Hh4WRmZkabNm2iy5cv0++//0716tWj3r1762sXWC0rKCigpKQkSkpKIgC0cOFCSkpKoitXrhARUVhYGIWEhAjlL1++TMbGxjRhwgQ6f/48LVu2jGQyGe3du1dfu6ATd5xesqVLl5KLiwvJ5XJq0aIF/fXXX8K8Nm3aUGhoqKj8li1byMvLi+RyOTVq1Ih2795dyxEzfatOm3F1dSUAWq/w8PDaD5zpTXWPM6Vxx+nfqbpt5tixY+Tv708KhYI8PDxo1qxZVFxcXMtRM32qTpspKiqi6dOnU7169cjIyIicnZ1p5MiRdP/+/doPnOlFfHy8zvMTTTsJDQ2lNm3aaC3j5+dHcrmcPDw8KCoqqtbjroyEiK+ZMsYYY4wxxlhF+BknxhhjjDHGGKsEd5wYY4wxxhhjrBLccWKMMcYYY4yxSnDHiTHGGGOMMcYqwR0nxhhjjDHGGKsEd5wYY4wxxhhjrBLccWKMMcYYY4yxSnDHiTHGGGOMMcYqwR0nxhjTo+joaFhYWOg7jBcikUiwY8eOCssMGDAAPXr0qJV4atu0adMwbNgwfYdRIw4ePAiJRILc3Fxh2o4dO+Dp6QmZTIaxY8dWu826ubnhu+++e6G40tLS4OTkhIcPH77Qehhj7EVwx4kxxl7QgAEDIJFItF6XLl3Sd2i1Ijs7Gx9++CEAICsrCxKJBMnJyaIyixcvRnR0dO0HVwW6OgtVlZOTg8WLF2Pq1KnCtD///BNdu3aFg4NDlTqVGikpKejWrRtsbGxgZGQENzc39OnTB7dv3652XM+rVatWyM7Ohrm5uTBt+PDh6NWrF65du4aZM2eiT58+uHDhQpXXefLkSVHHsjp1otGwYUO0bNkSCxcurNZyjDFWk7jjxBhjNaBTp07Izs4Wvdzd3fUdVq2ws7ODQqGosIy5uXmtX1krLCx86dtYs2YNWrVqBVdXV2Haw4cP4evri2XLllV5PXfu3EGHDh1Qp04d7Nu3D+fPn0dUVBQcHBxq9SqLXC6HnZ0dJBIJAODBgwe4ffs2goKC4ODgADMzMyiVStjY2FR5ndbW1jA2Nn7h2AYOHIgVK1aguLj4hdfFGGPPgztOjDFWAxQKBezs7EQvmUyGhQsXonHjxjAxMYGzszNGjhyJBw8elLuelJQUtGvXDmZmZlCpVGjevDlOnTolzD9y5Ajee+89KJVKODs7Y8yYMRWeWE+fPh1+fn5YtWoVnJ2dYWxsjN69eyMvL08oo1arMWPGDDg5OUGhUMDPzw979+4V5hcWFmL06NGwt7eHkZERXF1dMXv2bGF+6SsIms5i06ZNIZFI0LZtWwDiW/VWr14NBwcHqNVqUazdu3fHoEGDhPe//PILmjVrBiMjI3h4eOCbb76p8KRZs41Zs2bBwcEB3t7eAIAff/wRb7/9NszMzGBnZ4dPP/1UuIqTlZWFdu3aAQAsLS0hkUgwYMAAoV5mz54Nd3d3KJVK+Pr64ueffxZtMyYmBl27dhVN+/DDD/Htt9/i448/LjfWso4ePYq8vDysWbMGTZs2hbu7O9q1a4dFixYJdaq5MrZ79240adIERkZGaNmyJVJTU0XrqqyNPH36FJMmTYKzszMUCgU8PT0RGRkp2kZubi4OHjwIMzMzAED79u0hkUhw8OBBnbfq/frrr3jnnXdgZGQEKysr0b6XvlXPzc0NAPDxxx9DIpHAzc0NWVlZkEqlonYOAN999x1cXV2FdtKxY0fcu3cPhw4dqnK9MsZYTeKOE2OMvURSqRRLlizBuXPnsG7dOhw4cAATJ04st3y/fv3g5OSEkydPIjExEWFhYTA0NAQAZGRkoFOnTvjkk09w5swZbN68GUeOHMHo0aMrjOHSpUvYsmULfv31V+zduxdJSUkYOXKkMH/x4sVYsGAB5s+fjzNnziAoKAjdunXDxYsXAQBLlizBzp07sWXLFqSnp2PDhg3CCXBZJ06cAAD88ccfyM7Oxvbt27XK/Oc//8Hdu3cRHx8vTLt37x727t2Lfv36AQAOHz6Mzz77DF988QXS0tKwatUqREdHY9asWRXu6/79+5Geno64uDjs2rULAFBUVISZM2ciJSUFO3bsQFZWltA5cnZ2xrZt2wAA6enpyM7OxuLFiwEAs2fPxvr167Fy5UqcO3cO48aNQ//+/YUT93v37iEtLQ1vv/12hTFVhZ2dHYqLixEbGwsiqrDshAkTsGDBApw8eRLW1tbo2rUrioqKAFStjXz22WfYtGkTlixZgvPnz2PVqlUwNTXV2k6rVq2Qnp4OANi2bRuys7PRqlUrrXK7d+/Gxx9/jM6dOyMpKQn79+9HixYtdMZ+8uRJAEBUVBSys7Nx8uRJuLm5ITAwEFFRUaKyUVFRGDBgAKTSZ6cqcrkcfn5+OHz4cIX1wxhjLw0xxhh7IaGhoSSTycjExER49erVS2fZrVu3Ut26dYX3UVFRZG5uLrw3MzOj6OhoncsOHjyYhg0bJpp2+PBhkkql9PjxY53LhIeHk0wmo+vXrwvTfvvtN5JKpZSdnU1ERA4ODjRr1izRcu+88w6NHDmSiIg+//xzat++PanVap3bAECxsbFERJSZmUkAKCkpSVQmNDSUunfvLrzv3r07DRo0SHi/atUqcnBwoJKSEiIi6tChA0VERIjW8eOPP5K9vb3OGDTbsLW1padPn5Zbhojo5MmTBIAKCgqIiCg+Pp4A0P3794UyT548IWNjYzp27Jho2cGDB1NwcDARESUlJREAunr1arnbKl03lZkyZQoZGBhQnTp1qFOnTjR37lzKyckR5mvijImJEabdvXuXlEolbd68WYivojaSnp5OACguLk5nDGXr4v79+wSA4uPjhTJl22xAQAD169ev3P1ydXWlRYsWCe911cnmzZvJ0tKSnjx5QkREiYmJJJFIKDMzU1Tu448/pgEDBpS7LcYYe5n4ihNjjNWAdu3aITk5WXgtWbIEwLMrLx06dICjoyPMzMwQEhKCu3fv4tGjRzrXM378eAwZMgSBgYGYM2cOMjIyhHkpKSmIjo6Gqamp8AoKCoJarUZmZma5sbm4uMDR0VF4HxAQALVajfT0dOTn5+PmzZto3bq1aJnWrVvj/PnzAJ7dApecnAxvb2+MGTMGv//++3PXk0a/fv2wbds2PH36FACwYcMG9O3bV7i6kJKSghkzZoj2dejQocjOzi637gCgcePGkMvlommJiYno2rUrXFxcYGZmhjZt2gAArl69Wu56Ll26hEePHqFjx46iGNavXy98Jo8fPwYAGBkZVWvfIyIiROvUxDFr1izk5ORg5cqVaNSoEVauXAkfHx+cPXtWtHxAQIDwd506deDt7S18VpW1keTkZMhkMqEOakJycjI6dOjwQuvo0aMHZDIZYmNjATzLNtmuXTutK5tKpbLCz58xxl4m7jgxxlgNMDExgaenp/Cyt7dHVlYWPvroIzRp0gTbtm1DYmKikDCgvMQF06dPx7lz59ClSxccOHAADRs2FE4mHzx4gOHDh4s6aCkpKbh48SLq1av30vatWbNmyMzMxMyZM/H48WP07t0bvXr1eqF1du3aFUSE3bt349q1azh8+LBwmx7wbF+/+eYb0b6ePXsWFy9erLCjYmJiInr/8OFDBAUFQaVSYcOGDTh58qRQnxUlj9A8h7Z7925RDGlpacJzTlZWVgCA+/fvV2vfR4wYIVqng4ODMK9u3br4z3/+g/nz5+P8+fNwcHDA/Pnzq7zuytqIUqmsVqxVURPrlMvl+OyzzxAVFYXCwkJs3LhR9Lybxr1792Btbf3C22OMsedhoO8AGGPsTZWYmAi1Wo0FCxYIV1K2bNlS6XJeXl7w8vLCuHHjEBwcjKioKHz88cdo1qwZ0tLS4OnpWa04rl69ips3bwon6H/99RekUim8vb2hUqng4OCAo0ePiq5CHD16VPScikqlQp8+fdCnTx/06tULnTp1wr1791CnTh3RtjRXe0pKSiqMycjICD179sSGDRtw6dIleHt7o1mzZsL8Zs2aIT09vdr7Wtbff/+Nu3fvYs6cOXB2dgYArSQEumJu2LAhFAoFrl69Wu7VmXr16kGlUiEtLQ1eXl5VjqlOnTpa9aaLXC5HvXr1tJJ//PXXX3BxcQHwrNN24cIFNGjQAAAqbSONGzeGWq3GoUOHEBgYWOWYK9KkSRPs378fAwcOrFJ5Q0NDne1jyJAheOutt7B8+XIUFxejZ8+eWmVSU1NfuNPOGGPPiztOjDH2knh6eqKoqAhLly5F165dcfToUaxcubLc8o8fP8aECRPQq1cvuLu74/r16zh58iQ++eQTAMCkSZPQsmVLjB49GkOGDIGJiQnS0tIQFxeH77//vtz1GhkZITQ0FPPnz0d+fj7GjBmD3r17w87ODsCzZAPh4eGoV68e/Pz8EBUVheTkZGzYsAEAsHDhQtjb26Np06aQSqXYunUr7OzsdKYXt7GxgVKpxN69e+Hk5AQjIyPRmECl9evXDx999BHOnTuH/v37i+b997//xUcffQQXFxf06tULUqkUKSkpSE1NxbffflthvZfm4uICuVyOpUuXYsSIEUhNTcXMmTNFZVxdXSGRSLBr1y507twZSqUSZmZm+OqrrzBu3Dio1Wq8++67yMvLw9GjR6FSqRAaGgqpVIrAwEAcOXJENLjvgwcPRGN4aW6Rq1OnjtDhKWvXrl2IiYlB37594eXlBSLCr7/+ij179mglTZgxYwbq1q0LW1tbTJ06FVZWVsL2K2sjbm5uCA0NxaBBg7BkyRL4+vriypUruH37Nnr37l3lei0tPDwcHTp0QL169dC3b18UFxdjz549mDRpks7ybm5u2L9/P1q3bg2FQgFLS0sAQIMGDdCyZUtMmjQJgwYN0rqSlZWVhRs3btRYh48xxqpN3w9ZMcbY665s4oPSFi5cSPb29qRUKikoKIjWr18vevi+9IP2T58+pb59+5KzszPJ5XJycHCg0aNHixI/nDhxgjp27EimpqZkYmJCTZo00UrsUFp4eDj5+vrS8uXLycHBgYyMjKhXr1507949oUxJSQlNnz6dHB0dydDQkHx9fem3334T5q9evZr8/PzIxMSEVCoVdejQgU6fPi3MR5mH/X/44QdydnYmqVRKbdq0KbeOSkpKyN7engBQRkaGVux79+6lVq1akVKpJJVKRS1atKDVq1eXu6/lfQ4bN24kNzc3UigUFBAQQDt37tRKYDFjxgyys7MjiURCoaGhRESkVqvpu+++I29vbzI0NCRra2sKCgqiQ4cOCcvt2bOHHB0dhaQWRP+XYKHsS7NeXTIyMmjo0KHk5eVFSqWSLCws6J133qGoqCit9f7666/UqFEjksvl1KJFC0pJSRGtq7I28vjxYxo3bhzZ29uTXC4nT09PWrt2rWgb1UkOQUS0bds28vPzI7lcTlZWVtSzZ09hXtnkEDt37iRPT08yMDAgV1dX0XoiIyMJAJ04cUKrjiIiIigoKKjcOmSMsZdNQlRJ3lPGGGOvrenTp2PHjh1ITk7WdyhvJCKCv7+/cFvly3Tw4EG0a9cO9+/fr/XBhGvLzJkzsXXrVpw5c0Y0vbCwEPXr18fGjRu1Epkwxlht4eQQjDHG2HOSSCRYvXp1hQPzsso9ePAAqamp+P777/H5559rzb969SqmTJnCnSbGmF7xM06MMcbYC/Dz84Ofn5++w3itjR49Gps2bUKPHj10ZtPTZKtkjDF94lv1GGOMMcYYY6wSfKseY4wxxhhjjFWCO06MMcYYY4wxVgnuODHGGGOMMcZYJbjjxBhjjDHGGGOV4I4TY4wxxhhjjFWCO06MMcYYY4wxVgnuODHGGGOMMcZYJbjjxBhjjDHGGGOV+H/6OkGskvDxMQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================Blind Test=========================\n",
      "\n",
      "230 train samples for CV\n",
      "58 test samples for blind-test\n",
      "\n",
      "Over sampling: 230 -> 230\n",
      "Add train samples: 26\n",
      "256 new train samples for CV\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\tf28\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.00000010\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_28484\\4287042063.py:400: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================EPOCH 000001:======================\n",
      "   epoch      loss       acc  val_loss  val_acc      rocp   f1     rfa\n",
      "0    1.0  0.693718  0.457031  0.692654      0.5  0.681332  0.0  0.3794\n",
      "RFA : 0.37939952437574315\n",
      "8/8 - 7s - loss: 0.6937 - accuracy: 0.4570 - val_loss: 0.6927 - val_accuracy: 0.5000 - lr: 2.0010e-04 - 7s/epoch - 827ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 2/200\n",
      "======================EPOCH 000002:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "1    2.0  0.692399  0.539062  0.691797  0.586207  0.775268  0.707317  0.685314\n",
      "RFA : 0.685313656751132\n",
      "8/8 - 1s - loss: 0.6924 - accuracy: 0.5391 - val_loss: 0.6918 - val_accuracy: 0.5862 - lr: 4.0010e-04 - 564ms/epoch - 71ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 3/200\n",
      "======================EPOCH 000003:======================\n",
      "   epoch      loss     acc  val_loss   val_acc     rocp        f1       rfa\n",
      "2    3.0  0.691042  0.5625  0.689153  0.706897  0.80975  0.767123  0.758832\n",
      "RFA : 0.7588320300904623\n",
      "8/8 - 0s - loss: 0.6910 - accuracy: 0.5625 - val_loss: 0.6892 - val_accuracy: 0.7069 - lr: 6.0009e-04 - 423ms/epoch - 53ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 4/200\n",
      "8/8 - 1s - loss: 0.6870 - accuracy: 0.5977 - val_loss: 0.6776 - val_accuracy: 0.6034 - lr: 8.0009e-04 - 504ms/epoch - 63ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 5/200\n",
      "======================EPOCH 000005:======================\n",
      "   epoch    loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "4    5.0  0.6578  0.691406  0.612013  0.741379  0.787158  0.782609  0.769543\n",
      "RFA : 0.769543249215402\n",
      "8/8 - 1s - loss: 0.6578 - accuracy: 0.6914 - val_loss: 0.6120 - val_accuracy: 0.7414 - lr: 0.0010 - 873ms/epoch - 109ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 6/200\n",
      "8/8 - 0s - loss: 0.5334 - accuracy: 0.7812 - val_loss: 0.5939 - val_accuracy: 0.7241 - lr: 0.0012 - 412ms/epoch - 51ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 7/200\n",
      "8/8 - 1s - loss: 0.4297 - accuracy: 0.7734 - val_loss: 0.7581 - val_accuracy: 0.6897 - lr: 0.0014 - 610ms/epoch - 76ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 8/200\n",
      "======================EPOCH 000008:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "7    8.0  0.466168  0.757812  0.493584  0.724138  0.868014  0.777778  0.786075\n",
      "RFA : 0.7860747686314731\n",
      "8/8 - 0s - loss: 0.4662 - accuracy: 0.7578 - val_loss: 0.4936 - val_accuracy: 0.7241 - lr: 0.0016 - 434ms/epoch - 54ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 9/200\n",
      "8/8 - 0s - loss: 0.4395 - accuracy: 0.7852 - val_loss: 0.5238 - val_accuracy: 0.7069 - lr: 0.0018 - 394ms/epoch - 49ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 10/200\n",
      "======================EPOCH 000010:======================\n",
      "   epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "9   10.0  0.402129  0.820312  0.448718  0.775862  0.902497  0.811594  0.826359\n",
      "RFA : 0.8263588134280718\n",
      "8/8 - 0s - loss: 0.4021 - accuracy: 0.8203 - val_loss: 0.4487 - val_accuracy: 0.7759 - lr: 0.0020 - 438ms/epoch - 55ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 11/200\n",
      "8/8 - 1s - loss: 0.4039 - accuracy: 0.8203 - val_loss: 0.5241 - val_accuracy: 0.7069 - lr: 0.0022 - 574ms/epoch - 72ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 12/200\n",
      "======================EPOCH 000012:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "11   12.0  0.393084  0.816406  0.440645  0.775862  0.913199  0.811594  0.829569\n",
      "RFA : 0.8295692771617222\n",
      "8/8 - 0s - loss: 0.3931 - accuracy: 0.8164 - val_loss: 0.4406 - val_accuracy: 0.7759 - lr: 0.0024 - 442ms/epoch - 55ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 13/200\n",
      "======================EPOCH 000013:======================\n",
      "    epoch      loss      acc  val_loss   val_acc      rocp        f1      rfa\n",
      "12   13.0  0.351526  0.84375  0.385192  0.793103  0.920333  0.818182  0.84005\n",
      "RFA : 0.8400497272315894\n",
      "8/8 - 0s - loss: 0.3515 - accuracy: 0.8438 - val_loss: 0.3852 - val_accuracy: 0.7931 - lr: 0.0026 - 417ms/epoch - 52ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 14/200\n",
      "======================EPOCH 000014:======================\n",
      "    epoch      loss       acc  val_loss   val_acc    rocp        f1      rfa\n",
      "13   14.0  0.366673  0.835938  0.379787  0.793103  0.9239  0.818182  0.84112\n",
      "RFA : 0.841119881809473\n",
      "8/8 - 0s - loss: 0.3667 - accuracy: 0.8359 - val_loss: 0.3798 - val_accuracy: 0.7931 - lr: 0.0028 - 423ms/epoch - 53ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 15/200\n",
      "======================EPOCH 000015:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "14   15.0  0.329963  0.859375  0.391563  0.793103  0.925089  0.818182  0.841477\n",
      "RFA : 0.8414766000021008\n",
      "8/8 - 1s - loss: 0.3300 - accuracy: 0.8594 - val_loss: 0.3916 - val_accuracy: 0.7931 - lr: 0.0030 - 616ms/epoch - 77ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 16/200\n",
      "8/8 - 0s - loss: 0.4036 - accuracy: 0.8008 - val_loss: 0.3688 - val_accuracy: 0.7931 - lr: 0.0032 - 418ms/epoch - 52ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 17/200\n",
      "8/8 - 0s - loss: 0.3021 - accuracy: 0.8828 - val_loss: 0.4133 - val_accuracy: 0.7759 - lr: 0.0034 - 384ms/epoch - 48ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 18/200\n",
      "======================EPOCH 000018:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "17   18.0  0.407422  0.804688  0.353816  0.862069  0.920333  0.870968  0.882663\n",
      "RFA : 0.8826627236668247\n",
      "8/8 - 0s - loss: 0.4074 - accuracy: 0.8047 - val_loss: 0.3538 - val_accuracy: 0.8621 - lr: 0.0036 - 406ms/epoch - 51ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 19/200\n",
      "8/8 - 0s - loss: 0.3509 - accuracy: 0.8516 - val_loss: 0.3632 - val_accuracy: 0.8103 - lr: 0.0038 - 437ms/epoch - 55ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 20/200\n",
      "8/8 - 0s - loss: 0.3326 - accuracy: 0.8398 - val_loss: 0.3174 - val_accuracy: 0.8276 - lr: 0.0040 - 398ms/epoch - 50ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 21/200\n",
      "8/8 - 0s - loss: 0.3008 - accuracy: 0.8828 - val_loss: 0.4322 - val_accuracy: 0.7586 - lr: 0.0042 - 404ms/epoch - 51ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 22/200\n",
      "8/8 - 0s - loss: 0.3157 - accuracy: 0.8711 - val_loss: 0.4882 - val_accuracy: 0.7931 - lr: 0.0044 - 399ms/epoch - 50ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 23/200\n",
      "8/8 - 0s - loss: 0.3575 - accuracy: 0.8477 - val_loss: 0.3599 - val_accuracy: 0.7759 - lr: 0.0046 - 403ms/epoch - 50ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 24/200\n",
      "8/8 - 0s - loss: 0.3374 - accuracy: 0.8359 - val_loss: 0.3538 - val_accuracy: 0.7931 - lr: 0.0048 - 493ms/epoch - 62ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 25/200\n",
      "8/8 - 0s - loss: 0.3252 - accuracy: 0.8711 - val_loss: 0.3414 - val_accuracy: 0.8103 - lr: 0.0050 - 412ms/epoch - 52ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 26/200\n",
      "======================EPOCH 000026:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "25   26.0  0.315787  0.851562  0.277117  0.913793  0.958383  0.918033  0.928654\n",
      "RFA : 0.9286539191198273\n",
      "8/8 - 0s - loss: 0.3158 - accuracy: 0.8516 - val_loss: 0.2771 - val_accuracy: 0.9138 - lr: 0.0052 - 425ms/epoch - 53ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 27/200\n",
      "8/8 - 0s - loss: 0.3033 - accuracy: 0.8594 - val_loss: 0.3875 - val_accuracy: 0.7931 - lr: 0.0054 - 399ms/epoch - 50ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 28/200\n",
      "8/8 - 0s - loss: 0.2853 - accuracy: 0.8594 - val_loss: 0.3343 - val_accuracy: 0.8793 - lr: 0.0056 - 399ms/epoch - 50ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 29/200\n",
      "8/8 - 0s - loss: 0.3316 - accuracy: 0.8477 - val_loss: 0.4380 - val_accuracy: 0.8103 - lr: 0.0058 - 402ms/epoch - 50ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 30/200\n",
      "8/8 - 0s - loss: 0.4077 - accuracy: 0.8203 - val_loss: 0.4387 - val_accuracy: 0.8103 - lr: 0.0060 - 404ms/epoch - 50ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 31/200\n",
      "8/8 - 0s - loss: 0.3655 - accuracy: 0.8125 - val_loss: 0.3429 - val_accuracy: 0.8448 - lr: 0.0062 - 403ms/epoch - 50ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 32/200\n",
      "8/8 - 1s - loss: 0.3459 - accuracy: 0.8398 - val_loss: 0.2953 - val_accuracy: 0.9138 - lr: 0.0064 - 628ms/epoch - 79ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 33/200\n",
      "8/8 - 0s - loss: 0.3535 - accuracy: 0.8203 - val_loss: 0.4702 - val_accuracy: 0.7759 - lr: 0.0066 - 402ms/epoch - 50ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 34/200\n",
      "8/8 - 0s - loss: 0.4881 - accuracy: 0.7734 - val_loss: 0.4100 - val_accuracy: 0.7759 - lr: 0.0068 - 394ms/epoch - 49ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 35/200\n",
      "8/8 - 0s - loss: 0.4017 - accuracy: 0.8086 - val_loss: 0.3707 - val_accuracy: 0.8276 - lr: 0.0070 - 381ms/epoch - 48ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 36/200\n",
      "8/8 - 0s - loss: 0.3380 - accuracy: 0.8594 - val_loss: 0.5011 - val_accuracy: 0.7759 - lr: 0.0072 - 372ms/epoch - 46ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 37/200\n",
      "8/8 - 0s - loss: 0.4193 - accuracy: 0.8008 - val_loss: 0.4452 - val_accuracy: 0.7931 - lr: 0.0074 - 391ms/epoch - 49ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 38/200\n",
      "8/8 - 0s - loss: 0.3504 - accuracy: 0.8398 - val_loss: 0.2907 - val_accuracy: 0.8966 - lr: 0.0076 - 380ms/epoch - 48ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 39/200\n",
      "8/8 - 0s - loss: 0.2549 - accuracy: 0.8945 - val_loss: 0.4108 - val_accuracy: 0.8621 - lr: 0.0078 - 401ms/epoch - 50ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 40/200\n",
      "8/8 - 0s - loss: 0.3703 - accuracy: 0.8398 - val_loss: 0.3062 - val_accuracy: 0.8966 - lr: 0.0080 - 393ms/epoch - 49ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 41/200\n",
      "8/8 - 0s - loss: 0.3074 - accuracy: 0.8828 - val_loss: 0.3396 - val_accuracy: 0.8103 - lr: 0.0082 - 371ms/epoch - 46ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 42/200\n",
      "8/8 - 1s - loss: 0.3325 - accuracy: 0.8750 - val_loss: 0.3182 - val_accuracy: 0.8621 - lr: 0.0084 - 504ms/epoch - 63ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 43/200\n",
      "8/8 - 0s - loss: 0.3437 - accuracy: 0.8516 - val_loss: 0.4047 - val_accuracy: 0.8448 - lr: 0.0086 - 397ms/epoch - 50ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 44/200\n",
      "8/8 - 0s - loss: 0.3147 - accuracy: 0.8828 - val_loss: 0.3000 - val_accuracy: 0.8793 - lr: 0.0088 - 367ms/epoch - 46ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 45/200\n",
      "8/8 - 0s - loss: 0.3261 - accuracy: 0.8516 - val_loss: 0.4125 - val_accuracy: 0.8103 - lr: 0.0090 - 348ms/epoch - 44ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 46/200\n",
      "8/8 - 0s - loss: 0.3775 - accuracy: 0.8359 - val_loss: 0.3364 - val_accuracy: 0.8621 - lr: 0.0092 - 485ms/epoch - 61ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 47/200\n",
      "8/8 - 0s - loss: 0.3529 - accuracy: 0.8359 - val_loss: 0.3937 - val_accuracy: 0.8103 - lr: 0.0094 - 390ms/epoch - 49ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 48/200\n",
      "8/8 - 0s - loss: 0.2923 - accuracy: 0.8945 - val_loss: 0.3096 - val_accuracy: 0.8621 - lr: 0.0096 - 356ms/epoch - 45ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 49/200\n",
      "8/8 - 0s - loss: 0.2674 - accuracy: 0.8945 - val_loss: 0.4571 - val_accuracy: 0.8276 - lr: 0.0098 - 295ms/epoch - 37ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 50/200\n",
      "8/8 - 0s - loss: 0.3250 - accuracy: 0.8672 - val_loss: 0.3198 - val_accuracy: 0.8448 - lr: 0.0100 - 307ms/epoch - 38ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 51/200\n",
      "8/8 - 0s - loss: 0.3018 - accuracy: 0.8828 - val_loss: 0.2853 - val_accuracy: 0.8621 - lr: 0.0098 - 496ms/epoch - 62ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 52/200\n",
      "8/8 - 0s - loss: 0.2576 - accuracy: 0.9062 - val_loss: 0.2975 - val_accuracy: 0.8621 - lr: 0.0096 - 399ms/epoch - 50ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 53/200\n",
      "8/8 - 0s - loss: 0.2810 - accuracy: 0.8672 - val_loss: 0.3174 - val_accuracy: 0.8276 - lr: 0.0094 - 380ms/epoch - 48ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 54/200\n",
      "8/8 - 0s - loss: 0.2996 - accuracy: 0.8789 - val_loss: 0.2805 - val_accuracy: 0.8103 - lr: 0.0092 - 393ms/epoch - 49ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 55/200\n",
      "8/8 - 0s - loss: 0.3327 - accuracy: 0.8359 - val_loss: 0.3558 - val_accuracy: 0.8103 - lr: 0.0090 - 382ms/epoch - 48ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 56/200\n",
      "======================EPOCH 000056:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "55   56.0  0.309117  0.867188  0.255624  0.913793  0.960761  0.918033  0.929367\n",
      "RFA : 0.9293673555050832\n",
      "8/8 - 1s - loss: 0.3091 - accuracy: 0.8672 - val_loss: 0.2556 - val_accuracy: 0.9138 - lr: 0.0088 - 568ms/epoch - 71ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 57/200\n",
      "8/8 - 0s - loss: 0.3208 - accuracy: 0.8672 - val_loss: 0.3946 - val_accuracy: 0.8448 - lr: 0.0086 - 376ms/epoch - 47ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 58/200\n",
      "8/8 - 0s - loss: 0.2954 - accuracy: 0.8711 - val_loss: 0.2586 - val_accuracy: 0.8966 - lr: 0.0084 - 371ms/epoch - 46ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 59/200\n",
      "8/8 - 0s - loss: 0.3224 - accuracy: 0.8828 - val_loss: 0.3226 - val_accuracy: 0.8448 - lr: 0.0082 - 379ms/epoch - 47ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 60/200\n",
      "8/8 - 0s - loss: 0.2810 - accuracy: 0.8789 - val_loss: 0.4104 - val_accuracy: 0.8448 - lr: 0.0080 - 488ms/epoch - 61ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 61/200\n",
      "8/8 - 0s - loss: 0.2871 - accuracy: 0.8867 - val_loss: 0.3239 - val_accuracy: 0.8793 - lr: 0.0078 - 391ms/epoch - 49ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 62/200\n",
      "8/8 - 0s - loss: 0.3189 - accuracy: 0.8359 - val_loss: 0.2947 - val_accuracy: 0.8621 - lr: 0.0076 - 396ms/epoch - 50ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 63/200\n",
      "8/8 - 0s - loss: 0.2566 - accuracy: 0.8789 - val_loss: 0.2812 - val_accuracy: 0.8966 - lr: 0.0074 - 385ms/epoch - 48ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 64/200\n",
      "8/8 - 0s - loss: 0.2643 - accuracy: 0.9062 - val_loss: 0.2799 - val_accuracy: 0.8793 - lr: 0.0072 - 393ms/epoch - 49ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 65/200\n",
      "8/8 - 1s - loss: 0.2688 - accuracy: 0.8789 - val_loss: 0.3695 - val_accuracy: 0.7759 - lr: 0.0070 - 533ms/epoch - 67ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 66/200\n",
      "8/8 - 0s - loss: 0.2767 - accuracy: 0.8750 - val_loss: 0.2661 - val_accuracy: 0.8793 - lr: 0.0068 - 384ms/epoch - 48ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 67/200\n",
      "8/8 - 0s - loss: 0.2809 - accuracy: 0.8867 - val_loss: 0.3154 - val_accuracy: 0.8448 - lr: 0.0066 - 395ms/epoch - 49ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 68/200\n",
      "8/8 - 0s - loss: 0.2540 - accuracy: 0.8945 - val_loss: 0.2990 - val_accuracy: 0.8966 - lr: 0.0064 - 383ms/epoch - 48ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 69/200\n",
      "======================EPOCH 000069:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "68   69.0  0.277214  0.890625  0.265354  0.931035  0.954816  0.931034  0.938169\n",
      "RFA : 0.9381688545242075\n",
      "8/8 - 0s - loss: 0.2772 - accuracy: 0.8906 - val_loss: 0.2654 - val_accuracy: 0.9310 - lr: 0.0062 - 447ms/epoch - 56ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 70/200\n",
      "8/8 - 0s - loss: 0.2682 - accuracy: 0.9023 - val_loss: 0.3296 - val_accuracy: 0.8793 - lr: 0.0060 - 368ms/epoch - 46ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 71/200\n",
      "8/8 - 0s - loss: 0.2361 - accuracy: 0.9102 - val_loss: 0.3087 - val_accuracy: 0.8793 - lr: 0.0058 - 394ms/epoch - 49ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 72/200\n",
      "8/8 - 0s - loss: 0.2248 - accuracy: 0.9141 - val_loss: 0.3239 - val_accuracy: 0.8966 - lr: 0.0056 - 389ms/epoch - 49ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 73/200\n",
      "8/8 - 0s - loss: 0.2986 - accuracy: 0.8984 - val_loss: 0.3399 - val_accuracy: 0.8448 - lr: 0.0054 - 388ms/epoch - 49ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 74/200\n",
      "8/8 - 1s - loss: 0.2313 - accuracy: 0.9062 - val_loss: 0.3758 - val_accuracy: 0.8103 - lr: 0.0052 - 612ms/epoch - 76ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 75/200\n",
      "8/8 - 0s - loss: 0.2076 - accuracy: 0.9023 - val_loss: 0.3171 - val_accuracy: 0.8793 - lr: 0.0050 - 406ms/epoch - 51ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 76/200\n",
      "8/8 - 0s - loss: 0.3098 - accuracy: 0.8945 - val_loss: 0.2629 - val_accuracy: 0.9138 - lr: 0.0048 - 383ms/epoch - 48ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 77/200\n",
      "8/8 - 0s - loss: 0.2446 - accuracy: 0.9141 - val_loss: 0.2318 - val_accuracy: 0.9138 - lr: 0.0046 - 396ms/epoch - 50ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 78/200\n",
      "8/8 - 0s - loss: 0.2021 - accuracy: 0.9219 - val_loss: 0.2593 - val_accuracy: 0.8966 - lr: 0.0044 - 393ms/epoch - 49ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 79/200\n",
      "8/8 - 0s - loss: 0.2369 - accuracy: 0.9180 - val_loss: 0.3351 - val_accuracy: 0.8276 - lr: 0.0042 - 405ms/epoch - 51ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 80/200\n",
      "8/8 - 0s - loss: 0.2826 - accuracy: 0.8828 - val_loss: 0.3029 - val_accuracy: 0.8621 - lr: 0.0040 - 376ms/epoch - 47ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 81/200\n",
      "8/8 - 0s - loss: 0.2823 - accuracy: 0.8789 - val_loss: 0.2955 - val_accuracy: 0.8793 - lr: 0.0038 - 370ms/epoch - 46ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 82/200\n",
      "8/8 - 0s - loss: 0.2396 - accuracy: 0.8984 - val_loss: 0.2878 - val_accuracy: 0.8966 - lr: 0.0036 - 374ms/epoch - 47ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 83/200\n",
      "8/8 - 0s - loss: 0.2792 - accuracy: 0.8945 - val_loss: 0.3156 - val_accuracy: 0.8793 - lr: 0.0034 - 366ms/epoch - 46ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 84/200\n",
      "8/8 - 0s - loss: 0.2306 - accuracy: 0.9062 - val_loss: 0.3237 - val_accuracy: 0.8621 - lr: 0.0032 - 294ms/epoch - 37ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 85/200\n",
      "8/8 - 0s - loss: 0.2039 - accuracy: 0.9297 - val_loss: 0.2709 - val_accuracy: 0.8793 - lr: 0.0030 - 301ms/epoch - 38ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 86/200\n",
      "8/8 - 0s - loss: 0.2433 - accuracy: 0.9102 - val_loss: 0.3035 - val_accuracy: 0.8793 - lr: 0.0028 - 282ms/epoch - 35ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 87/200\n",
      "8/8 - 0s - loss: 0.2307 - accuracy: 0.9102 - val_loss: 0.2639 - val_accuracy: 0.8793 - lr: 0.0026 - 294ms/epoch - 37ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 88/200\n",
      "8/8 - 0s - loss: 0.2213 - accuracy: 0.9180 - val_loss: 0.2969 - val_accuracy: 0.9138 - lr: 0.0024 - 292ms/epoch - 37ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 89/200\n",
      "8/8 - 0s - loss: 0.1789 - accuracy: 0.9297 - val_loss: 0.2734 - val_accuracy: 0.8966 - lr: 0.0022 - 313ms/epoch - 39ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 90/200\n",
      "8/8 - 0s - loss: 0.2426 - accuracy: 0.8984 - val_loss: 0.2791 - val_accuracy: 0.9138 - lr: 0.0020 - 394ms/epoch - 49ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 91/200\n",
      "8/8 - 0s - loss: 0.2011 - accuracy: 0.9102 - val_loss: 0.2561 - val_accuracy: 0.8793 - lr: 0.0018 - 378ms/epoch - 47ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 92/200\n",
      "8/8 - 0s - loss: 0.2181 - accuracy: 0.9180 - val_loss: 0.2571 - val_accuracy: 0.8966 - lr: 0.0016 - 382ms/epoch - 48ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 93/200\n",
      "8/8 - 0s - loss: 0.1647 - accuracy: 0.9414 - val_loss: 0.2691 - val_accuracy: 0.8621 - lr: 0.0014 - 391ms/epoch - 49ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 94/200\n",
      "8/8 - 0s - loss: 0.2139 - accuracy: 0.9180 - val_loss: 0.2735 - val_accuracy: 0.9138 - lr: 0.0012 - 390ms/epoch - 49ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 95/200\n",
      "8/8 - 0s - loss: 0.2661 - accuracy: 0.9219 - val_loss: 0.2729 - val_accuracy: 0.8966 - lr: 0.0010 - 413ms/epoch - 52ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 96/200\n",
      "8/8 - 0s - loss: 0.1945 - accuracy: 0.9297 - val_loss: 0.2727 - val_accuracy: 0.8966 - lr: 8.0009e-04 - 380ms/epoch - 48ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 97/200\n",
      "8/8 - 0s - loss: 0.1850 - accuracy: 0.9180 - val_loss: 0.2620 - val_accuracy: 0.8966 - lr: 6.0009e-04 - 381ms/epoch - 48ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 98/200\n",
      "8/8 - 1s - loss: 0.2279 - accuracy: 0.9102 - val_loss: 0.2560 - val_accuracy: 0.8966 - lr: 4.0010e-04 - 565ms/epoch - 71ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 99/200\n",
      "8/8 - 0s - loss: 0.1875 - accuracy: 0.9297 - val_loss: 0.2611 - val_accuracy: 0.8966 - lr: 2.0010e-04 - 372ms/epoch - 46ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 100/200\n",
      "8/8 - 0s - loss: 0.2560 - accuracy: 0.9102 - val_loss: 0.2639 - val_accuracy: 0.8966 - lr: 1.0000e-07 - 370ms/epoch - 46ms/step\n",
      "Learning rate: 0.00000010\n",
      "Epoch 101/200\n",
      "8/8 - 0s - loss: 0.2429 - accuracy: 0.8984 - val_loss: 0.2642 - val_accuracy: 0.8966 - lr: 2.0010e-04 - 373ms/epoch - 47ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 102/200\n",
      "8/8 - 0s - loss: 0.2019 - accuracy: 0.9180 - val_loss: 0.2630 - val_accuracy: 0.8966 - lr: 4.0010e-04 - 378ms/epoch - 47ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 103/200\n",
      "8/8 - 0s - loss: 0.1885 - accuracy: 0.9180 - val_loss: 0.2649 - val_accuracy: 0.8966 - lr: 6.0009e-04 - 404ms/epoch - 50ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 104/200\n",
      "8/8 - 0s - loss: 0.2031 - accuracy: 0.9219 - val_loss: 0.2630 - val_accuracy: 0.8966 - lr: 8.0009e-04 - 428ms/epoch - 54ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 105/200\n",
      "8/8 - 0s - loss: 0.2341 - accuracy: 0.9180 - val_loss: 0.2832 - val_accuracy: 0.8966 - lr: 0.0010 - 393ms/epoch - 49ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 106/200\n",
      "8/8 - 0s - loss: 0.2184 - accuracy: 0.9102 - val_loss: 0.3056 - val_accuracy: 0.8966 - lr: 0.0012 - 383ms/epoch - 48ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 107/200\n",
      "8/8 - 0s - loss: 0.2313 - accuracy: 0.9258 - val_loss: 0.2910 - val_accuracy: 0.8966 - lr: 0.0014 - 372ms/epoch - 46ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 108/200\n",
      "8/8 - 0s - loss: 0.2435 - accuracy: 0.8945 - val_loss: 0.2863 - val_accuracy: 0.8793 - lr: 0.0016 - 394ms/epoch - 49ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 109/200\n",
      "8/8 - 0s - loss: 0.2365 - accuracy: 0.9102 - val_loss: 0.2889 - val_accuracy: 0.8966 - lr: 0.0018 - 394ms/epoch - 49ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 110/200\n",
      "8/8 - 0s - loss: 0.1956 - accuracy: 0.9180 - val_loss: 0.2877 - val_accuracy: 0.8966 - lr: 0.0020 - 379ms/epoch - 47ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 111/200\n",
      "8/8 - 0s - loss: 0.1809 - accuracy: 0.9336 - val_loss: 0.2654 - val_accuracy: 0.8966 - lr: 0.0022 - 383ms/epoch - 48ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 112/200\n",
      "8/8 - 1s - loss: 0.2196 - accuracy: 0.9102 - val_loss: 0.3021 - val_accuracy: 0.8966 - lr: 0.0024 - 556ms/epoch - 69ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 113/200\n",
      "8/8 - 0s - loss: 0.2460 - accuracy: 0.9219 - val_loss: 0.2400 - val_accuracy: 0.9138 - lr: 0.0026 - 460ms/epoch - 58ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 114/200\n",
      "8/8 - 0s - loss: 0.2209 - accuracy: 0.9023 - val_loss: 0.2721 - val_accuracy: 0.8966 - lr: 0.0028 - 371ms/epoch - 46ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 115/200\n",
      "8/8 - 0s - loss: 0.1849 - accuracy: 0.9258 - val_loss: 0.3120 - val_accuracy: 0.8448 - lr: 0.0030 - 378ms/epoch - 47ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 116/200\n",
      "8/8 - 0s - loss: 0.1930 - accuracy: 0.9180 - val_loss: 0.2703 - val_accuracy: 0.8448 - lr: 0.0032 - 384ms/epoch - 48ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 117/200\n",
      "8/8 - 0s - loss: 0.2449 - accuracy: 0.9023 - val_loss: 0.2602 - val_accuracy: 0.9138 - lr: 0.0034 - 392ms/epoch - 49ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 118/200\n",
      "8/8 - 0s - loss: 0.2035 - accuracy: 0.9062 - val_loss: 0.3004 - val_accuracy: 0.8621 - lr: 0.0036 - 411ms/epoch - 51ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 119/200\n",
      "8/8 - 0s - loss: 0.2139 - accuracy: 0.9336 - val_loss: 0.3736 - val_accuracy: 0.8448 - lr: 0.0038 - 391ms/epoch - 49ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 120/200\n",
      "8/8 - 0s - loss: 0.2496 - accuracy: 0.8867 - val_loss: 0.2853 - val_accuracy: 0.9138 - lr: 0.0040 - 394ms/epoch - 49ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 121/200\n",
      "8/8 - 1s - loss: 0.1700 - accuracy: 0.9414 - val_loss: 0.3175 - val_accuracy: 0.8793 - lr: 0.0042 - 557ms/epoch - 70ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 122/200\n",
      "8/8 - 0s - loss: 0.2242 - accuracy: 0.9219 - val_loss: 0.3123 - val_accuracy: 0.8621 - lr: 0.0044 - 407ms/epoch - 51ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 123/200\n",
      "8/8 - 0s - loss: 0.2871 - accuracy: 0.8828 - val_loss: 0.3346 - val_accuracy: 0.8621 - lr: 0.0046 - 371ms/epoch - 46ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 124/200\n",
      "8/8 - 0s - loss: 0.2361 - accuracy: 0.9023 - val_loss: 0.3854 - val_accuracy: 0.8448 - lr: 0.0048 - 367ms/epoch - 46ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 125/200\n",
      "8/8 - 0s - loss: 0.2462 - accuracy: 0.9102 - val_loss: 0.2425 - val_accuracy: 0.8966 - lr: 0.0050 - 382ms/epoch - 48ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 126/200\n",
      "8/8 - 0s - loss: 0.1921 - accuracy: 0.9297 - val_loss: 0.3213 - val_accuracy: 0.8966 - lr: 0.0052 - 382ms/epoch - 48ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 127/200\n",
      "8/8 - 0s - loss: 0.2152 - accuracy: 0.9141 - val_loss: 0.3201 - val_accuracy: 0.8793 - lr: 0.0054 - 450ms/epoch - 56ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 128/200\n",
      "8/8 - 0s - loss: 0.2563 - accuracy: 0.8945 - val_loss: 0.3369 - val_accuracy: 0.8621 - lr: 0.0056 - 393ms/epoch - 49ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 129/200\n",
      "8/8 - 0s - loss: 0.2420 - accuracy: 0.9062 - val_loss: 0.3497 - val_accuracy: 0.8793 - lr: 0.0058 - 378ms/epoch - 47ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 130/200\n",
      "8/8 - 1s - loss: 0.2402 - accuracy: 0.8945 - val_loss: 0.3209 - val_accuracy: 0.8448 - lr: 0.0060 - 602ms/epoch - 75ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 131/200\n",
      "8/8 - 0s - loss: 0.2341 - accuracy: 0.9062 - val_loss: 0.3265 - val_accuracy: 0.8448 - lr: 0.0062 - 439ms/epoch - 55ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 132/200\n",
      "8/8 - 0s - loss: 0.2062 - accuracy: 0.9062 - val_loss: 0.2569 - val_accuracy: 0.8966 - lr: 0.0064 - 383ms/epoch - 48ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 133/200\n",
      "8/8 - 1s - loss: 0.2318 - accuracy: 0.9062 - val_loss: 0.3244 - val_accuracy: 0.8966 - lr: 0.0066 - 1s/epoch - 183ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 134/200\n",
      "8/8 - 0s - loss: 0.1934 - accuracy: 0.9258 - val_loss: 0.2349 - val_accuracy: 0.9138 - lr: 0.0068 - 456ms/epoch - 57ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 135/200\n",
      "8/8 - 0s - loss: 0.2294 - accuracy: 0.9102 - val_loss: 0.2985 - val_accuracy: 0.8966 - lr: 0.0070 - 409ms/epoch - 51ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 136/200\n",
      "8/8 - 0s - loss: 0.2196 - accuracy: 0.9023 - val_loss: 0.3139 - val_accuracy: 0.8966 - lr: 0.0072 - 372ms/epoch - 47ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 137/200\n",
      "8/8 - 1s - loss: 0.2657 - accuracy: 0.8828 - val_loss: 0.3263 - val_accuracy: 0.8621 - lr: 0.0074 - 543ms/epoch - 68ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 138/200\n",
      "8/8 - 0s - loss: 0.2813 - accuracy: 0.8906 - val_loss: 0.3946 - val_accuracy: 0.7931 - lr: 0.0076 - 385ms/epoch - 48ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 139/200\n",
      "8/8 - 0s - loss: 0.3689 - accuracy: 0.8555 - val_loss: 0.3825 - val_accuracy: 0.8103 - lr: 0.0078 - 373ms/epoch - 47ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 140/200\n",
      "8/8 - 0s - loss: 5.9373 - accuracy: 0.7109 - val_loss: 22.9058 - val_accuracy: 0.5000 - lr: 0.0080 - 384ms/epoch - 48ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 141/200\n",
      "8/8 - 0s - loss: 4908.1016 - accuracy: 0.4961 - val_loss: 1455.6683 - val_accuracy: 0.5000 - lr: 0.0082 - 448ms/epoch - 56ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 142/200\n",
      "8/8 - 1s - loss: 12236.8516 - accuracy: 0.4297 - val_loss: 9468.1377 - val_accuracy: 0.5000 - lr: 0.0084 - 510ms/epoch - 64ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 143/200\n",
      "8/8 - 0s - loss: 30059.0703 - accuracy: 0.5508 - val_loss: 362.3044 - val_accuracy: 0.5000 - lr: 0.0086 - 378ms/epoch - 47ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 144/200\n",
      "8/8 - 0s - loss: 126.3392 - accuracy: 0.5273 - val_loss: 10.7144 - val_accuracy: 0.7069 - lr: 0.0088 - 378ms/epoch - 47ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 145/200\n",
      "8/8 - 1s - loss: 22.3820 - accuracy: 0.6367 - val_loss: 9.9425 - val_accuracy: 0.7414 - lr: 0.0090 - 615ms/epoch - 77ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 146/200\n",
      "8/8 - 0s - loss: 10.3002 - accuracy: 0.7031 - val_loss: 2.8566 - val_accuracy: 0.7931 - lr: 0.0092 - 395ms/epoch - 49ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 147/200\n",
      "8/8 - 0s - loss: 7.8830 - accuracy: 0.7266 - val_loss: 2.9473 - val_accuracy: 0.7069 - lr: 0.0094 - 378ms/epoch - 47ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 148/200\n",
      "8/8 - 0s - loss: 6.9553 - accuracy: 0.7344 - val_loss: 0.8831 - val_accuracy: 0.8793 - lr: 0.0096 - 377ms/epoch - 47ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 149/200\n",
      "8/8 - 0s - loss: 6.6940 - accuracy: 0.7383 - val_loss: 1.7347 - val_accuracy: 0.7931 - lr: 0.0098 - 380ms/epoch - 47ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 150/200\n",
      "8/8 - 0s - loss: 4.3956 - accuracy: 0.7344 - val_loss: 0.6807 - val_accuracy: 0.8966 - lr: 0.0100 - 395ms/epoch - 49ms/step\n",
      "Learning rate: 0.01000000\n",
      "Epoch 151/200\n",
      "8/8 - 0s - loss: 4.8577 - accuracy: 0.7539 - val_loss: 0.9401 - val_accuracy: 0.8448 - lr: 0.0098 - 376ms/epoch - 47ms/step\n",
      "Learning rate: 0.00980000\n",
      "Epoch 152/200\n",
      "8/8 - 0s - loss: 4.4956 - accuracy: 0.7383 - val_loss: 0.9976 - val_accuracy: 0.8621 - lr: 0.0096 - 295ms/epoch - 37ms/step\n",
      "Learning rate: 0.00960000\n",
      "Epoch 153/200\n",
      "8/8 - 0s - loss: 4.2036 - accuracy: 0.7539 - val_loss: 0.9792 - val_accuracy: 0.7759 - lr: 0.0094 - 293ms/epoch - 37ms/step\n",
      "Learning rate: 0.00940001\n",
      "Epoch 154/200\n",
      "8/8 - 0s - loss: 3.0781 - accuracy: 0.7695 - val_loss: 0.5700 - val_accuracy: 0.8793 - lr: 0.0092 - 298ms/epoch - 37ms/step\n",
      "Learning rate: 0.00920001\n",
      "Epoch 155/200\n",
      "8/8 - 0s - loss: 2.6258 - accuracy: 0.8047 - val_loss: 1.3004 - val_accuracy: 0.8276 - lr: 0.0090 - 479ms/epoch - 60ms/step\n",
      "Learning rate: 0.00900001\n",
      "Epoch 156/200\n",
      "8/8 - 0s - loss: 4.1352 - accuracy: 0.7461 - val_loss: 0.9375 - val_accuracy: 0.7759 - lr: 0.0088 - 380ms/epoch - 47ms/step\n",
      "Learning rate: 0.00880001\n",
      "Epoch 157/200\n",
      "8/8 - 0s - loss: 3.2455 - accuracy: 0.7344 - val_loss: 2.5869 - val_accuracy: 0.7586 - lr: 0.0086 - 372ms/epoch - 47ms/step\n",
      "Learning rate: 0.00860001\n",
      "Epoch 158/200\n",
      "8/8 - 0s - loss: 2.9145 - accuracy: 0.7461 - val_loss: 0.6862 - val_accuracy: 0.8276 - lr: 0.0084 - 381ms/epoch - 48ms/step\n",
      "Learning rate: 0.00840002\n",
      "Epoch 159/200\n",
      "8/8 - 0s - loss: 2.1367 - accuracy: 0.7930 - val_loss: 0.7856 - val_accuracy: 0.8448 - lr: 0.0082 - 382ms/epoch - 48ms/step\n",
      "Learning rate: 0.00820002\n",
      "Epoch 160/200\n",
      "8/8 - 0s - loss: 2.2419 - accuracy: 0.7930 - val_loss: 0.7210 - val_accuracy: 0.8448 - lr: 0.0080 - 404ms/epoch - 51ms/step\n",
      "Learning rate: 0.00800002\n",
      "Epoch 161/200\n",
      "8/8 - 0s - loss: 1.5529 - accuracy: 0.8359 - val_loss: 0.6369 - val_accuracy: 0.8793 - lr: 0.0078 - 415ms/epoch - 52ms/step\n",
      "Learning rate: 0.00780002\n",
      "Epoch 162/200\n",
      "8/8 - 0s - loss: 1.8416 - accuracy: 0.8008 - val_loss: 1.0911 - val_accuracy: 0.8621 - lr: 0.0076 - 381ms/epoch - 48ms/step\n",
      "Learning rate: 0.00760002\n",
      "Epoch 163/200\n",
      "8/8 - 0s - loss: 1.7305 - accuracy: 0.8242 - val_loss: 0.6818 - val_accuracy: 0.8621 - lr: 0.0074 - 417ms/epoch - 52ms/step\n",
      "Learning rate: 0.00740003\n",
      "Epoch 164/200\n",
      "8/8 - 1s - loss: 1.9693 - accuracy: 0.8203 - val_loss: 0.6086 - val_accuracy: 0.8621 - lr: 0.0072 - 645ms/epoch - 81ms/step\n",
      "Learning rate: 0.00720003\n",
      "Epoch 165/200\n",
      "8/8 - 0s - loss: 1.5739 - accuracy: 0.8047 - val_loss: 0.6298 - val_accuracy: 0.8448 - lr: 0.0070 - 479ms/epoch - 60ms/step\n",
      "Learning rate: 0.00700003\n",
      "Epoch 166/200\n",
      "8/8 - 0s - loss: 1.6252 - accuracy: 0.8164 - val_loss: 0.6636 - val_accuracy: 0.8621 - lr: 0.0068 - 369ms/epoch - 46ms/step\n",
      "Learning rate: 0.00680003\n",
      "Epoch 167/200\n",
      "8/8 - 0s - loss: 1.5802 - accuracy: 0.8047 - val_loss: 0.6943 - val_accuracy: 0.8103 - lr: 0.0066 - 369ms/epoch - 46ms/step\n",
      "Learning rate: 0.00660003\n",
      "Epoch 168/200\n",
      "8/8 - 0s - loss: 1.7001 - accuracy: 0.7812 - val_loss: 0.9666 - val_accuracy: 0.8621 - lr: 0.0064 - 304ms/epoch - 38ms/step\n",
      "Learning rate: 0.00640004\n",
      "Epoch 169/200\n",
      "8/8 - 0s - loss: 1.4479 - accuracy: 0.8359 - val_loss: 0.8748 - val_accuracy: 0.8621 - lr: 0.0062 - 304ms/epoch - 38ms/step\n",
      "Learning rate: 0.00620004\n",
      "Epoch 170/200\n",
      "8/8 - 0s - loss: 1.7944 - accuracy: 0.8164 - val_loss: 1.3351 - val_accuracy: 0.8276 - lr: 0.0060 - 301ms/epoch - 38ms/step\n",
      "Learning rate: 0.00600004\n",
      "Epoch 171/200\n",
      "8/8 - 0s - loss: 2.3897 - accuracy: 0.7891 - val_loss: 0.8069 - val_accuracy: 0.7931 - lr: 0.0058 - 368ms/epoch - 46ms/step\n",
      "Learning rate: 0.00580004\n",
      "Epoch 172/200\n",
      "8/8 - 0s - loss: 1.9747 - accuracy: 0.7891 - val_loss: 1.1458 - val_accuracy: 0.7759 - lr: 0.0056 - 378ms/epoch - 47ms/step\n",
      "Learning rate: 0.00560004\n",
      "Epoch 173/200\n",
      "8/8 - 0s - loss: 1.6483 - accuracy: 0.7969 - val_loss: 1.7315 - val_accuracy: 0.7759 - lr: 0.0054 - 391ms/epoch - 49ms/step\n",
      "Learning rate: 0.00540005\n",
      "Epoch 174/200\n",
      "8/8 - 0s - loss: 1.6500 - accuracy: 0.7969 - val_loss: 0.6416 - val_accuracy: 0.8793 - lr: 0.0052 - 390ms/epoch - 49ms/step\n",
      "Learning rate: 0.00520005\n",
      "Epoch 175/200\n",
      "8/8 - 1s - loss: 1.4245 - accuracy: 0.7969 - val_loss: 0.8050 - val_accuracy: 0.8793 - lr: 0.0050 - 677ms/epoch - 85ms/step\n",
      "Learning rate: 0.00500005\n",
      "Epoch 176/200\n",
      "8/8 - 1s - loss: 1.4713 - accuracy: 0.8320 - val_loss: 1.0120 - val_accuracy: 0.8621 - lr: 0.0048 - 926ms/epoch - 116ms/step\n",
      "Learning rate: 0.00480005\n",
      "Epoch 177/200\n",
      "8/8 - 0s - loss: 1.8174 - accuracy: 0.8086 - val_loss: 0.6185 - val_accuracy: 0.8793 - lr: 0.0046 - 403ms/epoch - 50ms/step\n",
      "Learning rate: 0.00460005\n",
      "Epoch 178/200\n",
      "8/8 - 0s - loss: 1.3963 - accuracy: 0.8047 - val_loss: 0.6897 - val_accuracy: 0.8793 - lr: 0.0044 - 404ms/epoch - 50ms/step\n",
      "Learning rate: 0.00440006\n",
      "Epoch 179/200\n",
      "8/8 - 1s - loss: 1.3602 - accuracy: 0.8242 - val_loss: 0.8411 - val_accuracy: 0.8276 - lr: 0.0042 - 1s/epoch - 180ms/step\n",
      "Learning rate: 0.00420006\n",
      "Epoch 180/200\n",
      "8/8 - 0s - loss: 1.4591 - accuracy: 0.8047 - val_loss: 0.7758 - val_accuracy: 0.8103 - lr: 0.0040 - 406ms/epoch - 51ms/step\n",
      "Learning rate: 0.00400006\n",
      "Epoch 181/200\n",
      "8/8 - 0s - loss: 1.1656 - accuracy: 0.8359 - val_loss: 0.5818 - val_accuracy: 0.8793 - lr: 0.0038 - 372ms/epoch - 46ms/step\n",
      "Learning rate: 0.00380006\n",
      "Epoch 182/200\n",
      "8/8 - 0s - loss: 1.2675 - accuracy: 0.8203 - val_loss: 0.5458 - val_accuracy: 0.8621 - lr: 0.0036 - 383ms/epoch - 48ms/step\n",
      "Learning rate: 0.00360006\n",
      "Epoch 183/200\n",
      "8/8 - 0s - loss: 1.2381 - accuracy: 0.8516 - val_loss: 0.5776 - val_accuracy: 0.8966 - lr: 0.0034 - 380ms/epoch - 47ms/step\n",
      "Learning rate: 0.00340007\n",
      "Epoch 184/200\n",
      "8/8 - 1s - loss: 1.0572 - accuracy: 0.8594 - val_loss: 0.7433 - val_accuracy: 0.8103 - lr: 0.0032 - 890ms/epoch - 111ms/step\n",
      "Learning rate: 0.00320007\n",
      "Epoch 185/200\n",
      "8/8 - 0s - loss: 1.1810 - accuracy: 0.8203 - val_loss: 0.7498 - val_accuracy: 0.8793 - lr: 0.0030 - 371ms/epoch - 46ms/step\n",
      "Learning rate: 0.00300007\n",
      "Epoch 186/200\n",
      "8/8 - 0s - loss: 1.4157 - accuracy: 0.8438 - val_loss: 0.5136 - val_accuracy: 0.8793 - lr: 0.0028 - 309ms/epoch - 39ms/step\n",
      "Learning rate: 0.00280007\n",
      "Epoch 187/200\n",
      "8/8 - 1s - loss: 1.4316 - accuracy: 0.8125 - val_loss: 0.5081 - val_accuracy: 0.8793 - lr: 0.0026 - 914ms/epoch - 114ms/step\n",
      "Learning rate: 0.00260007\n",
      "Epoch 188/200\n",
      "8/8 - 1s - loss: 1.1448 - accuracy: 0.8594 - val_loss: 0.5235 - val_accuracy: 0.8621 - lr: 0.0024 - 548ms/epoch - 69ms/step\n",
      "Learning rate: 0.00240008\n",
      "Epoch 189/200\n",
      "8/8 - 0s - loss: 0.9633 - accuracy: 0.8594 - val_loss: 0.5687 - val_accuracy: 0.8966 - lr: 0.0022 - 369ms/epoch - 46ms/step\n",
      "Learning rate: 0.00220008\n",
      "Epoch 190/200\n",
      "8/8 - 0s - loss: 1.0924 - accuracy: 0.8359 - val_loss: 0.5812 - val_accuracy: 0.8966 - lr: 0.0020 - 360ms/epoch - 45ms/step\n",
      "Learning rate: 0.00200008\n",
      "Epoch 191/200\n",
      "8/8 - 0s - loss: 1.2461 - accuracy: 0.8555 - val_loss: 0.5325 - val_accuracy: 0.8793 - lr: 0.0018 - 307ms/epoch - 38ms/step\n",
      "Learning rate: 0.00180008\n",
      "Epoch 192/200\n",
      "8/8 - 1s - loss: 1.6409 - accuracy: 0.7969 - val_loss: 0.5353 - val_accuracy: 0.8621 - lr: 0.0016 - 528ms/epoch - 66ms/step\n",
      "Learning rate: 0.00160008\n",
      "Epoch 193/200\n",
      "8/8 - 0s - loss: 1.1665 - accuracy: 0.8008 - val_loss: 0.5298 - val_accuracy: 0.8621 - lr: 0.0014 - 380ms/epoch - 47ms/step\n",
      "Learning rate: 0.00140009\n",
      "Epoch 194/200\n",
      "8/8 - 0s - loss: 1.1299 - accuracy: 0.8398 - val_loss: 0.5795 - val_accuracy: 0.8621 - lr: 0.0012 - 383ms/epoch - 48ms/step\n",
      "Learning rate: 0.00120009\n",
      "Epoch 195/200\n",
      "8/8 - 1s - loss: 1.1147 - accuracy: 0.8359 - val_loss: 0.5391 - val_accuracy: 0.8621 - lr: 0.0010 - 1s/epoch - 136ms/step\n",
      "Learning rate: 0.00100009\n",
      "Epoch 196/200\n",
      "8/8 - 0s - loss: 1.0732 - accuracy: 0.8164 - val_loss: 0.5365 - val_accuracy: 0.8621 - lr: 8.0009e-04 - 396ms/epoch - 50ms/step\n",
      "Learning rate: 0.00080009\n",
      "Epoch 197/200\n",
      "8/8 - 0s - loss: 0.9913 - accuracy: 0.8320 - val_loss: 0.5449 - val_accuracy: 0.8621 - lr: 6.0009e-04 - 383ms/epoch - 48ms/step\n",
      "Learning rate: 0.00060009\n",
      "Epoch 198/200\n",
      "8/8 - 1s - loss: 1.1372 - accuracy: 0.8594 - val_loss: 0.5491 - val_accuracy: 0.8621 - lr: 4.0010e-04 - 608ms/epoch - 76ms/step\n",
      "Learning rate: 0.00040010\n",
      "Epoch 199/200\n",
      "8/8 - 1s - loss: 1.2382 - accuracy: 0.8164 - val_loss: 0.5375 - val_accuracy: 0.8621 - lr: 2.0010e-04 - 526ms/epoch - 66ms/step\n",
      "Learning rate: 0.00020010\n",
      "Epoch 200/200\n",
      "8/8 - 1s - loss: 0.9732 - accuracy: 0.8477 - val_loss: 0.5385 - val_accuracy: 0.8621 - lr: 1.0000e-07 - 1s/epoch - 169ms/step\n",
      "\n",
      "======================BEST:======================\n",
      "    epoch      loss       acc  val_loss   val_acc      rocp        f1       rfa\n",
      "68   69.0  0.277214  0.890625  0.265354  0.931035  0.954816  0.931034  0.938169\n",
      "\n",
      "\n",
      "\n",
      "Test loss: 0.34702202677726746\n",
      "Test accuracy: 0.8275862336158752\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test loss: 0.29528552293777466\n",
      "Test accuracy: 0.8793103694915771\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test loss: 0.27050742506980896\n",
      "Test accuracy: 0.8448275923728943\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test loss: 0.4188043773174286\n",
      "Test accuracy: 0.8103448152542114\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test loss: 0.36292585730552673\n",
      "Test accuracy: 0.8448275923728943\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test loss: 0.43382027745246887\n",
      "Test accuracy: 0.8103448152542114\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test loss: 0.2951231896877289\n",
      "Test accuracy: 0.8793103694915771\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test loss: 0.31338194012641907\n",
      "Test accuracy: 0.8620689511299133\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test loss: 0.3194968104362488\n",
      "Test accuracy: 0.8620689511299133\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test loss: 0.24520343542099\n",
      "Test accuracy: 0.8793103694915771\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.83      0.87        29\n",
      "           1       0.84      0.93      0.89        29\n",
      "\n",
      "    accuracy                           0.88        58\n",
      "   macro avg       0.88      0.88      0.88        58\n",
      "weighted avg       0.88      0.88      0.88        58\n",
      "\n",
      "[[24  5]\n",
      " [ 2 27]]\n",
      "ACCURACY: 0.8793103448275862\n",
      "PRECISION: 0.84375\n",
      "RECALL: 0.9310344827586207\n",
      "F1: 0.8852459016393444\n",
      "ROC_AUC(Pr.): 0.9464922711058263\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.83      0.87        29\n",
      "           1       0.84      0.93      0.89        29\n",
      "\n",
      "    accuracy                           0.88        58\n",
      "   macro avg       0.88      0.88      0.88        58\n",
      "weighted avg       0.88      0.88      0.88        58\n",
      "\n",
      "[[24  5]\n",
      " [ 2 27]]\n",
      "ACCURACY: 0.8793103448275862\n",
      "PRECISION: 0.84375\n",
      "RECALL: 0.9310344827586207\n",
      "F1: 0.8852459016393444\n",
      "ROC_AUC(Pr.): 0.9464922711058263\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================CV Total=========================\n",
      "\n",
      "        acc      prec    recall        f1      rocp\n",
      "0  0.956522  0.923077  1.000000  0.960000  1.000000\n",
      "1  0.956522  0.923077  1.000000  0.960000  1.000000\n",
      "2  0.869565  0.846154  0.916667  0.880000  0.946970\n",
      "3  0.869565  0.846154  0.916667  0.880000  0.939394\n",
      "4  0.913043  0.857143  1.000000  0.923077  0.962121\n",
      "5  0.956522  0.916667  1.000000  0.956522  0.954545\n",
      "6  0.956522  1.000000  0.909091  0.952381  0.984848\n",
      "7  0.956522  0.916667  1.000000  0.956522  0.916667\n",
      "8  0.913043  0.846154  1.000000  0.916667  0.946970\n",
      "9  1.000000  1.000000  1.000000  1.000000  1.000000\n",
      "             acc       prec     recall         f1       rocp\n",
      "count  10.000000  10.000000  10.000000  10.000000  10.000000\n",
      "mean    0.934783   0.907509   0.974242   0.938517   0.965152\n",
      "std     0.042253   0.059247   0.041525   0.038163   0.029493\n",
      "min     0.869565   0.846154   0.909091   0.880000   0.916667\n",
      "25%     0.913043   0.848901   0.937500   0.918269   0.946970\n",
      "50%     0.956522   0.916667   1.000000   0.954451   0.958333\n",
      "75%     0.956522   0.923077   1.000000   0.959130   0.996212\n",
      "max     1.000000   1.000000   1.000000   1.000000   1.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===========================================================================\n",
      "\n",
      "b51s2100v10s2100-stf_dn0mm10_osbfv2b_mlite-d25_b32e200clr2lr01-85frpt-v190s10\n",
      "\n",
      "Run time: 1303.2767684459686\n",
      "\n",
      "===========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##cv_partial\n",
    "if cv_partial:\n",
    "    n_cv_partial=min(n_cv_partial,df_train.shape[0])\n",
    "    rnd_cv_partial=np.random.RandomState(cv_partial_seed)\n",
    "    cv_sel=rnd_cv_partial.choice(df_train.shape[0],n_cv_partial,replace=False)\n",
    "    data_cv=data_cv[cv_sel]\n",
    "    label_cv=label_cv[cv_sel]\n",
    "    bt_cv=bt_cv[cv_sel]\n",
    "    df_train=df_train.iloc[cv_sel,:]\n",
    "    df_train_std=df_train_std.iloc[cv_sel,:]\n",
    "\n",
    "##save and tracking\n",
    "if save_ds:\n",
    "    df_train.to_csv(outdata_dir+\"/df_train.csv\",index=False)\n",
    "    df_train_std.to_csv(outdata_dir+\"/df_train_std.csv\",index=False)\n",
    "    df_test.to_csv(outdata_dir+\"/df_test.csv\",index=False)\n",
    "    df_test_std.to_csv(outdata_dir+\"/df_test_std.csv\",index=False)\n",
    "\n",
    "    data_cv[:,-1]=ids_cv\n",
    "    data_out[:,-1]=ids_out\n",
    "\n",
    "if cv_as_all:\n",
    "    data_cv=np.concatenate((data_cv,data_out),axis=0)\n",
    "    label_cv=np.concatenate((label_cv,label_out),axis=0)\n",
    "    bt_cv=np.concatenate((bt_cv,bt_out),axis=0)\n",
    "\n",
    "if cv_mode==\"RND\":\n",
    "    splits_cv=make_cv_splits(data_cv,label_cv,n_cv_splits,cv_seed)\n",
    "elif cv_mode==\"BTRND\" or cv_mode==\"CTRND\":\n",
    "    if cv_mode==\"BTRND\":\n",
    "        bt_ct_all=bt_all\n",
    "        bt_ct_cv=bt_cv\n",
    "\n",
    "    splits_cv=[]\n",
    "    for i in range(n_cv_splits):\n",
    "        splits_cv.append([[],[],[],[]])\n",
    "    for bc in bt_ct_all:\n",
    "        bc_ids=np.where(bt_ct_cv==bc)[0]\n",
    "        splits_cv_bc=make_cv_splits(data_cv[bc_ids],label_cv[bc_ids],\n",
    "                                    n_cv_splits,cv_seed)\n",
    "        for i in range(n_cv_splits):\n",
    "            for j in range(4):\n",
    "                splits_cv[i][j].append(splits_cv_bc[i][j])\n",
    "    for i in range(n_cv_splits):\n",
    "        for j in range(4):\n",
    "            splits_cv[i][j]=np.concatenate(splits_cv[i][j],axis=0)\n",
    "\n",
    "if save_ds:\n",
    "    def save_cv_ds(i, cv_type):\n",
    "        cv_dir=outdata_dir+\"/cv%03d\"%(i+1)\n",
    "        if not os.path.isdir(cv_dir):\n",
    "            os.makedirs(cv_dir)\n",
    "\n",
    "        if cv_type==\"train\":\n",
    "            ds_pos=0\n",
    "        else:\n",
    "            ds_pos=2\n",
    "\n",
    "        data_cvtt=splits_cv[i][ds_pos]\n",
    "        snos=np.copy(data_cvtt[:,-1])\n",
    "        data_cvtt[:,-1]=data_cvtt[:,-2]\n",
    "\n",
    "        df_cvtt_org=df_cvtt_org.reset_index(drop=True)\n",
    "        df_cvtt_org.to_csv(cv_dir+\"/%s.csv\"%cv_type,index=False)\n",
    "\n",
    "        df_cvtt=pd.DataFrame(np.copy(data_cvtt),columns=cols_new)\n",
    "        df_cvtt[\"label\"]=splits_cv[i][ds_pos+1]\n",
    "        df_cvtt.to_csv(cv_dir+\"/%s_std.csv\"%cv_type,index=False)\n",
    "    for i in range(n_cv_splits):\n",
    "        save_cv_ds(i, \"train\")\n",
    "        save_cv_ds(i, \"val\")\n",
    "    data_cv[:,-1]=data_cv[:,-2]\n",
    "    data_out[:,-1]=data_out[:,-2]\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "df_hist = pd.DataFrame()\n",
    "hist_acc = []\n",
    "hist_prec = []\n",
    "hist_recall = []\n",
    "hist_f1 = []\n",
    "hist_rocp = []\n",
    "y_allcv = np.array([],dtype=int)\n",
    "rocp_allcv = np.array([],dtype=float)\n",
    "\n",
    "n_cv=0\n",
    "for split_cv in splits_cv: \n",
    "    n_cv+=1\n",
    "    if tb_enabled or conf_permtest!=\"\":\n",
    "        break\n",
    "    print(\"\\n=====================CV[%s]========================\\n\"%n_cv)\n",
    "    in_cv = True\n",
    "\n",
    "    (x_train_org,y_train_org,x_test_org,y_test_org)=split_cv\n",
    "\n",
    "    y_train_cat = to_categorical(y_train_org, num_classes)\n",
    "    y_test_cat = to_categorical(y_test_org, num_classes)\n",
    "    y_test_watch=y_test_cat[:,watch_cls]\n",
    "\n",
    "    print(x_train_org.shape[0], 'train samples')\n",
    "    print(x_test_org.shape[0], 'test samples\\n')\n",
    "\n",
    "    x_train_dl = np.reshape(x_train_org,[-1,img_rows,img_cols,channels])\n",
    "    x_test_dl = np.reshape(x_test_org,[-1,img_rows,img_cols,channels])\n",
    "\n",
    "    y_train_dl = y_train_cat\n",
    "    y_test_dl = y_test_cat\n",
    "\n",
    "    x_train_dl,y_train_bf=fix_batch(\n",
    "            batch_size_cv,osbfb_seed,x_train_dl,y_train_org,\n",
    "            train_ros,batch_fix)\n",
    "    y_train_dl = to_categorical(y_train_bf, num_classes)\n",
    "    x_train_org = np.reshape(x_train_dl,[x_train_dl.shape[0],-1])\n",
    "    y_train_org = y_train_bf\n",
    "    y_train_cat = y_train_dl\n",
    "    print(x_train_org.shape[0], 'new train samples\\n')\n",
    "\n",
    "    if enable_dl:\n",
    "        del(model)\n",
    "        model = create_dlmodel()\n",
    "\n",
    "        if need_training_cv:\n",
    "            callbacks_cv = callbacks_dl.copy()\n",
    "\n",
    "            csv_logger = keras.callbacks.CSVLogger(log_dir+\"/hist_cv%d.csv\"%n_cv)\n",
    "            callbacks_cv.append(csv_logger)\n",
    "\n",
    "            if use_clr:\n",
    "                step_size=math.ceil(\n",
    "                        x_train_dl.shape[0]/batch_size_cv)*epochs_clr_cv \n",
    "                clr = CyclicLR(base_lr=base_lr,max_lr=max_lr_cv,\n",
    "                               step_size=step_size,mode=step_mode)\n",
    "                callbacks_cv[0]=clr\n",
    "\n",
    "            if data_augmentation:\n",
    "                if mix_enable:\n",
    "                    from mixup_generator import MixupGenerator\n",
    "                    generator_cv = MixupGenerator(x_train_dl,y_train_dl,\n",
    "                                                batch_size=batch_size_cv,\n",
    "                                                alpha=mix_alpha, \n",
    "                                                datagen=datagen if mix_datagen else None,\n",
    "                                                )()\n",
    "                else:\n",
    "                    generator_cv = datagen.flow(x_train_dl,y_train_dl,\n",
    "                                                 batch_size=batch_size_cv,\n",
    "                                                 shuffle=True,seed=2100)\n",
    "\n",
    "            if not data_augmentation:\n",
    "                model.fit(x_train_dl,y_train_dl, \n",
    "                          batch_size=batch_size_cv,\n",
    "                          epochs=epochs_cv,\n",
    "                          verbose=verbose_dl,\n",
    "                          shuffle=True,\n",
    "                          callbacks=callbacks_cv,\n",
    "                          validation_data=(x_test_dl,y_test_dl), #y_test_cat y_test_org\n",
    "                          )\n",
    "            else:\n",
    "                model.fit_generator(\n",
    "                            generator=generator_cv,workers=1,\n",
    "                            epochs=epochs_cv,\n",
    "                            validation_data=(x_test_dl,y_test_dl),\n",
    "                            steps_per_epoch=x_train_dl.shape[0] // batch_size_cv,\n",
    "                            callbacks=callbacks_cv,\n",
    "                            use_multiprocessing=False, #True False\n",
    "                            verbose=verbose_dl)\n",
    "            model.save_weights(save_dir+\"/\"+conf_tags_dl+\"_cv%d_last.h5\"%n_cv)\n",
    "\n",
    "        if cv_using_selflast:\n",
    "            model.load_weights(save_dir+\"/\"+conf_tags_dl+\"_cv%d_last.h5\"%n_cv)\n",
    "            label_out_pred_prob_d = model.predict(x_test_dl)\n",
    "\n",
    "            if cv_using_selfbest:\n",
    "                label_out_pred_d = np.argmax(label_out_pred_prob_d,axis=1)\n",
    "                label_out_pred_m = label_out_pred_d\n",
    "                label_out_pred=np.asarray(label_out_pred_m==watch_cls,dtype=np.int32)\n",
    "                label_out_pred_prob = label_out_pred_prob_d[:,watch_cls]\n",
    "\n",
    "                print(classification_report(y_test_org, label_out_pred_m))\n",
    "                print(confusion_matrix(y_test_org, label_out_pred_m))\n",
    "                print(\"ACCURACY:\", accuracy_score(y_test_org, label_out_pred_m))\n",
    "                print(\"PRECISION:\", precision_score(y_test_watch, label_out_pred))\n",
    "                print(\"RECALL:\", recall_score(y_test_watch, label_out_pred))\n",
    "                print(\"F1:\", f1_score(y_test_watch, label_out_pred))\n",
    "                print(\"ROC_AUC(Pr.):\", roc_auc_score(y_test_watch, label_out_pred_prob))\n",
    "\n",
    "        if cv_using_selfbest:\n",
    "            model.load_weights(save_dir+\"/\"+conf_tags_dl+\"_cv%d.h5\"%n_cv)\n",
    "            label_out_pred_prob_d = model.predict(x_test_dl)\n",
    "\n",
    "        label_out_pred_d = np.argmax(label_out_pred_prob_d,axis=1)\n",
    "        label_out_pred_m = label_out_pred_d\n",
    "        label_out_pred=np.asarray(label_out_pred_m==watch_cls,dtype=np.int32)\n",
    "        label_out_pred_prob = label_out_pred_prob_d[:,watch_cls]\n",
    "\n",
    "        print(classification_report(y_test_org, label_out_pred_m))\n",
    "        print(confusion_matrix(y_test_org, label_out_pred_m))\n",
    "        print(\"ACCURACY:\", accuracy_score(y_test_org, label_out_pred_m))\n",
    "        print(\"PRECISION:\", precision_score(y_test_watch, label_out_pred))\n",
    "        print(\"RECALL:\", recall_score(y_test_watch, label_out_pred))\n",
    "        print(\"F1:\", f1_score(y_test_watch, label_out_pred))\n",
    "        print(\"ROC_AUC(Pr.):\", roc_auc_score(y_test_watch, label_out_pred_prob))\n",
    "\n",
    "    if enable_x:\n",
    "        print('\\n')\n",
    "        dtrain_cv = xgb.DMatrix(x_train_org,label=y_train_org,feature_names=cols_new)\n",
    "        dval = xgb.DMatrix(x_test_org,label=y_test_org,feature_names=cols_new)\n",
    "        watchlist  = [(dval,'eval'), (dtrain_cv,'train')]\n",
    "\n",
    "        bst = xgb.train(param_xgb, dtrain_cv, num_round_xgb,\n",
    "                        evals=watchlist,\n",
    "                        verbose_eval=25)\n",
    "\n",
    "\n",
    "\n",
    "        label_out_pred_prob_x = bst.predict(dval,output_margin=True,\n",
    "                                            ntree_limit=ntree_limit)\n",
    "        label_out_pred_prob_x = clf_sigmoid(label_out_pred_prob_x)\n",
    "        label_out_pred_x = np.argmax(label_out_pred_prob_x,axis=1)\n",
    "        label_out_pred_m = label_out_pred_x\n",
    "        label_out_pred=np.asarray(label_out_pred_m==watch_cls,dtype=np.int32)\n",
    "        label_out_pred_prob = label_out_pred_prob_x[:,watch_cls]\n",
    "\n",
    "        print(classification_report(y_test_org, label_out_pred_m))\n",
    "        print(confusion_matrix(y_test_org, label_out_pred_m))\n",
    "        print(\"ACCURACY:\", accuracy_score(y_test_org, label_out_pred_m))\n",
    "        print(\"PRECISION:\", precision_score(y_test_watch, label_out_pred))\n",
    "        print(\"RECALL:\", recall_score(y_test_watch, label_out_pred))\n",
    "        print(\"F1:\", f1_score(y_test_watch, label_out_pred))\n",
    "        print(\"ROC_AUC(Pr.):\", roc_auc_score(y_test_watch, label_out_pred_prob))\n",
    "\n",
    "    if enable_sk:\n",
    "        print('\\n')\n",
    "        ml.fit(x_train_org, y_train_org)\n",
    "\n",
    "        label_out_pred_prob_sk = ml.predict_proba(x_test_org)\n",
    "        label_out_pred_sk = np.argmax(label_out_pred_prob_sk,axis=1)\n",
    "        label_out_pred_m = label_out_pred_sk\n",
    "        label_out_pred=np.asarray(label_out_pred_m==watch_cls,dtype=np.int32)\n",
    "        label_out_pred_prob = label_out_pred_prob_sk[:,watch_cls]\n",
    "\n",
    "        print(classification_report(y_test_org, label_out_pred_m))\n",
    "        print(confusion_matrix(y_test_org, label_out_pred_m))\n",
    "        print(\"ACCURACY:\", accuracy_score(y_test_org, label_out_pred_m))\n",
    "        print(\"PRECISION:\", precision_score(y_test_watch, label_out_pred))\n",
    "        print(\"RECALL:\", recall_score(y_test_watch, label_out_pred))\n",
    "        print(\"F1:\", f1_score(y_test_watch, label_out_pred))\n",
    "        print(\"ROC_AUC(Pr.):\", roc_auc_score(y_test_watch, label_out_pred_prob))\n",
    "\n",
    "    if enable_x and enable_sk:\n",
    "        label_out_pred_prob_hyb = label_out_pred_prob_x*(1-w_sk) + label_out_pred_prob_sk*w_sk\n",
    "    elif enable_x:\n",
    "        label_out_pred_prob_hyb = label_out_pred_prob_x\n",
    "    elif enable_sk:\n",
    "        label_out_pred_prob_hyb = label_out_pred_prob_sk\n",
    "    if enable_dl:\n",
    "        if not enable_x and not enable_sk:\n",
    "            label_out_pred_prob_hyb = label_out_pred_prob_d\n",
    "        else:\n",
    "            label_out_pred_prob_hyb = label_out_pred_prob_hyb*(1-w_dl) + label_out_pred_prob_d*w_dl\n",
    "    label_out_pred_hyb = np.argmax(label_out_pred_prob_hyb,axis=1)\n",
    "    label_out_pred_m = label_out_pred_hyb\n",
    "    label_out_pred=np.asarray(label_out_pred_m==watch_cls,dtype=np.int32)\n",
    "    label_out_pred_prob = label_out_pred_prob_hyb[:,watch_cls]\n",
    "\n",
    "    print('\\n')\n",
    "    print(classification_report(y_test_org, label_out_pred_m))\n",
    "    print(confusion_matrix(y_test_org, label_out_pred_m))\n",
    "    print(\"ACCURACY:\", accuracy_score(y_test_org, label_out_pred_m))\n",
    "    print(\"PRECISION:\", precision_score(y_test_watch, label_out_pred))\n",
    "    print(\"RECALL:\", recall_score(y_test_watch, label_out_pred))\n",
    "    print(\"F1:\", f1_score(y_test_watch, label_out_pred))\n",
    "    print(\"ROC_AUC(Pr.):\", roc_auc_score(y_test_watch, label_out_pred_prob))\n",
    "\n",
    "    hist_acc.append(accuracy_score(y_test_org, label_out_pred_m))\n",
    "    hist_prec.append(precision_score(y_test_watch, label_out_pred))\n",
    "    hist_recall.append(recall_score(y_test_watch, label_out_pred))\n",
    "    hist_f1.append(f1_score(y_test_watch, label_out_pred))\n",
    "    hist_rocp.append(roc_auc_score(y_test_watch, label_out_pred_prob))\n",
    "\n",
    "    y_allcv = np.concatenate([y_allcv,y_test_watch])\n",
    "    rocp_allcv = np.concatenate([rocp_allcv,label_out_pred_prob])\n",
    "\n",
    "    fpr_oc, tpr_oc, _ = roc_curve(\n",
    "        y_test_watch,label_out_pred_prob,pos_label=1)\n",
    "    plt.figure(6661,figsize=(10,8))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_oc, tpr_oc, ':',\n",
    "             label='CV%s(AUC:%0.4f|F1:%0.4f|PR:%0.4f|RC:%0.4f|ACC:%0.4f)'\n",
    "             % (str(n_cv),hist_rocp[-1],hist_f1[-1],\n",
    "                hist_prec[-1],hist_recall[-1],hist_acc[-1])\n",
    "             )\n",
    "\n",
    "df_hist[\"acc\"] = np.array(hist_acc,dtype=float)\n",
    "df_hist[\"prec\"] = np.array(hist_prec,dtype=float)\n",
    "df_hist[\"recall\"] = np.array(hist_recall,dtype=float)\n",
    "df_hist[\"f1\"] = np.array(hist_f1,dtype=float)\n",
    "df_hist[\"rocp\"] = np.array(hist_rocp,dtype=float)\n",
    "    \n",
    "fpr_allcv, tpr_allcv, _ = roc_curve(\n",
    "        y_allcv,rocp_allcv,pos_label=1)\n",
    "\n",
    "print(df_hist.describe())\n",
    "\n",
    "plt.plot(fpr_allcv, tpr_allcv, 'r-',\n",
    "            label='AVG(AUC:%0.4f|F1:%0.4f|PR:%0.4f|RC:%0.4f|ACC:%0.4f)'\n",
    "            % (\n",
    "            df_hist.rocp.mean(),\n",
    "            df_hist.f1.mean(),\n",
    "            df_hist.prec.mean(),\n",
    "            df_hist.recall.mean(),\n",
    "            df_hist.acc.mean()\n",
    "            ))\n",
    "\n",
    "plt.xlabel('False positive rate(1-Specificity)')\n",
    "plt.ylabel('True positive rate(Sensitivity)')\n",
    "plt.title('ROC curve' )\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.savefig(log_dir+\"/roc-cvs.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n========================Blind Test=========================\\n\")\n",
    "in_cv = False\n",
    "\n",
    "x_cv_org = data_cv\n",
    "y_cv_org = label_cv\n",
    "\n",
    "if conf_permtest!=\"\":\n",
    "    rnd_pt=np.random.RandomState(permtest_seed)\n",
    "    y_cv_org=rnd_pt.randint(0,num_classes,size=y_cv_org.shape)\n",
    "\n",
    "x_blind_org = data_out\n",
    "y_blind_org = label_out\n",
    "\n",
    "y_cv_cat = to_categorical(y_cv_org, num_classes)\n",
    "y_blind_cat = to_categorical(y_blind_org, num_classes)\n",
    "y_blind_watch=y_blind_cat[:,watch_cls]\n",
    "\n",
    "print(x_cv_org.shape[0], 'train samples for CV')\n",
    "print(x_blind_org.shape[0], 'test samples for blind-test\\n')\n",
    "\n",
    "x_cv_dl = np.reshape(x_cv_org,[-1,img_rows,img_cols,channels])\n",
    "x_blind_dl = np.reshape(x_blind_org,[-1,img_rows,img_cols,channels])\n",
    "\n",
    "y_cv_dl = y_cv_cat\n",
    "y_blind_dl = y_blind_cat\n",
    "\n",
    "x_cv_dl,y_cv_bf=fix_batch(batch_size,osbfb_seed,\n",
    "                          x_cv_dl,y_cv_org,\n",
    "                          train_ros,batch_fix)\n",
    "y_cv_dl = to_categorical(y_cv_bf, num_classes)\n",
    "x_cv_org = np.reshape(x_cv_dl,[x_cv_dl.shape[0],-1])\n",
    "y_cv_org = y_cv_bf\n",
    "y_cv_cat = y_cv_dl\n",
    "print(x_cv_org.shape[0], 'new train samples for CV\\n')\n",
    "\n",
    "if enable_dl:\n",
    "    del(model)\n",
    "    model = create_dlmodel()\n",
    "\n",
    "    if need_training_bl:\n",
    "        callbacks_bl = callbacks_dl.copy()\n",
    "\n",
    "        if conf_permtest==\"\":\n",
    "            csv_logger = keras.callbacks.CSVLogger(log_dir+\"/hist_blind.csv\")\n",
    "            callbacks_bl.append(csv_logger)\n",
    "\n",
    "        if use_clr:\n",
    "            step_size=math.ceil(x_cv_dl.shape[0]/batch_size)*epochs_clr \n",
    "            clr = CyclicLR(base_lr=base_lr,max_lr=max_lr,\n",
    "                           step_size=step_size,mode=step_mode)\n",
    "            callbacks_bl[0]=clr\n",
    "\n",
    "        if data_augmentation:\n",
    "            if mix_enable:\n",
    "                from mixup_generator import MixupGenerator\n",
    "                generator = MixupGenerator(x_cv_dl, y_cv_dl,\n",
    "                                        batch_size=batch_size,\n",
    "                                        alpha=mix_alpha, \n",
    "                                        datagen=datagen if mix_datagen else None,\n",
    "                                        )()\n",
    "            else:\n",
    "                generator = datagen.flow(x_cv_dl,y_cv_dl,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True,seed=2100)\n",
    "\n",
    "        if not data_augmentation:\n",
    "            model.fit(x_cv_dl, y_cv_dl, \n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      verbose=verbose_dl,\n",
    "                      shuffle=True,\n",
    "                      callbacks=callbacks_bl,\n",
    "                      validation_data=(x_blind_dl, y_blind_dl) \n",
    "                      )\n",
    "        else:\n",
    "            model.fit_generator(\n",
    "                        generator,workers=1,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_blind_dl,y_blind_dl),\n",
    "                        steps_per_epoch=x_cv_dl.shape[0] // batch_size,\n",
    "                        callbacks=callbacks_bl,\n",
    "                        use_multiprocessing=False, \n",
    "                        verbose=verbose_dl)\n",
    "        if conf_permtest==\"\":\n",
    "            model.save_weights(save_dir+\"/\"+conf_tags_dl+\"_blind_last.h5\")\n",
    "\n",
    "    if bl_using_selflast:\n",
    "        model.load_weights(save_dir+\"/\"+conf_tags_dl+\"_blind_last.h5\")\n",
    "        label_out_pred_prob_d = model.predict(x_blind_dl)\n",
    "\n",
    "        if bl_using_selfbest or bl_using_cvbests:\n",
    "            label_out_pred_d = np.argmax(label_out_pred_prob_d,axis=1)\n",
    "            label_out_pred_m = label_out_pred_d\n",
    "            label_out_pred=np.asarray(label_out_pred_m==watch_cls,dtype=np.int32)\n",
    "            label_out_pred_prob = label_out_pred_prob_d[:,watch_cls]\n",
    "\n",
    "            print(classification_report(y_blind_org, label_out_pred_m))\n",
    "            print(confusion_matrix(y_blind_org, label_out_pred_m))\n",
    "            print(\"ACCURACY:\", accuracy_score(y_blind_org, label_out_pred_m))\n",
    "            print(\"PRECISION:\", precision_score(y_blind_watch, label_out_pred))\n",
    "            print(\"RECALL:\", recall_score(y_blind_watch, label_out_pred))\n",
    "            print(\"F1:\", f1_score(y_blind_watch, label_out_pred))\n",
    "            print(\"ROC_AUC(Pr.):\", roc_auc_score(y_blind_watch, label_out_pred_prob))\n",
    "\n",
    "    if bl_using_selfbest:\n",
    "        model.load_weights(save_dir+\"/\"+conf_tags_dl+\"_blind%s.h5\"%conf_permtest)\n",
    "        label_out_pred_prob_d = model.predict(x_blind_dl)\n",
    "\n",
    "        if bl_using_cvbests:\n",
    "            label_out_pred_d = np.argmax(label_out_pred_prob_d,axis=1)\n",
    "            label_out_pred_m = label_out_pred_d\n",
    "            label_out_pred=np.asarray(label_out_pred_m==watch_cls,dtype=np.int32)\n",
    "            label_out_pred_prob = label_out_pred_prob_d[:,watch_cls]\n",
    "\n",
    "            print(classification_report(y_blind_org, label_out_pred_m))\n",
    "            print(confusion_matrix(y_blind_org, label_out_pred_m))\n",
    "            print(\"ACCURACY:\", accuracy_score(y_blind_org, label_out_pred_m))\n",
    "            print(\"PRECISION:\", precision_score(y_blind_watch, label_out_pred))\n",
    "            print(\"RECALL:\", recall_score(y_blind_watch, label_out_pred))\n",
    "            print(\"F1:\", f1_score(y_blind_watch, label_out_pred))\n",
    "            print(\"ROC_AUC(Pr.):\", roc_auc_score(y_blind_watch, label_out_pred_prob))\n",
    "\n",
    "    if bl_using_cvbests:\n",
    "        w_merge_org = w_rocp*df_hist.rocp+w_f1*df_hist.f1+w_acc*df_hist.acc\n",
    "        w_merge = np.exp(w_merge_org*w_merge_exp_factor)/ \\\n",
    "                    np.exp(w_merge_org*w_merge_exp_factor).sum()\n",
    "        label_out_pred_prob_d = np.zeros_like(y_blind_dl,dtype=float)\n",
    "        for i in range(1,len(splits_cv)+1):\n",
    "            model.load_weights(save_dir+\"/\"+conf_tags_dl+\"_cv%d.h5\"%i)\n",
    "            score = model.evaluate(x_blind_dl, y_blind_dl, verbose=0)\n",
    "            print('\\n')\n",
    "            print('Test loss:', score[0])\n",
    "            print('Test accuracy:', score[1])\n",
    "            print('\\n')\n",
    "            label_out_pred_prob_d += np.asarray(model.predict(x_blind_dl)) * w_merge[i-1]\n",
    "\n",
    "    label_out_pred_d = np.argmax(label_out_pred_prob_d,axis=1)\n",
    "    label_out_pred_m = label_out_pred_d\n",
    "    label_out_pred=np.asarray(label_out_pred_m==watch_cls,dtype=np.int32)\n",
    "    label_out_pred_prob = label_out_pred_prob_d[:,watch_cls]\n",
    "\n",
    "    print(classification_report(y_blind_org, label_out_pred_m))\n",
    "    print(confusion_matrix(y_blind_org, label_out_pred_m))\n",
    "    print(\"ACCURACY:\", accuracy_score(y_blind_org, label_out_pred_m))\n",
    "    print(\"PRECISION:\", precision_score(y_blind_watch, label_out_pred))\n",
    "    print(\"RECALL:\", recall_score(y_blind_watch, label_out_pred))\n",
    "    print(\"F1:\", f1_score(y_blind_watch, label_out_pred))\n",
    "    print(\"ROC_AUC(Pr.):\", roc_auc_score(y_blind_watch, label_out_pred_prob))\n",
    "##=============================================================\n",
    "\n",
    "##=============================================================\n",
    "if enable_x:\n",
    "    print('\\n')\n",
    "    dtrain = xgb.DMatrix(x_cv_org,label=y_cv_org,feature_names=cols_new)\n",
    "    dtest = xgb.DMatrix(x_blind_org,label=y_blind_org,feature_names=cols_new)\n",
    "    watchlist  = [(dtest,'eval'), (dtrain,'train')]\n",
    "\n",
    "    bst = xgb.train(param_xgb, dtrain, num_round_xgb,\n",
    "                    evals=watchlist,\n",
    "                    verbose_eval=25)\n",
    "\n",
    "    label_out_pred_prob_x = bst.predict(dtest,output_margin=True,\n",
    "                                        ntree_limit=ntree_limit)\n",
    "    label_out_pred_prob_x = clf_sigmoid(label_out_pred_prob_x)\n",
    "    label_out_pred_x = np.argmax(label_out_pred_prob_x,axis=1)\n",
    "    label_out_pred_m = label_out_pred_x\n",
    "    label_out_pred=np.asarray(label_out_pred_m==watch_cls,dtype=np.int32)\n",
    "    label_out_pred_prob = label_out_pred_prob_x[:,watch_cls]\n",
    "\n",
    "    print(classification_report(y_blind_org, label_out_pred_m))\n",
    "    print(confusion_matrix(y_blind_org, label_out_pred_m))\n",
    "    print(\"ACCURACY:\", accuracy_score(y_blind_org, label_out_pred_m))\n",
    "    print(\"PRECISION:\", precision_score(y_blind_watch, label_out_pred))\n",
    "    print(\"RECALL:\", recall_score(y_blind_watch, label_out_pred))\n",
    "    print(\"F1:\", f1_score(y_blind_watch, label_out_pred))\n",
    "    print(\"ROC_AUC(Pr.):\", roc_auc_score(y_blind_watch, label_out_pred_prob))\n",
    "\n",
    "if enable_sk:\n",
    "    print('\\n')\n",
    "    ml.fit(x_cv_org, y_cv_org)\n",
    "\n",
    "    label_out_pred_prob_sk = ml.predict_proba(x_blind_org)\n",
    "    label_out_pred_sk = np.argmax(label_out_pred_prob_sk,axis=1)\n",
    "    label_out_pred_m = label_out_pred_sk\n",
    "    label_out_pred=np.asarray(label_out_pred_m==watch_cls,dtype=np.int32)\n",
    "    label_out_pred_prob = label_out_pred_prob_sk[:,watch_cls]\n",
    "\n",
    "    print(classification_report(y_blind_org, label_out_pred_m))\n",
    "    print(confusion_matrix(y_blind_org, label_out_pred_m))\n",
    "    print(\"ACCURACY:\", accuracy_score(y_blind_org, label_out_pred_m))\n",
    "    print(\"PRECISION:\", precision_score(y_blind_watch, label_out_pred))\n",
    "    print(\"RECALL:\", recall_score(y_blind_watch, label_out_pred))\n",
    "    print(\"F1:\", f1_score(y_blind_watch, label_out_pred))\n",
    "    print(\"ROC_AUC(Pr.):\", roc_auc_score(y_blind_watch, label_out_pred_prob))\n",
    "\n",
    "if enable_x and enable_sk:\n",
    "    label_out_pred_prob_hyb = label_out_pred_prob_x*(1-w_sk) + label_out_pred_prob_sk*w_sk\n",
    "elif enable_x:\n",
    "    label_out_pred_prob_hyb = label_out_pred_prob_x\n",
    "elif enable_sk:\n",
    "    label_out_pred_prob_hyb = label_out_pred_prob_sk\n",
    "if enable_dl:\n",
    "    if not enable_x and not enable_sk:\n",
    "        label_out_pred_prob_hyb = label_out_pred_prob_d\n",
    "    else:\n",
    "        label_out_pred_prob_hyb = label_out_pred_prob_hyb*(1-w_dl) + label_out_pred_prob_d*w_dl\n",
    "label_out_pred_hyb = np.argmax(label_out_pred_prob_hyb,axis=1)\n",
    "label_out_pred_m = label_out_pred_hyb\n",
    "label_out_pred=np.asarray(label_out_pred_m==watch_cls,dtype=np.int32)\n",
    "label_out_pred_prob = label_out_pred_prob_hyb[:,watch_cls]\n",
    "\n",
    "print('\\n')\n",
    "print(classification_report(y_blind_org, label_out_pred_m))\n",
    "print(confusion_matrix(y_blind_org, label_out_pred_m))\n",
    "print(\"ACCURACY:\", accuracy_score(y_blind_org, label_out_pred_m))\n",
    "print(\"PRECISION:\", precision_score(y_blind_watch, label_out_pred))\n",
    "print(\"RECALL:\", recall_score(y_blind_watch, label_out_pred))\n",
    "print(\"F1:\", f1_score(y_blind_watch, label_out_pred))\n",
    "print(\"ROC_AUC(Pr.):\", roc_auc_score(y_blind_watch, label_out_pred_prob))\n",
    "\n",
    "if conf_permtest==\"\":\n",
    "    fpr_oc, tpr_oc, _ = roc_curve(\n",
    "        y_blind_watch,label_out_pred_prob,pos_label=1)\n",
    "    plt.figure(8881,figsize=(10,8))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_oc, tpr_oc, 'r-',\n",
    "             label='BlindTest(AUC:%0.4f|F1:%0.4f|PR:%0.4f|RC:%0.4f|ACC:%0.4f)'\n",
    "             % (roc_auc_score(y_blind_watch, label_out_pred_prob),\n",
    "               f1_score(y_blind_watch, label_out_pred),\n",
    "               precision_score(y_blind_watch, label_out_pred),\n",
    "               recall_score(y_blind_watch, label_out_pred),\n",
    "               accuracy_score(y_blind_org, label_out_pred_m)))\n",
    "    plt.xlabel('False positive rate(1-Specificity)')\n",
    "    plt.ylabel('True positive rate(Sensitivity)')\n",
    "    plt.title('ROC curve' )\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(log_dir+\"/roc-blind.png\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "df_blind=pd.DataFrame()\n",
    "df_blind[\"pred_prob\"]=label_out_pred_prob\n",
    "df_blind[\"real_label\"]=y_blind_watch\n",
    "if conf_permtest==\"\":\n",
    "    df_blind_file=log_dir+\"/df-blind-pred.csv\"\n",
    "else:\n",
    "    df_blind_file=log_dir+\"/df-blind-pred%s_%d.csv\"%(conf_permtest,\n",
    "                                                     int(round(roc_auc_score(y_blind_watch, label_out_pred_prob)*10000,0)))\n",
    "df_blind.to_csv(df_blind_file,index=False)\n",
    "\n",
    "\n",
    "if conf_permtest==\"\" and not tb_enabled:\n",
    "    print(\"\\n========================CV Total=========================\\n\")\n",
    "    print(df_hist)\n",
    "    print(df_hist.describe())\n",
    "\n",
    "    rocp_avg = roc_auc_score(y_allcv, rocp_allcv)\n",
    "    pred_allcv = np.asarray(rocp_allcv>0.5,dtype=np.int32)\n",
    "    fpr_oc, tpr_oc, _ = roc_curve(\n",
    "        y_allcv,rocp_allcv,pos_label=1)\n",
    "    plt.figure(6661,figsize=(10,8))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_oc, tpr_oc, 'r-',\n",
    "             label='AVG(AUC:%0.4f|F1:%0.4f|PR:%0.4f|RC:%0.4f|ACC:%0.4f)'\n",
    "             % (\n",
    "\n",
    "                df_hist.rocp.mean(),\n",
    "                df_hist.f1.mean(),\n",
    "                df_hist.prec.mean(),\n",
    "                df_hist.recall.mean(),\n",
    "                df_hist.acc.mean()\n",
    "                ))\n",
    "    plt.xlabel('False positive rate(1-Specificity)')\n",
    "    plt.ylabel('True positive rate(Sensitivity)')\n",
    "    plt.title('ROC curve' )\n",
    "    plt.legend(loc='best')\n",
    "    # plt.savefig(log_dir+\"/roc-cvs.png\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    df_cvs=pd.DataFrame()\n",
    "    df_cvs[\"pred_prob\"]=rocp_allcv\n",
    "    df_cvs[\"real_label\"]=y_allcv\n",
    "    df_cvs.to_csv(log_dir+\"/df-cvs-pred.csv\",index=False)\n",
    "print(\"\")\n",
    "print(\"\\n===========================================================================\\n\")\n",
    "print(conf_tags)\n",
    "print(\"\\nRun time:\", time.time()-start)\n",
    "print(\"\\n===========================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381cb19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf28",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
